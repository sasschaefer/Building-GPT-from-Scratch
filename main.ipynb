{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aefbc68a",
   "metadata": {},
   "source": [
    "# Building GPT from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f71aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce19715a",
   "metadata": {},
   "source": [
    "## Task 1:\n",
    "* train the segmenter with varying k (try normalization strategies)\n",
    "* compare the performance against a different set\n",
    "* figure out a measure for accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e63654",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "* Use cleaned Shakespeare file (will be uploaded) — Train-Test-Validation set will be\n",
    "uploaded so that everyone has the same split\n",
    "    * Use Validation set mainly to optimise hyperparameters in interpolation, don‘t\n",
    "use it to optimise k\n",
    "* Develop n-gram engine (based on BPE encoding) that can deal with different n\n",
    "    * Unigram system first, then bigram system, then 3- and 4-gram; intrinsic\n",
    "evaluation for each:\n",
    "        * Report Perplexity (on BPE subwords)\n",
    "            * For bigram: look at how different k‘s affect perplexity\n",
    "        * „Add-one“ normalisation (Laplace Smoothing)\n",
    "        * Simple (not conditional) interpolation or Backoff\n",
    "* Write a program for extrinsic evaluation (generate sentence from n-gram system)\n",
    "    * Give context first\n",
    "    * Generation to predict next word (for now: argmax (most likely), or sampling\n",
    "for more variance)\n",
    "        * If word not present: assign average probability of all unigrams or\n",
    "assign most likely word of unigram\n",
    "        * Use end-of-sequence tokens to determine stop generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dcfc45",
   "metadata": {},
   "source": [
    "## Task 3:\n",
    "\n",
    "Implement neural embeddings – either hardcode or softer version, using PyTorch\n",
    "\n",
    "Watch RAM during training, especially for higher batch sizes (>=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167ba983",
   "metadata": {},
   "source": [
    "### Hardcode\n",
    "Hardcode version:\n",
    "* No PyTorch, no ML libraries, only numpy (at least for the neural embeddings,\n",
    "can use PyTorch, etc. for GPT implementation)\n",
    "    * Can use Counter and defaultdict from Collections\n",
    "    * We can, but do not have to hardcode the optimiser (can use Adam,\n",
    "need to use at least SGD)\n",
    "* Measure perplexity\n",
    "* Implement early stopping (when validation error/loss diverges from training\n",
    "error to avoid overfitting to training set) with patience\n",
    "    * Do not need to optimise for patience, but can\n",
    "    * Save top k (the amount that fits reasonably on your disk) of model\n",
    "checkpoints (can name that file for validation score and iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d5e590",
   "metadata": {},
   "source": [
    "We want a neural embedding with conditional generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e44ec23",
   "metadata": {},
   "source": [
    "### Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7958c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sample(probs, k):\n",
    "    \"\"\"\n",
    "    Top-k sampling from a probability distribution.\n",
    "    Args:\n",
    "        probs: 1D numpy array of probabilities for each word in the vocabulary.\n",
    "        k: number of top words to consider.\n",
    "    Returns:\n",
    "        index of the sampled word.\n",
    "    \"\"\"\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"k must be positive\")\n",
    "    # Get indices of top k probabilities\n",
    "    top_k_indices = probs.argsort()[-k:][::-1]\n",
    "    # Select top k probabilities and renormalize\n",
    "    top_k_probs = probs[top_k_indices]\n",
    "    top_k_probs = top_k_probs / top_k_probs.sum()\n",
    "    # Sample from the top k\n",
    "    sampled_idx = np.random.choice(top_k_indices, p=top_k_probs)\n",
    "    return sampled_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bff4f46",
   "metadata": {},
   "source": [
    "### Temperature sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a9262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dd5b453",
   "metadata": {},
   "source": [
    "### Softer Version\n",
    "\n",
    "* Using PyTorch\n",
    "* Implement early stopping (when validation error/loss diverges from training\n",
    "error to avoid overfitting to training set) with patience\n",
    "    * Do not have to optimise patience, but can\n",
    "    * Save top k (the amount that fits reasonably on disk) of model\n",
    "checkpoints (can name that file for validation score and iteration)\n",
    "* Tune hyperparameters using a grid search for each separately and\n",
    "validation set (order is important: number of merges, learning rate, weights of\n",
    "interpolation) – do not have to do all of this to pass, but for 1.0\n",
    "    * vocabulary size – gridsearch for max. 10 different amounts of merges\n",
    "    * learning rate of optimiser\n",
    "    * interpolation\n",
    "* Try versions with different optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ec9ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a0b86f8",
   "metadata": {},
   "source": [
    "## Task 4:\n",
    "Task remarks: GPT – Hand in, until 31.08.\n",
    "\n",
    "* If something is underspecified, just make decision yourself\n",
    "* Well-documented code\n",
    "* Submission format\n",
    "    * Notebook (incl. pdf) or GitHub readme (submit pdf with link to repo) as technical\n",
    "report of what we did\n",
    "        * Nice narrative and way to navigate code, not scientific paper\n",
    "        * Include plots (loss, perplexity scores, hyperparameters, etc.)\n",
    "        * Optional include pseudocode\n",
    "        * Qualitative analysis nice to have, e.g, add and evaluate generated text in\n",
    "report\n",
    "        * Can add appendix for additional plots\n",
    "* Hand in every mile stone, starting from UNIX comments\n",
    "* Removed in-between milestone of causal-self attention\n",
    "* Everything together in one file\n",
    "* Compare the models from each milestone, report perplexity for all\n",
    "    * Old-school n-gram\n",
    "    * Best neural n-gram\n",
    "    * GPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088fc2af",
   "metadata": {},
   "source": [
    "**GPT itself**\n",
    "* Hyperparameter tuning: do not need all of them, choose what is most interesting and\n",
    "explain why\n",
    "    * Number of merges in BPE (not complete gridsearch, isolate top three number of\n",
    "merges in perplexity in n-gram, test those for GPT)\n",
    "    * Regularisation\n",
    "    * How small can we make neural embedding\n",
    "    * Do not change optimiser\n",
    "* General remarks\n",
    "    * Transformer blocks from scratch would be beyond 1.0, not required\n",
    "    * Implement causal self-attention yourself, do not use ready-made PyTorch version\n",
    "    * For computing perplexity: Implementing teacher forcing annealing is necessary\n",
    "for good generation performance, but we don’t have to do it for our assignment\n",
    "* Reminders\n",
    "    * Skip weight initialisation and optimiser configuration\n",
    "        * Can use standard PyTorch initialisation → just get transformer\n",
    "parameters and add them when initialising the optimiser\n",
    "    * Remember to change device selection, currently “cuda”, you might want “mps” or\n",
    "“cpu”\n",
    "    * Configs: make n_embd smaller, don’t change betas and weight decay (unless\n",
    "you want to), can change batch size, chunk size, n_head, n_layer\n",
    "    * Specify temperature and top-k parameters for generate function\n",
    "    * Activation function used in MLP: not ReLU as in slides but GELU (might not be in\n",
    "PyTorch yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c1354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feac72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Networks\n",
    "    This consists of two linear transformations with a ReLU activation in between.\n",
    "    \n",
    "    FFN(x) = max(0, xW1 + b1 )W2 + b2\n",
    "    d_model: embedding dimension (e.g., 512)\n",
    "    d_ff: feed-forward dimension (e.g., 2048)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.d_ff= d_ff\n",
    "        \n",
    "        # Linear transformation y = xW+b\n",
    "        self.fc1 = nn.Linear(self.d_model, self.d_ff, bias = True)\n",
    "        self.fc2 = nn.Linear(self.d_ff, self.d_model, bias = True)\n",
    "        \n",
    "        # for potential speed up\n",
    "        # Pre-normalize the weights (can help with training stability)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # check input and first FF layer dimension matching\n",
    "        batch_size, seq_length, d_input = input.size()\n",
    "        assert self.d_model == d_input, \"d_model must be the same dimension as the input\"\n",
    "\n",
    "        # First linear transformation followed by ReLU\n",
    "        # There's no need for explicit torch.max() as F.relu() already implements max(0,x)\n",
    "        f1 = F.relu(self.fc1(input))\n",
    "\n",
    "        # max(0, xW_1 + b_1)W_2 + b_2 \n",
    "        f2 =  self.fc2(f1)\n",
    "\n",
    "        return f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf33542",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Causal Self-Attention (no cross attention, GPT style)\n",
    "    Args:\n",
    "        d_model: total hidden dimension of the model\n",
    "        num_head: number of attention heads\n",
    "        dropout: dropout rate for attention scores\n",
    "        bias: whether to include bias in linear projections\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_head, dropout=0.1, bias=True): # infer d_k, d_v, d_q from d_model\n",
    "        super().__init__()  # Missing in the original implementation\n",
    "        assert d_model % num_head == 0, \"d_model must be divisible by num_head\"\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "        self.d_head=d_model//num_head\n",
    "        self.dropout_rate = dropout  # Store dropout rate separately\n",
    "\n",
    "        # linear transformations\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.output_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Initiialize scaler\n",
    "        self.scaler = float(1.0 / math.sqrt(self.d_head)) # Store as float in initialization\n",
    "        \n",
    "\n",
    "    def forward(self, sequence, att_mask=None):\n",
    "        \"\"\"Input shape: [batch_size, seq_len, d_model=num_head * d_head]\"\"\"\n",
    "        batch_size, seq_len, model_dim = sequence.size()\n",
    "\n",
    "        # Check only critical input dimensions\n",
    "        assert model_dim == self.d_model, f\"Input dimension {model_dim} doesn't match model dimension {self.d_model}\"\n",
    "    \n",
    "        \n",
    "        # Linear projections and reshape for multi-head\n",
    "        Q_state = self.q_proj(sequence)\n",
    "        \n",
    "        kv_seq_len = seq_len\n",
    "        K_state = self.k_proj(sequence)\n",
    "        V_state = self.v_proj(sequence)\n",
    "\n",
    "        #[batch_size, self.num_head, seq_len, self.d_head]\n",
    "        Q_state = Q_state.view(batch_size, seq_len, self.num_head, self.d_head).transpose(1,2) \n",
    "            \n",
    "        # in cross-attention, key/value sequence length might be different from query sequence length\n",
    "        K_state = K_state.view(batch_size, kv_seq_len, self.num_head, self.d_head).transpose(1,2)\n",
    "        V_state = V_state.view(batch_size, kv_seq_len, self.num_head, self.d_head).transpose(1,2)\n",
    "\n",
    "        # Scale Q by 1/sqrt(d_k)\n",
    "        Q_state = Q_state * self.scaler\n",
    "    \n",
    "    \n",
    "        # Compute attention matrix: QK^T\n",
    "        self.att_matrix = torch.matmul(Q_state, K_state.transpose(-1,-2)) \n",
    "\n",
    "    \n",
    "        # apply attention mask to attention matrix\n",
    "        if att_mask is not None and not isinstance(att_mask, torch.Tensor):\n",
    "            raise TypeError(\"att_mask must be a torch.Tensor\")\n",
    "\n",
    "        if att_mask is not None:\n",
    "            self.att_matrix = self.att_matrix + att_mask\n",
    "        \n",
    "        # apply softmax to the last dimension to get the attention score: softmax(QK^T)\n",
    "        att_score = F.softmax(self.att_matrix, dim = -1)\n",
    "    \n",
    "        # apply drop out to attention score\n",
    "        att_score = self.dropout(att_score)\n",
    "    \n",
    "        # get final output: softmax(QK^T)V\n",
    "        att_output = torch.matmul(att_score, V_state)\n",
    "    \n",
    "        # concatinate all attention heads\n",
    "        att_output = att_output.transpose(1, 2)\n",
    "        att_output = att_output.contiguous().view(batch_size, seq_len, self.num_head*self.d_head) \n",
    "    \n",
    "        # final linear transformation to the concatenated output\n",
    "        att_output = self.output_proj(att_output)\n",
    "\n",
    "        assert att_output.size() == (batch_size, seq_len, self.d_model), \\\n",
    "        f\"Final output shape {att_output.size()} incorrect\"\n",
    "\n",
    "        return att_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe56a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.att = TransformerAttention(d_model, n_head, dropout=dropout)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FFN(d_model, d_ff)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_causal_mask(seq_len):\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, embed_input, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        embed_input: Decoder input sequence [batch_size, seq_len, d_model]\n",
    "        casual_attention_mask: Causal mask for self-attention [batch_size, seq_len, seq_len]\n",
    "        padding_mask: Padding mask for cross-attention [batch_size, seq_len, encoder_seq_len]\n",
    "        Returns:\n",
    "        Tensor: Decoded output [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = embed_input.size()\n",
    "        \n",
    "        assert embed_input.size(-1) == self.d_model, f\"Input dimension {embed_input.size(-1)} doesn't match model dimension {self.d_model}\"\n",
    "\n",
    "        # Generate and expand causal mask for self-attention\n",
    "        causal_mask = self.create_causal_mask(seq_len).to(embed_input.device)  # [seq_len, seq_len]\n",
    "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(1)  # [1, 1, seq_len, seq_len]\n",
    "\n",
    "\n",
    "        # Self-attention + residual + norm\n",
    "        att_out = self.att(x, att_mask=causal_mask)\n",
    "        x = self.ln1(x + self.dropout(att_out))\n",
    "\n",
    "        # FFN + residual + norm\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.ln2(x + self.dropout(ffn_out))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec78c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layer, n_embd, n_head, d_ff, max_seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, n_embd)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GPTBlock(n_embd, n_head, d_ff, dropout) for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        batch_size, seq_len = idx.size()\n",
    "        \n",
    "        pos = torch.arange(0, seq_len, device=idx.device).unsqueeze(0)\n",
    "        x = self.token_emb(idx) + self.pos_emb(pos)\n",
    "        # Causal mask for GPT\n",
    "        mask = GPTBlock.create_causal_mask(seq_len).to(idx.device)\n",
    "        mask = mask.unsqueeze(0).unsqueeze(1)  # [1, 1, seq_len, seq_len]\n",
    "        for block in self.blocks:\n",
    "            x = block(x, att_mask=mask)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c76066",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000      # depends on tokenizer\n",
    "n_layer = 4            # small for testing\n",
    "n_embd = 128           # embedding dimension\n",
    "n_head = 4             # must divide n_embd\n",
    "d_ff = 512             # feed-forward dimension\n",
    "max_seq_len = 128      # context window\n",
    "\n",
    "model = MiniGPT(vocab_size, n_layer, n_embd, n_head, d_ff, max_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c53984",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_len = 10\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))  # random token ids\n",
    "\n",
    "logits = model(x)  # [batch_size, seq_len, vocab_size]\n",
    "print(logits.shape)\n",
    "# torch.Size([2, 10, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a81f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "for step in range(100):\n",
    "    x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    y = x.clone()  # next-token prediction (shifted later)\n",
    "\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step} | Loss {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63156ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(idx)                       # [batch, seq, vocab_size]\n",
    "        logits = logits[:, -1, :]                 # last token logits\n",
    "        probs = F.softmax(logits, dim=-1)         # convert to probs\n",
    "        next_token = torch.multinomial(probs, 1)  # sample\n",
    "        idx = torch.cat([idx, next_token], dim=1) # append\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48afe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generation\n",
    "start = torch.tensor([[1]])  # BOS token or just any token id\n",
    "out = generate(model, start, max_new_tokens=20)\n",
    "print(\"Generated sequence:\", out.tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
