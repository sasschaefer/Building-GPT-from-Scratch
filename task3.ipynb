{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d644b447",
   "metadata": {},
   "source": [
    "## Task 3:\n",
    "\n",
    "Implement neural embeddings – either hardcode or softer version, using PyTorch\n",
    "\n",
    "Watch RAM during training, especially for higher batch sizes (>=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a21994",
   "metadata": {},
   "source": [
    "This is a neural n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc94f3e4",
   "metadata": {},
   "source": [
    "### Hardcode\n",
    "Hardcode version:\n",
    "* No PyTorch, no ML libraries, only numpy (at least for the neural embeddings,\n",
    "can use PyTorch, etc. for GPT implementation)\n",
    "    * Can use Counter and defaultdict from Collections\n",
    "    * We can, but do not have to hardcode the optimiser (can use Adam,\n",
    "need to use at least SGD)\n",
    "* Measure perplexity\n",
    "* Implement early stopping (when validation error/loss diverges from training\n",
    "error to avoid overfitting to training set) with patience\n",
    "    * Do not need to optimise for patience, but can\n",
    "    * Save top k (the amount that fits reasonably on your disk) of model\n",
    "checkpoints (can name that file for validation score and iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce67885",
   "metadata": {},
   "source": [
    "We want a neural embedding with conditional generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6508dc1c",
   "metadata": {},
   "source": [
    "### Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fcda46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hardcoded Neural n-gram trainer (NumPy)\n",
    "- No PyTorch; all model math + gradients in NumPy.\n",
    "- Saves CSV, PNG, checkpoints (.npz), samples.\n",
    "- Early stopping with patience; keeps top-k checkpoints by val loss.\n",
    "Usage (terminal):\n",
    "    python hardcode_neural_ngram.py --k 1000 --n 4 --embd 64 --hidden 128 --batch_size 512 --max_epochs 30\n",
    "In Jupyter the CLI is safe.\n",
    "\"\"\"\n",
    "\n",
    "import os, sys, json, time, random, math, argparse, csv\n",
    "from dataclasses import dataclass, asdict\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------- Tokenizer / BPE helpers -------------------------\n",
    "WORD_END = \"</w>\"\n",
    "EOS = \"<eos>\"\n",
    "BOS = \"<bos>\"\n",
    "_wsre = __import__(\"re\").compile(r\"\\s+\")\n",
    "\n",
    "def find_merges_file(k: int, verbose: bool = True):\n",
    "    candidates = [\n",
    "        os.path.join(\"Generated_tokens\", f\"bpe_merges with k = {k}.txt\"),\n",
    "        os.path.join(\"Generated_tokens\", f\"standard_bpe_merges_k{k}.txt\"),\n",
    "        os.path.join(\"Generated_tokens\", f\"aggressive_clean_bpe_merges_k{k}.txt\"),\n",
    "        os.path.join(\"Generated_tokens\", f\"bpe_merges_k{k}.txt\"),\n",
    "        os.path.join(\"Generated_tokens\", f\"bpe_merges_k{k}_webtext_clean.txt\"),\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            if verbose:\n",
    "                print(\"[Found merges]\", p)\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"No merges file found for k={k}. Tried: {candidates}\")\n",
    "\n",
    "def load_merges(path: str) -> List[Tuple[str, str]]:\n",
    "    merges = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                merges.append((parts[0], parts[1]))\n",
    "    return merges\n",
    "\n",
    "def words_from_text(text: str, lowercase: bool = True) -> List[str]:\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    return [w for w in _wsre.split(text.strip()) if w]\n",
    "\n",
    "def apply_merges_to_word(word: str, merges: List[Tuple[str, str]]) -> List[str]:\n",
    "    symbols = tuple(list(word) + [WORD_END])\n",
    "    for a, b in merges:\n",
    "        out = []\n",
    "        i, L = 0, len(symbols)\n",
    "        while i < L:\n",
    "            if i < L-1 and symbols[i] == a and symbols[i+1] == b:\n",
    "                out.append(a + b); i += 2\n",
    "            else:\n",
    "                out.append(symbols[i]); i += 1\n",
    "        symbols = tuple(out)\n",
    "    return list(symbols)\n",
    "\n",
    "def tokenize_lines_with_merges(text: str, merges: List[Tuple[str, str]]) -> List[List[str]]:\n",
    "    token_lines = []\n",
    "    for line in text.strip().splitlines():\n",
    "        words = words_from_text(line)\n",
    "        if not words: continue\n",
    "        toks = []\n",
    "        for w in words:\n",
    "            toks.extend(apply_merges_to_word(w, merges))\n",
    "        toks.append(EOS)\n",
    "        token_lines.append(toks)\n",
    "    return token_lines\n",
    "\n",
    "def build_vocab_from_texts(texts: Dict[str, str], merges: List[Tuple[str, str]], extras: List[str]=None):\n",
    "    vocab = set()\n",
    "    for name, txt in texts.items():\n",
    "        for ln in tokenize_lines_with_merges(txt, merges):\n",
    "            vocab.update(ln)\n",
    "    if extras:\n",
    "        vocab.update(extras)\n",
    "    id_to_token = sorted(vocab)\n",
    "    token_to_id = {t:i for i,t in enumerate(id_to_token)}\n",
    "    return id_to_token, token_to_id\n",
    "\n",
    "def encode_lines_to_ids(text: str, merges: List[Tuple[str, str]], token_to_id: Dict[str,int]) -> List[int]:\n",
    "    ids = []\n",
    "    for ln in tokenize_lines_with_merges(text, merges):\n",
    "        for t in ln:\n",
    "            if t in token_to_id:\n",
    "                ids.append(token_to_id[t])\n",
    "    return ids\n",
    "\n",
    "# --------------------------- NumPy model utilities --------------------------\n",
    "def one_hot(idx: np.ndarray, dim: int):\n",
    "    # idx: (B,) or (B,1) etc -> returns (B, dim)\n",
    "    x = np.zeros((idx.shape[0], dim), dtype=np.float32)\n",
    "    x[np.arange(idx.shape[0]), idx.ravel()] = 1.0\n",
    "    return x\n",
    "\n",
    "# --------------------------- Neural n-gram model ---------------------------\n",
    "@dataclass\n",
    "class HardModelParams:\n",
    "    \"\"\"Container for model weight matrices; simple to save/load via np.savez\"\"\"\n",
    "    # Will store as dict of arrays\n",
    "\n",
    "class NeuralNGramHard:\n",
    "    \"\"\"\n",
    "    Neural n-gram implemented in numpy.\n",
    "    Architecture:\n",
    "      - Embedding matrix: (V, embd)\n",
    "      - Linear1: ((n-1)*embd, hidden)\n",
    "      - ReLU\n",
    "      - Linear2: (hidden, vocab) -> logits\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size:int, n:int, embd:int, hidden:int, seed:int=1337):\n",
    "        assert n >= 2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n = n\n",
    "        self.embd = embd\n",
    "        self.hidden = hidden\n",
    "        rng = np.random.RandomState(seed)\n",
    "\n",
    "        # params\n",
    "        self.E = rng.normal(0, 0.02, size=(vocab_size, embd)).astype(np.float32)  # embeddings\n",
    "        self.W1 = rng.normal(0, 0.02, size=((n-1)*embd, hidden)).astype(np.float32)\n",
    "        self.b1 = np.zeros((hidden,), dtype=np.float32)\n",
    "        self.W2 = rng.no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97044899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sample(probs, k):\n",
    "    \"\"\"\n",
    "    Top-k sampling from a probability distribution.\n",
    "    Args:\n",
    "        probs: 1D numpy array of probabilities for each word in the vocabulary.\n",
    "        k: number of top words to consider.\n",
    "    Returns:\n",
    "        index of the sampled word.\n",
    "    \"\"\"\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"k must be positive\")\n",
    "    # Get indices of top k probabilities\n",
    "    top_k_indices = probs.argsort()[-k:][::-1]\n",
    "    # Select top k probabilities and renormalize\n",
    "    top_k_probs = probs[top_k_indices]\n",
    "    top_k_probs = top_k_probs / top_k_probs.sum()\n",
    "    # Sample from the top k\n",
    "    sampled_idx = np.random.choice(top_k_indices, p=top_k_probs)\n",
    "    return sampled_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e918b33",
   "metadata": {},
   "source": [
    "### Temperature sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c12e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "188da78c",
   "metadata": {},
   "source": [
    "### Softer Version\n",
    "\n",
    "* Using PyTorch\n",
    "* Implement early stopping (when validation error/loss diverges from training\n",
    "error to avoid overfitting to training set) with patience\n",
    "    * Do not have to optimise patience, but can\n",
    "    * Save top k (the amount that fits reasonably on disk) of model\n",
    "checkpoints (can name that file for validation score and iteration)\n",
    "* Tune hyperparameters using a grid search for each separately and\n",
    "validation set (order is important: number of merges, learning rate, weights of\n",
    "interpolation) – do not have to do all of this to pass, but for 1.0\n",
    "    * vocabulary size – gridsearch for max. 10 different amounts of merges\n",
    "    * learning rate of optimiser\n",
    "    * interpolation\n",
    "* Try versions with different optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c86167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
