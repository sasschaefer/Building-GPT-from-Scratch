{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d644b447",
   "metadata": {},
   "source": [
    "## Task 3:\n",
    "\n",
    "Implement neural embeddings – either hardcode or softer version, using PyTorch\n",
    "\n",
    "Watch RAM during training, especially for higher batch sizes (>=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a21994",
   "metadata": {},
   "source": [
    "This is a neural n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc94f3e4",
   "metadata": {},
   "source": [
    "### Hardcode\n",
    "Hardcode version:\n",
    "* No PyTorch, no ML libraries, only numpy (at least for the neural embeddings,\n",
    "can use PyTorch, etc. for GPT implementation)\n",
    "    * Can use Counter and defaultdict from Collections\n",
    "    * We can, but do not have to hardcode the optimiser (can use Adam,\n",
    "need to use at least SGD)\n",
    "* Measure perplexity\n",
    "* Implement early stopping (when validation error/loss diverges from training\n",
    "error to avoid overfitting to training set) with patience\n",
    "    * Do not need to optimise for patience, but can\n",
    "    * Save top k (the amount that fits reasonably on your disk) of model\n",
    "checkpoints (can name that file for validation score and iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce67885",
   "metadata": {},
   "source": [
    "We want a neural embedding with conditional generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6508dc1c",
   "metadata": {},
   "source": [
    "### Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf1bcb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_ngram_model() missing 3 required positional arguments: 'merges', 'id_to_token', and 'token_to_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     76\u001b[39m test_ids  = tokenize_with_bpe(\u001b[33m\"\u001b[39m\u001b[33mCorpus/shakespeare_clean_test.txt\u001b[39m\u001b[33m\"\u001b[39m, tok)\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Train neural n-gram\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m model = train_ngram_model(train_ids, val_ids, n=\u001b[32m5\u001b[39m, embd=\u001b[32m32\u001b[39m, hidden=\u001b[32m128\u001b[39m)\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     82\u001b[39m ppl = perplexity(model, test_ids, n=\u001b[32m5\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: train_ngram_model() missing 3 required positional arguments: 'merges', 'id_to_token', and 'token_to_id'"
     ]
    }
   ],
   "source": [
    "import os, numpy as np, time\n",
    "from collections import deque\n",
    "from src.bpe import BPETokenizer\n",
    "from src.neural_ngram import NeuralNGramHard\n",
    "\n",
    "# ---------------------- Dataset utilities ----------------------\n",
    "def tokenize_with_bpe(path, tokenizer: BPETokenizer):\n",
    "    \"\"\"Tokenize a text file into ids using a BPETokenizer object.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return tokenizer.encode_text(text)\n",
    "\n",
    "\n",
    "def make_ngram_batches(ids, n, batch_size):\n",
    "    \"\"\"Yield mini-batches of n-gram contexts and targets.\"\"\"\n",
    "    N = len(ids) - (n-1)\n",
    "    idx = np.arange(N)\n",
    "    np.random.shuffle(idx)\n",
    "    for start in range(0, N, batch_size):\n",
    "        batch = idx[start:start+batch_size]\n",
    "        X = np.stack([ids[i:i+n-1] for i in batch])\n",
    "        Y = np.array([ids[i+n-1] for i in batch])\n",
    "        yield X, Y\n",
    "\n",
    "# ---------------------- Training loop ----------------------\n",
    "def train_ngram_model(train_ids, val_ids, id_to_token,\n",
    "                      n=5, embd=32, hidden=128, batch_size=32,\n",
    "                      lr=0.3, epochs=20, patience=3, top_k=3):\n",
    "\n",
    "    model = NeuralNGramHard(vocab_size=len(id_to_token), n=n,\n",
    "                            embd=embd, hidden=hidden)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    saved_ckpts = deque(maxlen=top_k)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        losses = []\n",
    "        for X, Y in make_ngram_batches(train_ids, n, batch_size):\n",
    "            loss = model.train_batch(X, Y, lr=lr)\n",
    "            losses.append(loss)\n",
    "        train_loss = np.mean(losses)\n",
    "        val_loss = model.eval_loss(np.array(val_ids), n=n)\n",
    "\n",
    "        print(f\"Epoch {epoch}: train={train_loss:.4f}, val={val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            patience_counter = 0\n",
    "            ckpt_path = f\"checkpoints/epoch{epoch}_valloss{val_loss:.4f}.npz\"\n",
    "            os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "            model.save(ckpt_path)\n",
    "            saved_ckpts.append(ckpt_path)\n",
    "            print(f\"  [Saved checkpoint: {ckpt_path}]\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    return model\n",
    "\n",
    "# ---------------------- Perplexity ----------------------\n",
    "def perplexity(model, ids, n):\n",
    "    loss = model.eval_loss(np.array(ids), n=n)\n",
    "    return np.exp(loss)\n",
    "\n",
    "# ---------------------- Run ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load tokenizer\n",
    "    tok = BPETokenizer.load_json(\"artifacts/bpe_tokenizer.json\")\n",
    "\n",
    "    # Tokenize datasets\n",
    "    train_ids = tokenize_with_bpe(\"Corpus/shakespeare_clean_train.txt\", tok)\n",
    "    val_ids   = tokenize_with_bpe(\"Corpus/shakespeare_clean_valid.txt\", tok)\n",
    "    test_ids  = tokenize_with_bpe(\"Corpus/shakespeare_clean_test.txt\", tok)\n",
    "\n",
    "    # Train neural n-gram\n",
    "    model = train_ngram_model(train_ids, val_ids, n=5, embd=32, hidden=128)\n",
    "\n",
    "    # Evaluate\n",
    "    ppl = perplexity(model, test_ids, n=5)\n",
    "    print(f\"Test Perplexity = {ppl:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ab99e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bpe import BPETokenizer\n",
    "\n",
    "# Train\n",
    "tok = BPETokenizer()\n",
    "tok.fit(\"Corpus/shakespeare_clean_train.txt\", k=200)  # k = number of merges\n",
    "tok.save_json(\"artifacts/bpe_tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fcda46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hardcoded Neural n-gram trainer (NumPy)\n",
    "- No PyTorch; all model math + gradients in NumPy.\n",
    "- Saves CSV, PNG, checkpoints (.npz), samples.\n",
    "- Early stopping with patience; keeps top-k checkpoints by val loss.\n",
    "Usage (terminal):\n",
    "    python hardcode_neural_ngram.py --k 1000 --n 4 --embd 64 --hidden 128 --batch_size 512 --max_epochs 30\n",
    "In Jupyter the CLI is safe.\n",
    "\"\"\"\n",
    "\n",
    "import os, sys, json, time, random, math, argparse, csv\n",
    "from dataclasses import dataclass, asdict\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------- Tokenizer / BPE helpers -------------------------\n",
    "WORD_END = \"</w>\"\n",
    "EOS = \"<eos>\"\n",
    "BOS = \"<bos>\"\n",
    "_wsre = __import__(\"re\").compile(r\"\\s+\")\n",
    "\n",
    "def find_merges_file(k: int, verbose: bool = True):\n",
    "    candidates = [\n",
    "        os.path.join(\"Generated_tokens\", f\"bpe_merges with k = {k}.txt\"),\n",
    "        os.path.join(\"Generated_tokens\", f\"standard_bpe_merges_k{k}.txt\"),\n",
    "        os.path.join(\"Generated_tokens\", f\"aggressive_clean_bpe_merges_k{k}.txt\"),\n",
    "        os.path.join(\"Generated_tokens\", f\"bpe_merges_k{k}.txt\"),\n",
    "        os.path.join(\"Generated_tokens\", f\"bpe_merges_k{k}_webtext_clean.txt\"),\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            if verbose:\n",
    "                print(\"[Found merges]\", p)\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"No merges file found for k={k}. Tried: {candidates}\")\n",
    "\n",
    "def load_merges(path: str) -> List[Tuple[str, str]]:\n",
    "    merges = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                merges.append((parts[0], parts[1]))\n",
    "    return merges\n",
    "\n",
    "def words_from_text(text: str, lowercase: bool = True) -> List[str]:\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    return [w for w in _wsre.split(text.strip()) if w]\n",
    "\n",
    "def apply_merges_to_word(word: str, merges: List[Tuple[str, str]]) -> List[str]:\n",
    "    symbols = tuple(list(word) + [WORD_END])\n",
    "    for a, b in merges:\n",
    "        out = []\n",
    "        i, L = 0, len(symbols)\n",
    "        while i < L:\n",
    "            if i < L-1 and symbols[i] == a and symbols[i+1] == b:\n",
    "                out.append(a + b); i += 2\n",
    "            else:\n",
    "                out.append(symbols[i]); i += 1\n",
    "        symbols = tuple(out)\n",
    "    return list(symbols)\n",
    "\n",
    "def tokenize_lines_with_merges(text: str, merges: List[Tuple[str, str]]) -> List[List[str]]:\n",
    "    token_lines = []\n",
    "    for line in text.strip().splitlines():\n",
    "        words = words_from_text(line)\n",
    "        if not words: continue\n",
    "        toks = []\n",
    "        for w in words:\n",
    "            toks.extend(apply_merges_to_word(w, merges))\n",
    "        toks.append(EOS)\n",
    "        token_lines.append(toks)\n",
    "    return token_lines\n",
    "\n",
    "def build_vocab_from_texts(texts: Dict[str, str], merges: List[Tuple[str, str]], extras: List[str]=None):\n",
    "    vocab = set()\n",
    "    for name, txt in texts.items():\n",
    "        for ln in tokenize_lines_with_merges(txt, merges):\n",
    "            vocab.update(ln)\n",
    "    if extras:\n",
    "        vocab.update(extras)\n",
    "    id_to_token = sorted(vocab)\n",
    "    token_to_id = {t:i for i,t in enumerate(id_to_token)}\n",
    "    return id_to_token, token_to_id\n",
    "\n",
    "def encode_lines_to_ids(text: str, merges: List[Tuple[str, str]], token_to_id: Dict[str,int]) -> List[int]:\n",
    "    ids = []\n",
    "    for ln in tokenize_lines_with_merges(text, merges):\n",
    "        for t in ln:\n",
    "            if t in token_to_id:\n",
    "                ids.append(token_to_id[t])\n",
    "    return ids\n",
    "\n",
    "# --------------------------- NumPy model utilities --------------------------\n",
    "def one_hot(idx: np.ndarray, dim: int):\n",
    "    # idx: (B,) or (B,1) etc -> returns (B, dim)\n",
    "    x = np.zeros((idx.shape[0], dim), dtype=np.float32)\n",
    "    x[np.arange(idx.shape[0]), idx.ravel()] = 1.0\n",
    "    return x\n",
    "\n",
    "# --------------------------- Neural n-gram model ---------------------------\n",
    "@dataclass\n",
    "class HardModelParams:\n",
    "    \"\"\"Container for model weight matrices; simple to save/load via np.savez\"\"\"\n",
    "    # Will store as dict of arrays\n",
    "\n",
    "class NeuralNGramHard:\n",
    "    \"\"\"\n",
    "    Neural n-gram implemented in numpy.\n",
    "    Architecture:\n",
    "      - Embedding matrix: (V, embd)\n",
    "      - Linear1: ((n-1)*embd, hidden)\n",
    "      - ReLU\n",
    "      - Linear2: (hidden, vocab) -> logits\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size:int, n:int, embd:int, hidden:int, seed:int=1337):\n",
    "        assert n >= 2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n = n\n",
    "        self.embd = embd\n",
    "        self.hidden = hidden\n",
    "        rng = np.random.RandomState(seed)\n",
    "\n",
    "        # params\n",
    "        self.E = rng.normal(0, 0.02, size=(vocab_size, embd)).astype(np.float32)  # embeddings\n",
    "        self.W1 = rng.normal(0, 0.02, size=((n-1)*embd, hidden)).astype(np.float32)\n",
    "        self.b1 = np.zeros((hidden,), dtype=np.float32)\n",
    "        self.W2 = rng.no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97044899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sample(probs, k):\n",
    "    \"\"\"\n",
    "    Top-k sampling from a probability distribution.\n",
    "    Args:\n",
    "        probs: 1D numpy array of probabilities for each word in the vocabulary.\n",
    "        k: number of top words to consider.\n",
    "    Returns:\n",
    "        index of the sampled word.\n",
    "    \"\"\"\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"k must be positive\")\n",
    "    # Get indices of top k probabilities\n",
    "    top_k_indices = probs.argsort()[-k:][::-1]\n",
    "    # Select top k probabilities and renormalize\n",
    "    top_k_probs = probs[top_k_indices]\n",
    "    top_k_probs = top_k_probs / top_k_probs.sum()\n",
    "    # Sample from the top k\n",
    "    sampled_idx = np.random.choice(top_k_indices, p=top_k_probs)\n",
    "    return sampled_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e918b33",
   "metadata": {},
   "source": [
    "### Temperature sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c12e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "188da78c",
   "metadata": {},
   "source": [
    "### Softer Version\n",
    "\n",
    "* Using PyTorch\n",
    "* Implement early stopping (when validation error/loss diverges from training\n",
    "error to avoid overfitting to training set) with patience\n",
    "    * Do not have to optimise patience, but can\n",
    "    * Save top k (the amount that fits reasonably on disk) of model\n",
    "checkpoints (can name that file for validation score and iteration)\n",
    "* Tune hyperparameters using a grid search for each separately and\n",
    "validation set (order is important: number of merges, learning rate, weights of\n",
    "interpolation) – do not have to do all of this to pass, but for 1.0\n",
    "    * vocabulary size – gridsearch for max. 10 different amounts of merges\n",
    "    * learning rate of optimiser\n",
    "    * interpolation\n",
    "* Try versions with different optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c86167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
