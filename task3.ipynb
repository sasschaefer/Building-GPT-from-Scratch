{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d644b447",
   "metadata": {},
   "source": [
    "## Task 3:\n",
    "\n",
    "Implement neural embeddings – either hardcode or softer version, using PyTorch\n",
    "\n",
    "Watch RAM during training, especially for higher batch sizes (>=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc94f3e4",
   "metadata": {},
   "source": [
    "### Hardcode\n",
    "Hardcode version:\n",
    "* No PyTorch, no ML libraries, only numpy (at least for the neural embeddings,\n",
    "can use PyTorch, etc. for GPT implementation)\n",
    "    * Can use Counter and defaultdict from Collections\n",
    "    * We can, but do not have to hardcode the optimiser (can use Adam,\n",
    "need to use at least SGD)\n",
    "* Measure perplexity\n",
    "* Implement early stopping (when validation error/loss diverges from training\n",
    "error to avoid overfitting to training set) with patience\n",
    "    * Do not need to optimise for patience, but can\n",
    "    * Save top k (the amount that fits reasonably on your disk) of model\n",
    "checkpoints (can name that file for validation score and iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce67885",
   "metadata": {},
   "source": [
    "We want a neural embedding with conditional generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6508dc1c",
   "metadata": {},
   "source": [
    "### Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97044899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sample(probs, k):\n",
    "    \"\"\"\n",
    "    Top-k sampling from a probability distribution.\n",
    "    Args:\n",
    "        probs: 1D numpy array of probabilities for each word in the vocabulary.\n",
    "        k: number of top words to consider.\n",
    "    Returns:\n",
    "        index of the sampled word.\n",
    "    \"\"\"\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"k must be positive\")\n",
    "    # Get indices of top k probabilities\n",
    "    top_k_indices = probs.argsort()[-k:][::-1]\n",
    "    # Select top k probabilities and renormalize\n",
    "    top_k_probs = probs[top_k_indices]\n",
    "    top_k_probs = top_k_probs / top_k_probs.sum()\n",
    "    # Sample from the top k\n",
    "    sampled_idx = np.random.choice(top_k_indices, p=top_k_probs)\n",
    "    return sampled_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e918b33",
   "metadata": {},
   "source": [
    "### Temperature sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c12e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "188da78c",
   "metadata": {},
   "source": [
    "### Softer Version\n",
    "\n",
    "* Using PyTorch\n",
    "* Implement early stopping (when validation error/loss diverges from training\n",
    "error to avoid overfitting to training set) with patience\n",
    "    * Do not have to optimise patience, but can\n",
    "    * Save top k (the amount that fits reasonably on disk) of model\n",
    "checkpoints (can name that file for validation score and iteration)\n",
    "* Tune hyperparameters using a grid search for each separately and\n",
    "validation set (order is important: number of merges, learning rate, weights of\n",
    "interpolation) – do not have to do all of this to pass, but for 1.0\n",
    "    * vocabulary size – gridsearch for max. 10 different amounts of merges\n",
    "    * learning rate of optimiser\n",
    "    * interpolation\n",
    "* Try versions with different optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c86167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
