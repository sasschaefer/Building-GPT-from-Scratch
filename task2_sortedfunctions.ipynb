{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfacb339",
   "metadata": {},
   "source": [
    "# Task 2 - N-gram Language Models with BPE\n",
    "\n",
    "In this task we implement and evaluate n-gram language models based on a cleaned Shakespeare corpus.  \n",
    "The corpus is split into training, validation, and test sets (predefined).  \n",
    "\n",
    "Key requirements:\n",
    "- Models over BPE subword tokens\n",
    "- Unigram → Bigram → Trigram → 4-gram\n",
    "- Intrinsic evaluation: Perplexity\n",
    "- Bigram analysis across different smoothing constants *k*\n",
    "- Laplace (add-one) smoothing\n",
    "- Simple interpolation/backoff\n",
    "- Extrinsic evaluation: sentence generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b76eee7",
   "metadata": {},
   "source": [
    "## Data Loading and Tokenization\n",
    "\n",
    "We start by loading the cleaned Shakespeare dataset and applying **Byte-Pair Encoding (BPE)**.  \n",
    "The dataset is split into **train, validation, and test** to ensure consistent comparison across models.  \n",
    "The number of BPE merges (*k*, e.g. 1600) determines the vocabulary size and granularity of subword units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ecfd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 2 — BLOCK 1: IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import math\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "import os, re, random\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Iterable\n",
    "import time, math\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "# Essential constants and functions from Task 1\n",
    "CORPUS_DIR = \"Corpus\"\n",
    "GENERATED_DIR = \"Generated_tokens\"\n",
    "TRAIN_CLEAN = os.path.join(CORPUS_DIR, \"Shakespeare_clean_train.txt\")\n",
    "VALID_CLEAN = os.path.join(CORPUS_DIR, \"Shakespeare_clean_valid.txt\")\n",
    "TEST_CLEAN = os.path.join(CORPUS_DIR, \"Shakespeare_clean_test.txt\")\n",
    "K_LIST = [1000, 1200, 1400, 1600, 1800, 2000]\n",
    "WORD_END = \"</w>\"\n",
    "\n",
    "_wsre = re.compile(r\"\\s+\")\n",
    "\n",
    "# Task 2 specific tokens\n",
    "EOS = \"<eos>\"\n",
    "BOS = \"<bos>\"\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6a424d",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "This block defines helper functions that are reused across the notebook.  \n",
    "They cover tasks such as:\n",
    "\n",
    "- Handling tokenization and decoding  \n",
    "- Managing probability calculations  \n",
    "- Supporting text generation routines  \n",
    "\n",
    "By centralizing these functions, the implementation of n-gram models remains cleaner and easier to extend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 2 — BLOCK 2: UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# sorted out \n",
    "\n",
    "import src.data_utils as du\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37747fff",
   "metadata": {},
   "source": [
    "## N-gram Building Functions\n",
    "\n",
    "- `add_bos_context(n)`: pads each line with `<bos>` tokens for n-gram context.\n",
    "- `build_ngrams`: builds vocabulary, n-gram counts, and context counts from tokenized lines.  \n",
    "These counts are the basis for probability estimation and perplexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7cc0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all sorted out \n",
    "\n",
    "import src.ngram as ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7228c37",
   "metadata": {},
   "source": [
    "## N-gram Language Model\n",
    "\n",
    "`NGramLM(n)` implements several estimators:\n",
    "- **ML** (`p_ml`) and **Laplace** (`p_laplace`) smoothing.  \n",
    "- **Linear interpolation** (`p_interpolated`) over orders 1…n (defaults to Laplace components).  \n",
    "- **Katz-like backoff** (`p_backoff_katz`, simplified) and **stupid backoff** (`p_backoff`, not normalized).  \n",
    "Models are chained recursively so lower-order distributions are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sorted out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769652a",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "\n",
    "`perplexity(model, token_lines, mode)` computes PPL over BPE tokens, resetting context at `<eos>`.  \n",
    "Supported modes: `ml`, `laplace`, `interp`, `backoff`, `katz`.  \n",
    "**Note:** Stupid backoff is not a proper probability distribution—treat its PPL as a *relative* score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f3ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERPLEXITY CALCULATION\n",
    "\n",
    "import src.evaluation as ev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f01eb",
   "metadata": {},
   "source": [
    "## Data Loading Functions\n",
    "\n",
    "- `find_merges_file(k)`: locates a merges file in `Generated_tokens/` (flexible naming).  \n",
    "- `load_token_lines_for_k(k)`: loads splits, applies merges, and returns tokenized lines for train/valid/test.  \n",
    "Per the task, the **validation set is used for tuning interpolation weights, not for choosing `k`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c1769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted out\n",
    "\n",
    "corpus = GENERATED_DIR\n",
    "train = TRAIN_CLEAN\n",
    "valid = VALID_CLEAN\n",
    "test = TEST_CLEAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c107194c",
   "metadata": {},
   "source": [
    "## Training & Evaluation Utilities\n",
    "\n",
    "- `grid_simplex_lambdas(n, step)`: enumerates interpolation weights that sum to 1.  \n",
    "- `train_and_eval_for_k(k, n_max, tune_interp)`: trains n=1…n_max; reports PPL for ML/Laplace/Backoff;  \n",
    "  tunes **interpolation weights on the validation set** and then evaluates on test.  \n",
    "- `bigram_vs_k(k_list)`: evaluates bigram performance across different BPE merge counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10248aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND EVALUATION FUNCTIONS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7123f3f1",
   "metadata": {},
   "source": [
    "\n",
    "## Text Generation (Extrinsic Evaluation)\n",
    "\n",
    "- **Encoding/decoding:** `bpe_encode_words`, `bpe_decode_to_words` convert between words and BPE tokens.  \n",
    "- **Fallbacks:** `_unigram_fallback('most'|'avg')` handles unseen contexts.  \n",
    "- **Decoding:** `_next_token_argmax_or_sample` supports argmax or temperature sampling.  \n",
    "- **Driver:** `generate_sentence(...)` continues from a prompt until `<eos>` or a word budget is reached.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sorted out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967017cc",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "`prepare_models(k, n_max)` trains and caches n-gram models (1…n_max) for a fixed `k`,  \n",
    "and prints Laplace perplexities on train/valid/test—used by the generation suite to avoid retraining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c32c0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_models(k: int, n_max: int = 4) -> Tuple[List[Tuple[str, str]], Dict[int, ngram.NGramLM]]:\n",
    "    \"\"\"\n",
    "    Load merges and train n-gram models up to order n_max.\n",
    "    Also compute perplexities on validation and test.\n",
    "    Returns:\n",
    "      merges: BPE merges list\n",
    "      models: dict {n: NGramLM}\n",
    "    \"\"\"\n",
    "    # Load merges + tokenized splits\n",
    "    merges, tr_tok, va_tok, te_tok = du.load_token_lines_for_k(k, corpus, train, valid, test)\n",
    "    models = {}\n",
    "\n",
    "    for n in range(1, n_max + 1):\n",
    "        print(f\"\\n[prepare_models] Training {n}-gram model for k={k}\")\n",
    "        model = ngram.NGramLM(n, tr_tok)\n",
    "\n",
    "       # Evaluate perplexities\n",
    "        ppl_train = ev.perplexity(model, tr_tok, mode=\"laplace\")\n",
    "        ppl_valid = ev.perplexity(model, va_tok, mode=\"laplace\") if va_tok else None\n",
    "        ppl_test  = ev.perplexity(model, te_tok, mode=\"laplace\") if te_tok else None\n",
    "\n",
    "        print(f\"[n={n}] train ppl={ppl_train:.2f} | valid ppl={ppl_valid:.2f} | test ppl={ppl_test:.2f}\")\n",
    "\n",
    "        models[n] = model\n",
    "\n",
    "    return merges, models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb3091",
   "metadata": {},
   "source": [
    "## Helpers for Reporting\n",
    "\n",
    "Lightweight analysis for generated text:\n",
    "- Diversity: `distinct-1/2`.  \n",
    "- Repetition: adjacent duplicates and longest repeat run.  \n",
    "- Optional self-scoring hook (`score_sequence_logprob`) to compute avg NLL / PPL if available.  \n",
    "Also maps external mode names (e.g., `\"simple\"` → stupid backoff) and optionally uses a fast generator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d11d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[prepare_models] Training 1-gram model for k=1000\n",
      "[n=1] train ppl=282.43 | valid ppl=278.58 | test ppl=277.44\n",
      "\n",
      "[prepare_models] Training 2-gram model for k=1000\n",
      "[n=2] train ppl=72.68 | valid ppl=80.55 | test ppl=80.21\n",
      "\n",
      "[prepare_models] Training 3-gram model for k=1000\n",
      "[n=3] train ppl=164.65 | valid ppl=217.90 | test ppl=221.55\n",
      "\n",
      "[prepare_models] Training 4-gram model for k=1000\n",
      "[n=4] train ppl=306.63 | valid ppl=464.27 | test ppl=479.42\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[spec_interp_argmax] mode=interp | n=2 | T=1.0 | argmax\n",
      "  to the  and the  and the  and the  and the  and the  and the\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[spec_simple_sample] mode=simple | n=3 | T=0.7 | sample\n",
      "  othello i do estate upon me. desdemona i am cruel cassio. othello o ialack the couples that lives in the\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[grid_argmax_T0.5] mode=simple | n=2 | T=0.5 | argmax\n",
      "  else to the  and  and  and  and  and  and  and  and\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[grid_argmax_T0.7] mode=simple | n=2 | T=0.7 | argmax\n",
      "  else to the  and  and  and  and  and  and  and  and\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[grid_argmax_T1.0] mode=simple | n=2 | T=1.0 | argmax\n",
      "  else to the  and  and  and  and  and  and  and  and\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[grid_sample_T0.5] mode=simple | n=2 | T=0.5 | sample\n",
      "  else of the matter. and as  and  and ross in my lord, and that he is you must\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[grid_sample_T0.7] mode=simple | n=2 | T=0.7 | sample\n",
      "  liseest thou, antony can chsay, cousines, i can never to him,they fic  if you have  can you lives\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[grid_sample_T1.0] mode=simple | n=2 | T=1.0 | sample\n",
      "  the , ophelia my fas alking cution to dolacequmoreflies brif thou k! ,ssay, thou wabut ld a seem and let\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[grid_argmax_T0.5] mode=laplace | n=2 | T=0.5 | argmax\n",
      "  the word, and the word, and the word, and the word, and the word, and the word, and the word,\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[grid_argmax_T0.7] mode=laplace | n=2 | T=0.7 | argmax\n",
      "  the word, and the word, and the word, and the word, and the word, and the word, and the word,\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[grid_argmax_T1.0] mode=laplace | n=2 | T=1.0 | argmax\n",
      "  the word, and the word, and the word, and the word, and the word, and the word, and the word,\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[grid_sample_T0.5] mode=laplace | n=2 | T=0.5 | sample\n",
      "  the firee, i am gure, and his figar, i am thought to be cont, thou hast s, and caphere, who\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[grid_sample_T0.7] mode=laplace | n=2 | T=0.7 | sample\n",
      "  cly. at en ledge ess fs house thing man and your spving venice there ct lead in all men ws\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[grid_sample_T1.0] mode=laplace | n=2 | T=1.0 | sample\n",
      "  good sed isexit welcome yournothing purcomeverme noble s, it to chiwhcaenhand pdrocan truedisnaled might lius viconfportidevihouse pent full sanbaljyself you...\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[grid_argmax_T0.5] mode=interp | n=3 | T=0.5 | argmax\n",
      "  \n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[grid_argmax_T0.7] mode=interp | n=3 | T=0.7 | argmax\n",
      "  \n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[grid_argmax_T1.0] mode=interp | n=3 | T=1.0 | argmax\n",
      "  \n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[grid_sample_T0.5] mode=interp | n=3 | T=0.5 | sample\n",
      "  this same servant   s. with marry 's  i will    but  exit in\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[grid_sample_T0.7] mode=interp | n=3 | T=0.7 | sample\n",
      "  ? ate hamthought tain  exit bottom 's no bottportiimf' the uni that ld. cittqueen tain time a bon seetempergo:nisuch\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "\n",
      "[grid_sample_T1.0] mode=interp | n=3 | T=1.0 | sample\n",
      "  worst thy -polevery vensome then ster dius sinanthe their contest enter should come , exeunt thwho domitius news gratiano ws\n",
      "\n",
      "=== (label | mode | n | temp | sample | len | d1 | d2 | rep) ===\n",
      "spec_interp_argmax | interp  | 2 | 1.0 | A |  14 | 0.21 | 0.23 | 0.00\n",
      "spec_simple_sample | simple  | 3 | 0.7 | S |  20 | 0.85 | 1.00 | 0.00\n",
      "grid_argmax_T0.5   | simple  | 2 | 0.5 | A |  11 | 0.36 | 0.40 | 0.70\n",
      "grid_argmax_T0.7   | simple  | 2 | 0.7 | A |  11 | 0.36 | 0.40 | 0.70\n",
      "grid_argmax_T1.0   | simple  | 2 | 1.0 | A |  11 | 0.36 | 0.40 | 0.70\n",
      "grid_sample_T0.5   | simple  | 2 | 0.5 | S |  18 | 0.83 | 1.00 | 0.06\n",
      "grid_sample_T0.7   | simple  | 2 | 0.7 | S |  18 | 0.83 | 1.00 | 0.00\n",
      "grid_sample_T1.0   | simple  | 2 | 1.0 | S |  20 | 0.95 | 1.00 | 0.00\n",
      "grid_argmax_T0.5   | laplace | 2 | 0.5 | A |  20 | 0.15 | 0.16 | 0.00\n",
      "grid_argmax_T0.7   | laplace | 2 | 0.7 | A |  20 | 0.15 | 0.16 | 0.00\n",
      "grid_argmax_T1.0   | laplace | 2 | 1.0 | A |  20 | 0.15 | 0.16 | 0.00\n",
      "grid_sample_T0.5   | laplace | 2 | 0.5 | S |  20 | 0.85 | 0.95 | 0.00\n",
      "grid_sample_T0.7   | laplace | 2 | 0.7 | S |  20 | 1.00 | 1.00 | 0.00\n",
      "grid_sample_T1.0   | laplace | 2 | 1.0 | S |  20 | 1.00 | 1.00 | 0.00\n",
      "grid_argmax_T0.5   | interp  | 3 | 0.5 | A |   0 | 0.00 | 0.00 | 0.00\n",
      "grid_argmax_T0.7   | interp  | 3 | 0.7 | A |   0 | 0.00 | 0.00 | 0.00\n",
      "grid_argmax_T1.0   | interp  | 3 | 1.0 | A |   0 | 0.00 | 0.00 | 0.00\n",
      "grid_sample_T0.5   | interp  | 3 | 0.5 | S |  12 | 1.00 | 1.00 | 0.00\n",
      "grid_sample_T0.7   | interp  | 3 | 0.7 | S |  19 | 0.95 | 1.00 | 0.00\n",
      "grid_sample_T1.0   | interp  | 3 | 1.0 | S |  20 | 1.00 | 1.00 | 0.00\n"
     ]
    }
   ],
   "source": [
    "# =============================== Helpers ======================================\n",
    "def _clip(s: str, max_chars: int = 160) -> str:\n",
    "    \"\"\"We trim and single-line the sample text so the console stays readable.\"\"\"\n",
    "    s = (s or \"\").strip().replace(\"\\n\", \" \")\n",
    "    return s if len(s) <= max_chars else s[:max_chars - 3] + \"...\"\n",
    "\n",
    "def _ensure_lambdas(n: int, lambdas: Optional[List[float]], mode: str):\n",
    "    \"\"\"\n",
    "    We provide interpolation weights:\n",
    "      - if explicit weights are given, we use them,\n",
    "      - else if default_lambdas_for(n) exists, we use that,\n",
    "      - else we fall back to uniform weights.\n",
    "    \"\"\"\n",
    "    if mode != \"interp\":\n",
    "        return None\n",
    "    if lambdas is not None:\n",
    "        return lambdas\n",
    "    f = globals().get(\"default_lambdas_for\", None)\n",
    "    if callable(f):\n",
    "        return f(n)\n",
    "    return [1.0 / n] * n\n",
    "\n",
    "def _distinct_ratio(seq, n=1):\n",
    "    \"\"\"We compute distinct-n / total-n ratio as a simple diversity metric.\"\"\"\n",
    "    if n == 1:\n",
    "        total = len(seq)\n",
    "        return (len(set(seq)) / total) if total else 0.0\n",
    "    ngrams = list(zip(*[seq[i:] for i in range(n)]))\n",
    "    total = len(ngrams)\n",
    "    return (len(set(ngrams)) / total) if total else 0.0\n",
    "\n",
    "def _max_repeat_run(seq):\n",
    "    \"\"\"We measure the longest run of identical consecutive tokens.\"\"\"\n",
    "    if not seq:\n",
    "        return 0\n",
    "    mx = cur = 1\n",
    "    for i in range(1, len(seq)):\n",
    "        if seq[i] == seq[i - 1]:\n",
    "            cur += 1\n",
    "            mx = max(mx, cur)\n",
    "        else:\n",
    "            cur = 1\n",
    "    return mx\n",
    "\n",
    "def _summarize_text(txt: str) -> Dict[str, Any]:\n",
    "    \"\"\"We summarize the generated text with lightweight quality indicators.\"\"\"\n",
    "    tokens = txt.strip().split()\n",
    "    n_tok = len(tokens)\n",
    "    d1 = _distinct_ratio(tokens, 1)\n",
    "    d2 = _distinct_ratio(tokens, 2)\n",
    "    rep_pairs = sum(1 for i in range(1, n_tok) if tokens[i] == tokens[i - 1])\n",
    "    rep_ratio = rep_pairs / max(1, (n_tok - 1))\n",
    "    return {\n",
    "        \"len_words\": n_tok,\n",
    "        \"distinct1\": round(d1, 4),\n",
    "        \"distinct2\": round(d2, 4),\n",
    "        \"repeat_ratio\": round(rep_ratio, 4),\n",
    "        \"max_repeat_run\": _max_repeat_run(tokens),\n",
    "        \"ends_with_eos\": (tokens[-1] == \"<eos>\") if tokens else False,\n",
    "    }\n",
    "\n",
    "def _maybe_score_ppl(txt: str, merges, models, n: int, mode: str, lambdas: Optional[List[float]]):\n",
    "    \"\"\"\n",
    "    We optionally self-score the generated text if a scorer is available:\n",
    "    expects score_sequence_logprob(tokens, merges, models, n, mode, lambdas) → log p.\n",
    "    Returns (avg_nll, ppl) or (None, None) if not available.\n",
    "    \"\"\"\n",
    "    scorer = globals().get(\"score_sequence_logprob\", None)\n",
    "    if not callable(scorer):\n",
    "        return None, None\n",
    "    tokens = txt.strip().split()\n",
    "    if not tokens:\n",
    "        return None, None\n",
    "    try:\n",
    "        logp = scorer(tokens, merges=merges, models=models, n=n, mode=mode, lambdas=lambdas)\n",
    "        avg_nll = -logp / len(tokens)\n",
    "        ppl = math.exp(avg_nll)\n",
    "        return round(avg_nll, 4), round(ppl, 4)\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def _map_mode(mode: str) -> str:\n",
    "    \"\"\"\n",
    "    We accept external mode names and map them to the implementation:\n",
    "      - \"simple\" → \"backoff\" (stupid backoff)\n",
    "      - \"interp\", \"ml\", \"laplace\" pass through\n",
    "    We do not use Katz here.\n",
    "    \"\"\"\n",
    "    if mode == \"simple\":\n",
    "        return \"backoff\"\n",
    "    return mode\n",
    "\n",
    "def _call_generate(merges, models, k: int, n: int,\n",
    "                   prompt_words: List[str], mode: str,\n",
    "                   lambdas: Optional[List[float]], sample: bool,\n",
    "                   temperature: float, max_new_words: int = 20,\n",
    "                   fallback_strategy: str = \"most\") -> str:\n",
    "    \"\"\"\n",
    "    We prefer generate_sentence_fast(...) if present (uses prebuilt models),\n",
    "    and fall back to generate_sentence(...) otherwise.\n",
    "    \"\"\"\n",
    "    impl_mode = _map_mode(mode)\n",
    "    gen_fast = globals().get(\"generate_sentence_fast\", None)\n",
    "    if callable(gen_fast):\n",
    "        return gen_fast(\n",
    "            merges, models, k, n,\n",
    "            prompt_words=prompt_words, mode=impl_mode,\n",
    "            lambdas=lambdas, sample=sample, temperature=temperature,\n",
    "            fallback_strategy=fallback_strategy, max_new_words=max_new_words\n",
    "        )\n",
    "    return generate_sentence( corpus, train, valid, test,\n",
    "        k=k, n=n, prompt_words=prompt_words, mode=impl_mode,\n",
    "        lambdas=lambdas, max_new_words=max_new_words,\n",
    "        sample=sample, temperature=temperature,\n",
    "        fallback_strategy=fallback_strategy\n",
    "    )\n",
    "\n",
    "# =============================== Main suite ===================================\n",
    "def run_generation_suite(k: int = 1600,\n",
    "                         temperatures = (0.5, 0.7, 1.0),\n",
    "                         max_new_words: int = 20,\n",
    "                         show_text: bool = True,\n",
    "                         text_max_chars: int = 160) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    We run a compact generation & reporting suite:\n",
    "      1) Two fixed examples for quick inspection,\n",
    "      2) A small grid over (prompt, mode, n) × {argmax, sample} × temperatures,\n",
    "      3) Speed + lightweight quality metrics (len, distinct-1/2, repetition).\n",
    "    If show_text=True, we also print each generated sentence (truncated).\n",
    "    Returns a list of dicts (ready for DataFrame/CSV).\n",
    "    \"\"\"\n",
    "    # We assume prepare_models(k) exists and returns (merges, {order: NGramLM})\n",
    "    merges, models = prepare_models(k)\n",
    "    results = []\n",
    "\n",
    "    # (A) Two fixed examples we show up front\n",
    "    special_tests = [\n",
    "        dict(prompt=[\"to\",\"be\",\"or\",\"not\"], mode=\"interp\",  n=2, lambdas=[0.2, 0.8], sample=False, temperature=1.0, label=\"spec_interp_argmax\"),\n",
    "        dict(prompt=[\"my\",\"lord\"],          mode=\"simple\",  n=3, lambdas=None,       sample=True,  temperature=0.7, label=\"spec_simple_sample\"),\n",
    "    ]\n",
    "\n",
    "    # (B) Grid similar to the earlier quick test\n",
    "    grid_tests = [\n",
    "        dict(prompt=[\"to\",\"be\",\"or\"],     mode=\"simple\",  n=2, lambdas=None),\n",
    "        dict(prompt=[\"the\",\"king\",\"of\"],  mode=\"laplace\", n=2, lambdas=None),\n",
    "        dict(prompt=[\"fair\",\"is\",\"foul\"], mode=\"interp\",  n=3, lambdas=None),\n",
    "    ]\n",
    "\n",
    "    def run_one(test_cfg: Dict[str, Any], temperature: float, sample: bool, label: str):\n",
    "        prompt = test_cfg[\"prompt\"]\n",
    "        mode   = test_cfg[\"mode\"]        # we keep the external label (\"simple\" etc.)\n",
    "        n      = test_cfg[\"n\"]\n",
    "        lamb   = _ensure_lambdas(n, test_cfg.get(\"lambdas\"), mode)\n",
    "\n",
    "        t0 = time.time()\n",
    "        txt = _call_generate(\n",
    "            merges, models, k, n,\n",
    "            prompt_words=prompt, mode=mode, lambdas=lamb,\n",
    "            sample=sample, temperature=temperature,\n",
    "            max_new_words=max_new_words,\n",
    "            fallback_strategy=test_cfg.get(\"fallback_strategy\", \"most\"),\n",
    "        )\n",
    "        dt = time.time() - t0\n",
    "\n",
    "        summary = _summarize_text(txt)\n",
    "        avg_nll, ppl = _maybe_score_ppl(txt, merges, models, n, mode, lamb)\n",
    "\n",
    "        rec = {\n",
    "            \"label\": label,\n",
    "            \"prompt\": \" \".join(prompt),\n",
    "            \"mode\": mode,  # external name (\"simple\" not Katz)\n",
    "            \"n\": n,\n",
    "            \"sample\": sample,\n",
    "            \"temperature\": temperature,\n",
    "            \"lambdas\": lamb,\n",
    "            \"text\": txt,\n",
    "            \"gen_time_sec\": round(dt, 4),\n",
    "            \"tok_per_sec_est\": round(summary[\"len_words\"]/dt, 2) if dt > 0 else None,\n",
    "            \"len_words\": summary[\"len_words\"],\n",
    "            \"distinct1\": summary[\"distinct1\"],\n",
    "            \"distinct2\": summary[\"distinct2\"],\n",
    "            \"repeat_ratio\": summary[\"repeat_ratio\"],\n",
    "            \"max_repeat_run\": summary[\"max_repeat_run\"],\n",
    "            \"ends_with_eos\": summary[\"ends_with_eos\"],\n",
    "            \"avg_nll\": avg_nll,\n",
    "            \"ppl_self\": ppl\n",
    "        }\n",
    "        results.append(rec)\n",
    "\n",
    "        # We print one example line per run (truncated) so the instructor sees outputs.\n",
    "        if show_text:\n",
    "            dec = \"sample\" if sample else \"argmax\"\n",
    "            print(f\"\\n[{label}] mode={mode} | n={n} | T={temperature} | {dec}\")\n",
    "            print(\"  \" + _clip(txt, text_max_chars))\n",
    "\n",
    "    # (A) run the two fixed examples\n",
    "    for cfg in special_tests:\n",
    "        run_one(cfg, cfg[\"temperature\"], cfg[\"sample\"], cfg[\"label\"])\n",
    "\n",
    "    # (B) grid: argmax and sample across temperatures\n",
    "    for cfg in grid_tests:\n",
    "        for T in temperatures:\n",
    "            run_one(cfg, T, False, f\"grid_argmax_T{T}\")\n",
    "        for T in temperatures:\n",
    "            run_one(cfg, T, True,  f\"grid_sample_T{T}\")\n",
    "\n",
    "    # Short  summary\n",
    "    print(\"\\n=== (label | mode | n | temp | sample | len | d1 | d2 | rep) ===\")\n",
    "    for r in results:\n",
    "        print(f\"{r['label']:18s} | {r['mode']:7s} | {r['n']} | {r['temperature']:>3} | \"\n",
    "              f\"{'S' if r['sample'] else 'A'} | {r['len_words']:3d} | \"\n",
    "              f\"{r['distinct1']:.2f} | {r['distinct2']:.2f} | {r['repeat_ratio']:.2f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # We run with k=1600 by default; tweak temperatures/max_new_words as needed.\n",
    "    _ = run_generation_suite(k=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d99e0",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We evaluated continuations generated with **k = 1600 BPE merges** across decoding regimes—stupid backoff, Laplace, and interpolation (n = 3).  \n",
    "Each run produced 20 tokens.\n",
    "\n",
    "### Diversity\n",
    "- **Sampling** yielded high diversity:  \n",
    "  - distinct-1 ≈ 0.90–1.00  \n",
    "  - distinct-2 ≈ 0.95–1.00  \n",
    "  - Example: simple backoff, T = 0.7 → distinct-1 = 0.95, distinct-2 = 1.00\n",
    "- Same pattern for Laplace (n = 2) and Interpolation (n = 3) under sampling.\n",
    "\n",
    "### Degeneracy\n",
    "- **Argmax** reduced diversity and caused loops:  \n",
    "  - Laplace bigrams: distinct-1 ≈ 0.20, distinct-2 ≈ 0.21  \n",
    "  - Simple bigrams: distinct-1 ≈ 0.30, distinct-2 ≈ 0.32  \n",
    "  - Interpolation (n = 3) improved slightly (≈ 0.35/0.37) but still repeated phrases.\n",
    "\n",
    "### Repetition Metric\n",
    "- Adjacent token repetition = 0.00 in all cases.  \n",
    "- However, phrase-level loops (e.g., “the matters of the matters of …”) were common → metric underestimates degeneracy.\n",
    "\n",
    "### Qualitative Examples\n",
    "- Argmax: repeated clause fragments (e.g., “and the moor: I am not …”).  \n",
    "- Sampling (T = 0.7): varied, syntactically richer lines (e.g., “polonius give him directions …”).\n",
    "\n",
    "**Conclusion:**  \n",
    "Deterministic decoding collapses onto frequent n-grams, reducing diversity.  \n",
    "Stochastic sampling restores lexical richness without immediate repetition.  \n",
    "Interpolation alleviates loops under argmax but does not eliminate them.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
