{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc25d52",
   "metadata": {},
   "source": [
    "## Task 4:\n",
    "Task remarks: GPT – Hand in, until 31.08.\n",
    "\n",
    "* If something is underspecified, just make decision yourself\n",
    "* Well-documented code\n",
    "* Submission format\n",
    "    * Notebook (incl. pdf) or GitHub readme (submit pdf with link to repo) as technical\n",
    "report of what we did\n",
    "        * Nice narrative and way to navigate code, not scientific paper\n",
    "        * Include plots (loss, perplexity scores, hyperparameters, etc.)\n",
    "        * Optional include pseudocode\n",
    "        * Qualitative analysis nice to have, e.g, add and evaluate generated text in\n",
    "report\n",
    "        * Can add appendix for additional plots\n",
    "* Hand in every mile stone, starting from UNIX comments\n",
    "* Removed in-between milestone of causal-self attention\n",
    "* Everything together in one file\n",
    "* Compare the models from each milestone, report perplexity for all\n",
    "    * Old-school n-gram\n",
    "    * Best neural n-gram\n",
    "    * GPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31503ede",
   "metadata": {},
   "source": [
    "**GPT itself**\n",
    "* Hyperparameter tuning: do not need all of them, choose what is most interesting and\n",
    "explain why\n",
    "    * Number of merges in BPE (not complete gridsearch, isolate top three number of\n",
    "merges in perplexity in n-gram, test those for GPT)\n",
    "    * Regularisation\n",
    "    * How small can we make neural embedding\n",
    "    * Do not change optimiser\n",
    "* General remarks\n",
    "    * Transformer blocks from scratch would be beyond 1.0, not required\n",
    "    * Implement causal self-attention yourself, do not use ready-made PyTorch version\n",
    "    * For computing perplexity: Implementing teacher forcing annealing is necessary\n",
    "for good generation performance, but we don’t have to do it for our assignment\n",
    "* Reminders\n",
    "    * Skip weight initialisation and optimiser configuration\n",
    "        * Can use standard PyTorch initialisation → just get transformer\n",
    "parameters and add them when initialising the optimiser\n",
    "    * Remember to change device selection, currently “cuda”, you might want “mps” or\n",
    "“cpu”\n",
    "    * Configs: make n_embd smaller, don’t change betas and weight decay (unless\n",
    "you want to), can change batch size, chunk size, n_head, n_layer\n",
    "    * Specify temperature and top-k parameters for generate function\n",
    "    * Activation function used in MLP: not ReLU as in slides but GELU (might not be in\n",
    "PyTorch yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438c661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "[Save] Encoded splits to runs\\gpt_20250831_200113_k1000\n",
      "[Info] Vocab size = 1028\n",
      "[Info] Device: cpu | Parameters: 1.06M | Vocab=1028 | block=64\n",
      "[Step    50] train_loss=6.2732 (ema 6.4469)\n",
      "[Step   100] train_loss=5.5120 (ema 5.6544)\n",
      "[Step   150] train_loss=5.0238 (ema 5.0975)\n",
      "[Step   200] train_loss=4.6748 (ema 4.8054)\n",
      "[Step   250] train_loss=4.6635 (ema 4.6314)\n",
      "[Step   300] train_loss=4.5805 (ema 4.5115)\n",
      "[Step   350] train_loss=4.3110 (ema 4.4233)\n",
      "[Step   400] train_loss=4.2795 (ema 4.3166)\n",
      "[Step   450] train_loss=4.2358 (ema 4.2783)\n",
      "[Step   500] train_loss=4.2132 (ema 4.2182)\n",
      "[Eval    500] val_loss=4.1863 | val_ppl=65.78\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_200113_k1000\\samples/step500_eval.txt\n",
      "[Step   550] train_loss=3.9659 (ema 4.1180)\n",
      "[Step   600] train_loss=4.0263 (ema 4.0882)\n",
      "[Step   650] train_loss=4.0694 (ema 4.0503)\n",
      "[Step   700] train_loss=3.9535 (ema 4.0074)\n",
      "[Step   750] train_loss=3.9800 (ema 3.9815)\n",
      "[Step   800] train_loss=3.9881 (ema 3.9534)\n",
      "[Step   850] train_loss=3.8205 (ema 3.8903)\n",
      "[Step   900] train_loss=3.8852 (ema 3.8658)\n",
      "[Step   950] train_loss=3.8921 (ema 3.8226)\n",
      "[Step  1000] train_loss=3.8791 (ema 3.8190)\n",
      "[Eval   1000] val_loss=3.8693 | val_ppl=47.91\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_200113_k1000\\samples/step1000_eval.txt\n",
      "[Save] checkpoint → runs\\gpt_20250831_200113_k1000\\ckpt_step1000.pt\n",
      "[Step  1050] train_loss=3.7260 (ema 3.7754)\n",
      "[Step  1100] train_loss=3.6583 (ema 3.7399)\n",
      "[Step  1150] train_loss=3.5604 (ema 3.7176)\n",
      "[Step  1200] train_loss=3.6008 (ema 3.6766)\n",
      "[Step  1250] train_loss=3.7762 (ema 3.6666)\n",
      "[Step  1300] train_loss=3.6804 (ema 3.6456)\n",
      "[Step  1350] train_loss=3.5278 (ema 3.6027)\n",
      "[Step  1400] train_loss=3.5224 (ema 3.5762)\n",
      "[Step  1450] train_loss=3.4798 (ema 3.5683)\n",
      "[Step  1500] train_loss=3.5525 (ema 3.5541)\n",
      "[Eval   1500] val_loss=3.6855 | val_ppl=39.87\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_200113_k1000\\samples/step1500_eval.txt\n",
      "[Step  1550] train_loss=3.6582 (ema 3.5508)\n",
      "[Step  1600] train_loss=3.4473 (ema 3.5079)\n",
      "[Step  1650] train_loss=3.4769 (ema 3.5005)\n",
      "[Step  1700] train_loss=3.4032 (ema 3.4892)\n",
      "[Step  1750] train_loss=3.4384 (ema 3.4447)\n",
      "[Step  1800] train_loss=3.5972 (ema 3.4654)\n",
      "[Step  1850] train_loss=3.4198 (ema 3.4395)\n",
      "[Step  1900] train_loss=3.4154 (ema 3.4089)\n",
      "[Step  1950] train_loss=3.2983 (ema 3.3970)\n",
      "[Step  2000] train_loss=3.4138 (ema 3.3859)\n",
      "[Eval   2000] val_loss=3.5676 | val_ppl=35.43\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_200113_k1000\\samples/step2000_eval.txt\n",
      "[Save] checkpoint → runs\\gpt_20250831_200113_k1000\\ckpt_step2000.pt\n",
      "[Step  2050] train_loss=3.3200 (ema 3.3951)\n",
      "[Step  2100] train_loss=3.5034 (ema 3.3775)\n",
      "[Step  2150] train_loss=3.3289 (ema 3.3537)\n",
      "[Step  2200] train_loss=3.3169 (ema 3.3333)\n",
      "[Step  2250] train_loss=3.3355 (ema 3.3413)\n",
      "[Step  2300] train_loss=3.2337 (ema 3.2982)\n",
      "[Step  2350] train_loss=3.2906 (ema 3.2944)\n",
      "[Step  2400] train_loss=3.1011 (ema 3.2507)\n",
      "[Step  2450] train_loss=3.2346 (ema 3.2688)\n",
      "[Step  2500] train_loss=3.3762 (ema 3.2910)\n",
      "[Eval   2500] val_loss=3.4910 | val_ppl=32.82\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_200113_k1000\\samples/step2500_eval.txt\n",
      "[Step  2550] train_loss=3.3714 (ema 3.2681)\n",
      "[Step  2600] train_loss=3.2862 (ema 3.2456)\n",
      "[Step  2650] train_loss=3.1462 (ema 3.2142)\n",
      "[Step  2700] train_loss=3.0784 (ema 3.1907)\n",
      "[Step  2750] train_loss=3.1191 (ema 3.1934)\n",
      "[Step  2800] train_loss=3.2619 (ema 3.1888)\n",
      "[Step  2850] train_loss=3.2523 (ema 3.1926)\n",
      "[Step  2900] train_loss=3.1694 (ema 3.1687)\n",
      "[Step  2950] train_loss=3.0445 (ema 3.1470)\n",
      "[Step  3000] train_loss=3.2126 (ema 3.1612)\n",
      "[Eval   3000] val_loss=3.4194 | val_ppl=30.55\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_200113_k1000\\samples/step3000_eval.txt\n",
      "[Save] checkpoint → runs\\gpt_20250831_200113_k1000\\ckpt_step3000.pt\n",
      "[Step  3050] train_loss=3.1139 (ema 3.1285)\n",
      "[Step  3100] train_loss=3.0938 (ema 3.1326)\n",
      "[Step  3150] train_loss=3.0510 (ema 3.0801)\n",
      "[Step  3200] train_loss=3.0286 (ema 3.1004)\n",
      "[Step  3250] train_loss=3.1102 (ema 3.0844)\n",
      "[Step  3300] train_loss=3.0084 (ema 3.0528)\n",
      "[Step  3350] train_loss=3.0839 (ema 3.0490)\n",
      "[Step  3400] train_loss=3.1420 (ema 3.0619)\n",
      "[Step  3450] train_loss=3.1107 (ema 3.0559)\n",
      "[Step  3500] train_loss=2.9219 (ema 2.9864)\n",
      "[Eval   3500] val_loss=3.4012 | val_ppl=30.00\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_200113_k1000\\samples/step3500_eval.txt\n",
      "[Step  3550] train_loss=3.1051 (ema 3.0333)\n",
      "[Step  3600] train_loss=3.1492 (ema 3.0443)\n",
      "[Step  3650] train_loss=2.8934 (ema 2.9861)\n",
      "[Step  3700] train_loss=2.9888 (ema 2.9987)\n",
      "[Step  3750] train_loss=2.9285 (ema 2.9866)\n",
      "[Step  3800] train_loss=2.8672 (ema 2.9586)\n",
      "[Step  3850] train_loss=2.9251 (ema 2.9436)\n",
      "[Step  3900] train_loss=2.9374 (ema 2.9355)\n",
      "[Step  3950] train_loss=2.9311 (ema 2.9508)\n",
      "[Step  4000] train_loss=2.8458 (ema 2.8932)\n",
      "[Eval   4000] val_loss=3.3472 | val_ppl=28.42\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_200113_k1000\\samples/step4000_eval.txt\n",
      "[Save] checkpoint → runs\\gpt_20250831_200113_k1000\\ckpt_step4000.pt\n",
      "[Step  4050] train_loss=2.9635 (ema 2.9298)\n",
      "[Step  4100] train_loss=2.8158 (ema 2.8989)\n",
      "[Step  4150] train_loss=2.8464 (ema 2.8704)\n",
      "[Step  4200] train_loss=2.9463 (ema 2.9108)\n",
      "[Step  4250] train_loss=2.8283 (ema 2.8892)\n",
      "[Step  4300] train_loss=2.7177 (ema 2.8692)\n",
      "[Step  4350] train_loss=2.8379 (ema 2.8778)\n",
      "[Step  4400] train_loss=2.8460 (ema 2.8231)\n",
      "[Step  4450] train_loss=2.7566 (ema 2.8351)\n",
      "[Step  4500] train_loss=2.7013 (ema 2.7800)\n",
      "[Eval   4500] val_loss=3.3521 | val_ppl=28.56\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_200113_k1000\\samples/step4500_eval.txt\n",
      "[Step  4550] train_loss=2.8130 (ema 2.8019)\n",
      "[Step  4600] train_loss=2.8065 (ema 2.8042)\n",
      "[Step  4650] train_loss=2.7607 (ema 2.7739)\n",
      "[Step  4700] train_loss=2.8892 (ema 2.8120)\n",
      "[Step  4750] train_loss=2.8554 (ema 2.7709)\n",
      "[Step  4800] train_loss=2.6324 (ema 2.7627)\n",
      "[Step  4850] train_loss=2.7616 (ema 2.7459)\n",
      "[Step  4900] train_loss=2.7412 (ema 2.7368)\n",
      "[Step  4950] train_loss=2.6774 (ema 2.7063)\n",
      "[Step  5000] train_loss=2.6862 (ema 2.7299)\n",
      "[Eval   5000] val_loss=3.3896 | val_ppl=29.65\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_200113_k1000\\samples/step5000_eval.txt\n",
      "[Save] checkpoint → runs\\gpt_20250831_200113_k1000\\ckpt_step5000.pt\n",
      "[Final Val] loss=3.3435 | ppl=28.32\n",
      "[Final Test] loss=3.4472 | ppl=31.41\n",
      "[Plot Warning] Could not generate plot: 'step'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GPT (Transformer) training pipeline for Shakespeare with BPE tokenizer\n",
    "====================================================================\n",
    "\n",
    "Goals\n",
    "-----\n",
    "- PyTorch implementation of a small GPT (nanoGPT-style) with clean structure.\n",
    "- Re-use your existing BPE merges and token conventions (</w>, <bos>, <eos>).\n",
    "- Detailed logging, file outputs, checkpoints, CSV logs, and PNG loss plots.\n",
    "- Validation + test perplexity.\n",
    "- Sample text generation at checkpoints.\n",
    "\n",
    "Directory layout (inputs & outputs)\n",
    "----------------------------------\n",
    "Inputs (must exist):\n",
    "- Corpus/\n",
    "    Shakespeare_clean_train.txt\n",
    "    Shakespeare_clean_valid.txt\n",
    "    Shakespeare_clean_test.txt\n",
    "- Generated_tokens/\n",
    "    (one of) bpe_merges with k = {k}.txt, standard_bpe_merges_k{k}.txt, ...\n",
    "\n",
    "Outputs (this script will create):\n",
    "- runs/gpt_{timestamp}_k{k}/\n",
    "    config.json\n",
    "    tokenizer.json\n",
    "    train_encoded.pt, valid_encoded.pt, test_encoded.pt\n",
    "    logs.csv\n",
    "    loss_plot.png\n",
    "    ckpt_step{...}.pt (model + optimizer + scaler + config + samples)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    /\n",
    "        step{...}_sample.txt\n",
    "\n",
    "Usage\n",
    "-----\n",
    "python gpt_shakespeare_trainer.py --k 1600 --batch_size 64 --block_size 128 --n_layer 4 --n_head 4 --n_embd 256 --max_steps 2000\n",
    "\n",
    "Notes\n",
    "-----\n",
    "- Designed for small models/datasets. Mixed precision is optional (amp).\n",
    "- If no CUDA, training runs on CPU (slower but fine for tiny configs).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import sys\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Tuple, Dict, Iterable, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # headless\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"  # Matplotlib’s default bundled font\n",
    "\n",
    "# ================================ Constants ================================\n",
    "CORPUS_DIR = \"Corpus\"\n",
    "GENERATED_DIR = \"Generated_tokens\"\n",
    "WORD_END = \"</w>\"\n",
    "EOS = \"<eos>\"\n",
    "BOS = \"<bos>\"\n",
    "_wsre = re.compile(r\"\\s+\")\n",
    "#random.seed(42)\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    seed: int = 42\n",
    "    k: int = 1000\n",
    "    batch_size: int = 32\n",
    "    block_size: int = 128\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 128\n",
    "    dropout: float = 0.1\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.0\n",
    "    max_steps: int = 1000\n",
    "    eval_interval: int = 200\n",
    "    eval_batches: int = 20\n",
    "    ckpt_interval: int = 500\n",
    "    warmup_steps: int = 100\n",
    "    grad_clip: float = 1.0\n",
    "    amp: bool = True\n",
    "    no_amp: bool = False \n",
    "\n",
    "\n",
    "def parse_args() -> TrainConfig:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--k\", type=int, default=1000)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--block_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--n_layer\", type=int, default=4)\n",
    "    parser.add_argument(\"--n_head\", type=int, default=4)\n",
    "    parser.add_argument(\"--n_embd\", type=int, default=128)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--lr\", type=float, default=3e-4)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-2)\n",
    "    parser.add_argument(\"--max_steps\", type=int, default=5000)\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=100)\n",
    "    parser.add_argument(\"--grad_clip\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--eval_interval\", type=int, default=500)\n",
    "    parser.add_argument(\"--eval_batches\", type=int, default=20)\n",
    "    parser.add_argument(\"--ckpt_interval\", type=int, default=1000)\n",
    "    parser.add_argument(\"--no_amp\", action=\"store_true\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "\n",
    "    if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "        args, _ = parser.parse_known_args()\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    return TrainConfig(**vars(args))\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Core training params\n",
    "    parser.add_argument(\"--k\", type=int, default=1000)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--block_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--n_layer\", type=int, default=4)\n",
    "    parser.add_argument(\"--n_head\", type=int, default=4)\n",
    "    parser.add_argument(\"--n_embd\", type=int, default=128)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "\n",
    "    # Optimization\n",
    "    parser.add_argument(\"--lr\", type=float, default=3e-4)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-2)\n",
    "    parser.add_argument(\"--max_steps\", type=int, default=5000)\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=100)\n",
    "    parser.add_argument(\"--grad_clip\", type=float, default=1.0)\n",
    "\n",
    "    # Evaluation/checkpoints\n",
    "    parser.add_argument(\"--eval_interval\", type=int, default=500)\n",
    "    parser.add_argument(\"--eval_batches\", type=int, default=20)\n",
    "    parser.add_argument(\"--ckpt_interval\", type=int, default=1000)\n",
    "\n",
    "    # Misc\n",
    "    parser.add_argument(\"--no_amp\", action=\"store_true\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "\n",
    "    # Use parse_known_args to ignore --f=...json\n",
    "    if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "        args, _ = parser.parse_known_args()\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "# ============================= BPE Tokenizer ===============================\n",
    "\n",
    "def find_merges_file(k: int, verbose: bool = True) -> str:\n",
    "    candidates = [\n",
    "        os.path.join(GENERATED_DIR, f\"bpe_merges with k = {k}.txt\"),\n",
    "        os.path.join(GENERATED_DIR, f\"standard_bpe_merges_k{k}.txt\"),\n",
    "        os.path.join(GENERATED_DIR, f\"aggressive_clean_bpe_merges_k{k}.txt\"),\n",
    "        os.path.join(GENERATED_DIR, f\"bpe_merges_k{k}.txt\"),\n",
    "        os.path.join(GENERATED_DIR, f\"bpe_merges_k{k}_webtext_clean.txt\"),\n",
    "    ]\n",
    "    for path in candidates:\n",
    "        if os.path.exists(path):\n",
    "            if verbose:\n",
    "                print(f\"[Found] Using merges file: {path}\")\n",
    "            return path\n",
    "    raise FileNotFoundError(f\"No merges file found for k={k}. Tried: {candidates}\")\n",
    "\n",
    "def load_merges(merges_path: str) -> List[Tuple[str, str]]:\n",
    "    merges = []\n",
    "    with open(merges_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                merges.append((parts[0], parts[1]))\n",
    "    return merges\n",
    "\n",
    "def words_from_text(text: str, lowercase: bool = True) -> List[str]:\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    return [w for w in _wsre.split(text.strip()) if w]\n",
    "\n",
    "def apply_merges_to_word(word: str, merges: List[Tuple[str, str]]) -> List[str]:\n",
    "    symbols = tuple(list(word) + [WORD_END])\n",
    "    for a, b in merges:\n",
    "        out = []\n",
    "        i, L = 0, len(symbols)\n",
    "        while i < L:\n",
    "            if i < L-1 and symbols[i] == a and symbols[i+1] == b:\n",
    "                out.append(a + b); i += 2\n",
    "            else:\n",
    "                out.append(symbols[i]); i += 1\n",
    "        symbols = tuple(out)\n",
    "    return list(symbols)\n",
    "\n",
    "def tokenize_lines_with_merges(text: str, merges: List[Tuple[str, str]]) -> List[List[str]]:\n",
    "    token_lines: List[List[str]] = []\n",
    "    for line in text.strip().splitlines():\n",
    "        words = words_from_text(line)\n",
    "        if not words:\n",
    "            continue\n",
    "        toks: List[str] = []\n",
    "        for w in words:\n",
    "            toks.extend(apply_merges_to_word(w, merges))\n",
    "        toks.append(EOS)\n",
    "        token_lines.append(toks)\n",
    "    return token_lines\n",
    "\n",
    "# Convert tokens to ids, build vocab\n",
    "class BPETokenizer:\n",
    "    def __init__(self, merges: List[Tuple[str, str]], extra_tokens: Optional[List[str]] = None):\n",
    "        self.merges = merges\n",
    "        self.extra_tokens = extra_tokens or []\n",
    "        self.token_to_id: Dict[str, int] = {}\n",
    "        self.id_to_token: List[str] = []\n",
    "\n",
    "    def build_vocab_from_texts(self, texts: Dict[str, str]):\n",
    "        vocab = set()\n",
    "        for name, txt in texts.items():\n",
    "            for line in tokenize_lines_with_merges(txt, self.merges):\n",
    "                vocab.update(line)\n",
    "        vocab.update(self.extra_tokens)\n",
    "        # Deterministic order\n",
    "        self.id_to_token = sorted(vocab)\n",
    "        self.token_to_id = {t: i for i, t in enumerate(self.id_to_token)}\n",
    "\n",
    "    def encode_words(self, words: Iterable[str]) -> List[str]:\n",
    "        toks: List[str] = []\n",
    "        for w in (w.lower() for w in words):\n",
    "            toks.extend(apply_merges_to_word(w, self.merges))\n",
    "        return toks\n",
    "\n",
    "    def encode_lines(self, text: str) -> List[List[int]]:\n",
    "        lines_tok = tokenize_lines_with_merges(text, self.merges)\n",
    "        ids_lines: List[List[int]] = []\n",
    "        for line in lines_tok:\n",
    "            ids_lines.append([self.token_to_id[t] for t in line if t in self.token_to_id])\n",
    "        return ids_lines\n",
    "\n",
    "    def decode_tokens(self, token_stream: List[str]) -> List[str]:\n",
    "        words: List[str] = []\n",
    "        buf: List[str] = []\n",
    "        for t in token_stream:\n",
    "            if t == EOS:\n",
    "                break\n",
    "            buf.append(t)\n",
    "            if t.endswith(WORD_END):\n",
    "                chars: List[str] = []\n",
    "                for sub in buf:\n",
    "                    if sub.endswith(WORD_END):\n",
    "                        chars.extend(list(sub[:-len(WORD_END)]))\n",
    "                    else:\n",
    "                        chars.extend(list(sub))\n",
    "                words.append(\"\".join(chars))\n",
    "                buf = []\n",
    "        if buf:\n",
    "            chars = []\n",
    "            for sub in buf:\n",
    "                if sub.endswith(WORD_END):\n",
    "                    chars.extend(list(sub[:-len(WORD_END)]))\n",
    "                else:\n",
    "                    chars.extend(list(sub))\n",
    "            if chars:\n",
    "                words.append(\"\".join(chars))\n",
    "        return words\n",
    "\n",
    "    def save(self, path: str):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"id_to_token\": self.id_to_token,\n",
    "                \"token_to_id\": self.token_to_id,\n",
    "                \"extra_tokens\": self.extra_tokens,\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: str) -> \"BPETokenizer\":\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            obj = json.load(f)\n",
    "        tok = BPETokenizer(merges=[], extra_tokens=obj.get(\"extra_tokens\", []))\n",
    "        tok.id_to_token = obj[\"id_to_token\"]\n",
    "        tok.token_to_id = {k: int(v) for k, v in obj[\"token_to_id\"].items()}\n",
    "        return tok\n",
    "\n",
    "# ============================ Data preparation ==============================\n",
    "\n",
    "def read_text(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "@dataclass\n",
    "class EncodedSplits:\n",
    "    train: torch.Tensor\n",
    "    valid: torch.Tensor\n",
    "    test: torch.Tensor\n",
    "\n",
    "\n",
    "def build_or_load_encoded(run_dir: str, k: int) -> Tuple[BPETokenizer, EncodedSplits]:\n",
    "    enc_train_path = os.path.join(run_dir, \"train_encoded.pt\")\n",
    "    enc_valid_path = os.path.join(run_dir, \"valid_encoded.pt\")\n",
    "    enc_test_path  = os.path.join(run_dir, \"test_encoded.pt\")\n",
    "    tok_path       = os.path.join(run_dir, \"tokenizer.json\")\n",
    "\n",
    "    if all(os.path.exists(p) for p in [enc_train_path, enc_valid_path, enc_test_path, tok_path]):\n",
    "        print(\"[Load] Using cached encoded splits + tokenizer\")\n",
    "        tokenizer = BPETokenizer.load(tok_path)\n",
    "        train_ids = torch.load(enc_train_path)\n",
    "        valid_ids = torch.load(enc_valid_path)\n",
    "        test_ids  = torch.load(enc_test_path)\n",
    "        return tokenizer, EncodedSplits(train_ids, valid_ids, test_ids)\n",
    "\n",
    "    # Build from raw\n",
    "    merges_path = find_merges_file(k, verbose=True)\n",
    "    merges = load_merges(merges_path)\n",
    "\n",
    "    train_txt = read_text(os.path.join(CORPUS_DIR, \"Shakespeare_clean_train.txt\"))\n",
    "    valid_txt = read_text(os.path.join(CORPUS_DIR, \"Shakespeare_clean_valid.txt\"))\n",
    "    test_txt  = read_text(os.path.join(CORPUS_DIR, \"Shakespeare_clean_test.txt\"))\n",
    "\n",
    "    tokenizer = BPETokenizer(merges=merges, extra_tokens=[BOS, EOS])\n",
    "    tokenizer.build_vocab_from_texts({\"train\": train_txt, \"valid\": valid_txt, \"test\": test_txt})\n",
    "\n",
    "    def flatten(lines: List[List[int]]) -> List[int]:\n",
    "        flat = []\n",
    "        for ln in lines: flat.extend(ln)\n",
    "        return flat\n",
    "\n",
    "    train_ids = torch.tensor(flatten(tokenizer.encode_lines(train_txt)), dtype=torch.long)\n",
    "    valid_ids = torch.tensor(flatten(tokenizer.encode_lines(valid_txt)), dtype=torch.long)\n",
    "    test_ids  = torch.tensor(flatten(tokenizer.encode_lines(test_txt)),  dtype=torch.long)\n",
    "\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    tokenizer.save(tok_path)\n",
    "    torch.save(train_ids, enc_train_path)\n",
    "    torch.save(valid_ids, enc_valid_path)\n",
    "    torch.save(test_ids,  enc_test_path)\n",
    "    print(f\"[Save] Encoded splits to {run_dir}\")\n",
    "    print(f\"[Info] Vocab size = {len(tokenizer.id_to_token)}\")\n",
    "    return tokenizer, EncodedSplits(train_ids, valid_ids, test_ids)\n",
    "\n",
    "# ================================ Dataset ==================================\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, ids: torch.Tensor, block_size: int):\n",
    "        self.ids = ids\n",
    "        self.block_size = block_size\n",
    "        # we will sample random start positions in __getitem__\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # approximate number of sequences of length block_size we can draw\n",
    "        return max(1, len(self.ids) - self.block_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ignore idx and sample randomly to add stochasticity\n",
    "        i = random.randint(0, len(self.ids) - self.block_size - 1)\n",
    "        x = self.ids[i : i + self.block_size]\n",
    "        y = self.ids[i + 1 : i + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "# ============================== GPT Model ==================================\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd: int, n_head: int, block_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.key = nn.Linear(n_embd, n_embd)\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        self.value = nn.Linear(n_embd, n_embd)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        # causal mask\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(k.size(-1))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v  # (B, n_head, T, head_dim)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd: int, n_head: int, block_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = CausalSelfAttention(n_embd, n_head, block_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_layer: int, n_head: int, n_embd: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte': nn.Embedding(vocab_size, n_embd),\n",
    "            'wpe': nn.Embedding(block_size, n_embd),\n",
    "            'drop': nn.Dropout(dropout),\n",
    "            'h': nn.ModuleList([Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)]),\n",
    "            'ln_f': nn.LayerNorm(n_embd),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.block_size\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        tok_emb = self.transformer['wte'](idx)\n",
    "        pos_emb = self.transformer['wpe'](pos)[None, :, :]\n",
    "        x = self.transformer['drop'](tok_emb + pos_emb)\n",
    "        for block in self.transformer['h']:\n",
    "            x = block(x)\n",
    "        x = self.transformer['ln_f'](x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "# ============================ Training utilities ============================\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# ============================== Evaluation ==================================\n",
    "\n",
    "def evaluate(model: GPT, loader: DataLoader, device: torch.device, max_batches: int) -> float:\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            if i >= max_batches:\n",
    "                break\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            losses.append(loss.item())\n",
    "    model.train()\n",
    "    return float(sum(losses) / max(1, len(losses)))\n",
    "\n",
    "# ============================== Generation ==================================\n",
    "\n",
    "def generate(model: GPT, start_tokens: List[int], max_new_tokens: int, temperature: float = 1.0, top_k: Optional[int] = None) -> List[int]:\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    idx = torch.tensor(start_tokens, dtype=torch.long, device=device)[None, :]\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -model.block_size:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / max(1e-8, temperature)\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, next_id), dim=1)\n",
    "    return idx[0].tolist()\n",
    "\n",
    "# ============================== Plot / CSV ==================================\n",
    "\n",
    "def save_plot_and_csv(run_dir, history):\n",
    "    \"\"\"Save training/validation loss plot and history CSV.\"\"\"\n",
    "    # make sure run_dir and subdirs exist\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(run_dir, \"samples\"), exist_ok=True)\n",
    "\n",
    "    csv_path = os.path.join(run_dir, \"history.csv\")\n",
    "    png_path = os.path.join(run_dir, \"history.png\")\n",
    "\n",
    "    # Save CSV\n",
    "    df = pd.DataFrame(history)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    # Try plotting\n",
    "    try:\n",
    "        # Force matplotlib to use a safe font\n",
    "        matplotlib.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "\n",
    "        steps = [h[\"step\"] for h in history]\n",
    "        train_loss = [h[\"train_loss\"] for h in history]\n",
    "        val_loss = [h[\"val_loss\"] for h in history]\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(steps, train_loss, label=\"train_loss\")\n",
    "        plt.plot(steps, val_loss, label=\"val_loss\")\n",
    "        plt.xlabel(\"step\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(png_path)\n",
    "        plt.close()\n",
    "        print(f\"[Plot] Saved to {png_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Plot Warning] Could not generate plot: {e}\")\n",
    "\n",
    "\n",
    "# ============================== Main training ===============================\n",
    "\n",
    "def train_and_eval_with_logging(cfg: TrainConfig):\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(\"runs\", f\"gpt_{ts}_k{cfg.k}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(run_dir, \"samples\"), exist_ok=True)\n",
    "\n",
    "    # Save config\n",
    "    with open(os.path.join(run_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(asdict(cfg), f, indent=2)\n",
    "\n",
    "    # Encode or load\n",
    "    tokenizer, splits = build_or_load_encoded(run_dir, cfg.k)\n",
    "    vocab_size = len(tokenizer.id_to_token)\n",
    "\n",
    "    # Datasets/loaders\n",
    "    train_ds = GPTDataset(splits.train, cfg.block_size)\n",
    "    valid_ds = GPTDataset(splits.valid, cfg.block_size)\n",
    "    test_ds  = GPTDataset(splits.test,  cfg.block_size)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=cfg.batch_size, shuffle=False, drop_last=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    # Model\n",
    "    model = GPT(vocab_size=vocab_size, block_size=cfg.block_size, n_layer=cfg.n_layer, n_head=cfg.n_head, n_embd=cfg.n_embd, dropout=cfg.dropout)\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"[Info] Device: {device} | Parameters: {count_parameters(model)/1e6:.2f}M | Vocab={vocab_size} | block={cfg.block_size}\")\n",
    "\n",
    "    # Optimizer / Scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    lr_sched = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: min(1.0, step / max(1, cfg.warmup_steps))\n",
    "    )\n",
    "\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=cfg.amp and device.type == \"cuda\")\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = []\n",
    "    running_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    step = 0\n",
    "    while step < cfg.max_steps:\n",
    "        for xb, yb in train_loader:\n",
    "            step += 1\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\", enabled=scaler.is_enabled()):\n",
    "                logits = model(xb)\n",
    "                loss = loss_fn(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            if cfg.grad_clip:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            lr_sched.step()\n",
    "\n",
    "            running_loss = 0.9 * running_loss + 0.1 * loss.item() if step > 1 else loss.item()\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                print(f\"[Step {step:5d}] train_loss={loss.item():.4f} (ema {running_loss:.4f})\")\n",
    "\n",
    "\n",
    "            # Eval\n",
    "            if step % cfg.eval_interval == 0 or step == cfg.max_steps:\n",
    "                val_loss = evaluate(model, valid_loader, device, cfg.eval_batches)\n",
    "                val_ppl = math.exp(val_loss)\n",
    "                print(f\"[Eval  {step:5d}] val_loss={val_loss:.4f} | val_ppl={val_ppl:.2f}\")\n",
    "                history.append({\"step\": step, \"train_loss\": running_loss, \"val_loss\": val_loss, \"val_ppl\": val_ppl})\n",
    "                save_plot_and_csv(run_dir, history)\n",
    "\n",
    "                # save extrinsic evaluation with fixed prompts\n",
    "                prompts = [\"To be or not to be\", \"Once upon a midnight dreary\"]\n",
    "                eval_path = os.path.join(run_dir, f\"samples/step{step}_eval.txt\")\n",
    "                with open(eval_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    for prompt in prompts:\n",
    "                        start_ids = tokenizer.encode_words(prompt.split())\n",
    "                        start_ids = [tokenizer.token_to_id.get(tok, 0) for tok in start_ids]\n",
    "                        f.write(f\"\\nPrompt: {prompt}\\n\")\n",
    "\n",
    "                        greedy_ids = generate(model, start_tokens=start_ids, max_new_tokens=40, temperature=1.0, top_k=None)\n",
    "                        greedy_text = \" \".join([tokenizer.id_to_token[i] for i in greedy_ids])\n",
    "                        f.write(\"Greedy: \" + greedy_text + \"\\n\")\n",
    "\n",
    "                        topk_ids = generate(model, start_tokens=start_ids, max_new_tokens=40, temperature=0.8, top_k=50)\n",
    "                        topk_text = \" \".join([tokenizer.id_to_token[i] for i in topk_ids])\n",
    "                        f.write(\"Top-k: \" + topk_text + \"\\n\")\n",
    "                print(f\"[Sample Eval] saved → {eval_path}\")\n",
    "\n",
    "            # Checkpoint + sample\n",
    "            if step % cfg.ckpt_interval == 0 or step == cfg.max_steps:\n",
    "                ckpt_path = os.path.join(run_dir, f\"ckpt_step{step}.pt\")\n",
    "                torch.save({\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"optimizer_state\": optimizer.state_dict(),\n",
    "                    \"scaler_state\": scaler.state_dict(),\n",
    "                    \"config\": asdict(cfg),\n",
    "                    \"vocab_size\": vocab_size,\n",
    "                    \"step\": step,\n",
    "                }, ckpt_path)\n",
    "                print(f\"[Save] checkpoint → {ckpt_path}\")\n",
    "\n",
    "            if step >= cfg.max_steps:\n",
    "                break\n",
    "\n",
    "    # Final test evaluation\n",
    "    val_loss = evaluate(model, valid_loader, device, cfg.eval_batches)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    test_loss = evaluate(model, test_loader, device, cfg.eval_batches)\n",
    "    test_ppl = math.exp(test_loss)\n",
    "    \n",
    "    print(f\"[Final Val] loss={val_loss:.4f} | ppl={val_ppl:.2f}\")\n",
    "    print(f\"[Final Test] loss={test_loss:.4f} | ppl={test_ppl:.2f}\")\n",
    "\n",
    "    # Save final metrics\n",
    "    results = {\n",
    "        \"k\": cfg.k,\n",
    "        \"n_embd\": cfg.n_embd,\n",
    "        \"dropout\": cfg.dropout,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_ppl\": val_ppl,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_ppl\": test_ppl,\n",
    "    }\n",
    "    with open(os.path.join(run_dir, \"final_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = parse_args()   # returns a TrainConfig object\n",
    "    results = train_and_eval_with_logging(cfg)\n",
    "    save_plot_and_csv(os.path.join(\"runs\", f\"gpt_final_k{cfg.k}\"), [results])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc73fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Training & Evaluation ----------------\n",
    "def train_and_eval_for_gpt(k, n_embd, dropout, run_dir, num_steps=2000, batch_size=32, lr=3e-4, eval_batches=50):\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# --- Load data & tokenizer ---\n",
    "tokenizer, splits = build_or_load_encoded(run_dir, k)\n",
    "vocab_size = len(tokenizer.token_to_id)\n",
    "\n",
    "\n",
    "train_ds = GPTDataset(splits.train, block_size=64)\n",
    "val_ds = GPTDataset(splits.valid, block_size=64)\n",
    "test_ds = GPTDataset(splits.test, block_size=64)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "\n",
    "# --- Build model ---\n",
    "model = GPT(\n",
    "vocab_size=vocab_size,\n",
    "block_size=64,\n",
    "n_layer=4,\n",
    "n_head=4,\n",
    "n_embd=n_embd,\n",
    "dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# --- Training loop ---\n",
    "print(f\"Training GPT (k={k}, n_embd={n_embd}, dropout={dropout})...\")\n",
    "for step in range(1, num_steps+1):\n",
    "loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "\n",
    "if step % 200 == 0:\n",
    "val_loss = evaluate(model, val_loader, device, max_batches=eval_batches)\n",
    "val_ppl = math.exp(val_loss)\n",
    "print(f\"Step {step}: train_loss={loss:.4f}, val_loss={val_loss:.4f}, val_ppl={val_ppl:.2f}\")\n",
    "\n",
    "\n",
    "# --- Final evaluation ---\n",
    "val_loss = evaluate(model, val_loader, device, max_batches=eval_batches)\n",
    "test_loss = evaluate(model, test_loader, device, max_batches=eval_batches)\n",
    "val_ppl = math.exp(val_loss)\n",
    "test_ppl = math.exp(test_loss)\n",
    "\n",
    "\n",
    "print(f\"Final [Val] loss={val_loss:.4f}, ppl={val_ppl:.2f}\")\n",
    "print(f\"Final [Test] loss={test_loss:.4f}, ppl={test_ppl:.2f}\")\n",
    "\n",
    "\n",
    "# --- Extrinsic: Generation ---\n",
    "prompts = [\"To be or not to be\", \"Once upon a midnight dreary\"]\n",
    "for prompt in prompts:\n",
    "start_ids = tokenizer.encode_words(prompt.split())\n",
    "start_ids = [tokenizer.token_to_id.get(tok, 0) for tok in start_ids]\n",
    "\n",
    "\n",
    "print(\"\\nPrompt:\", prompt)\n",
    "print(\"Greedy:\")\n",
    "greedy_ids = generate(model, start_tokens=start_ids, max_new_tokens=40, temperature=1.0, top_k=None)\n",
    "print(\" \".join(tokenizer.decode_tokens([tokenizer.id_to_token[i] for i in greedy_ids])))\n",
    "\n",
    "\n",
    "print(\"Top-k:\")\n",
    "topk_ids = generate(model, start_tokens=start_ids, max_new_tokens=40, temperature=0.8, top_k=50)\n",
    "print(\" \".join(tokenizer.decode_tokens([tokenizer.id_to_token[i] for i in topk_ids])))\n",
    "\n",
    "\n",
    "return {\n",
    "\"k\": k,\n",
    "\"n_embd\": n_embd,\n",
    "\"dropout\": dropout,\n",
    "\"val_loss\": val_loss,\n",
    "\"val_ppl\": val_ppl,\n",
    "\"test_loss\": test_loss,\n",
    "\"test_ppl\": test_ppl,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d674a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for k in [200, 500, 1000]:   # your top 3 from n-grams\n",
    "    for n_embd in [64, 128]:\n",
    "        res = train_and_eval_for_gpt(k, n_embd, dropout=0.1, run_dir=\"runs/gpt_exp\")\n",
    "        results.append(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83749dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## probably not used\n",
    "\n",
    "\"\"\"# sample generation from random position of valid set\n",
    "start = random.randint(0, max(0, len(splits.valid) - cfg.block_size - 1))\n",
    "prefix = splits.valid[start:start+min(32, cfg.block_size)].tolist()\n",
    "gen_ids = generate(model, start_tokens=prefix, max_new_tokens=60, temperature=0.8, top_k=50)\n",
    "sample_txt = \" \".join([tokenizer.id_to_token[i] for i in gen_ids])\n",
    "sample_path = os.path.join(run_dir, \"samples\", f\"step{step}_sample.txt\")\n",
    "with open(sample_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sample_txt)\n",
    "print(f\"[Sample] saved → {sample_path}\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
