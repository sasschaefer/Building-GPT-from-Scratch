{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc25d52",
   "metadata": {},
   "source": [
    "## Task 4:\n",
    "Task remarks: GPT – Hand in, until 31.08.\n",
    "\n",
    "* If something is underspecified, just make decision yourself\n",
    "* Well-documented code\n",
    "* Submission format\n",
    "    * Notebook (incl. pdf) or GitHub readme (submit pdf with link to repo) as technical\n",
    "report of what we did\n",
    "        * Nice narrative and way to navigate code, not scientific paper\n",
    "        * Include plots (loss, perplexity scores, hyperparameters, etc.)\n",
    "        * Optional include pseudocode\n",
    "        * Qualitative analysis nice to have, e.g, add and evaluate generated text in\n",
    "report\n",
    "        * Can add appendix for additional plots\n",
    "* Hand in every mile stone, starting from UNIX comments\n",
    "* Removed in-between milestone of causal-self attention\n",
    "* Everything together in one file\n",
    "* Compare the models from each milestone, report perplexity for all\n",
    "    * Old-school n-gram\n",
    "    * Best neural n-gram\n",
    "    * GPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31503ede",
   "metadata": {},
   "source": [
    "**GPT itself**\n",
    "* Hyperparameter tuning: do not need all of them, choose what is most interesting and\n",
    "explain why\n",
    "    * Number of merges in BPE (not complete gridsearch, isolate top three number of\n",
    "merges in perplexity in n-gram, test those for GPT)\n",
    "    * Regularisation\n",
    "    * How small can we make neural embedding\n",
    "    * Do not change optimiser\n",
    "* General remarks\n",
    "    * Transformer blocks from scratch would be beyond 1.0, not required\n",
    "    * Implement causal self-attention yourself, do not use ready-made PyTorch version\n",
    "    * For computing perplexity: Implementing teacher forcing annealing is necessary\n",
    "for good generation performance, but we don’t have to do it for our assignment\n",
    "* Reminders\n",
    "    * Skip weight initialisation and optimiser configuration\n",
    "        * Can use standard PyTorch initialisation → just get transformer\n",
    "parameters and add them when initialising the optimiser\n",
    "    * Remember to change device selection, currently “cuda”, you might want “mps” or\n",
    "“cpu”\n",
    "    * Configs: make n_embd smaller, don’t change betas and weight decay (unless\n",
    "you want to), can change batch size, chunk size, n_head, n_layer\n",
    "    * Specify temperature and top-k parameters for generate function\n",
    "    * Activation function used in MLP: not ReLU as in slides but GELU (might not be in\n",
    "PyTorch yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b1cbad",
   "metadata": {},
   "source": [
    "## First try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b642298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Networks\n",
    "    This consists of two linear transformations with a ReLU activation in between.\n",
    "    \n",
    "    FFN(x) = max(0, xW1 + b1 )W2 + b2\n",
    "    d_model: embedding dimension (e.g., 512)\n",
    "    d_ff: feed-forward dimension (e.g., 2048)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.d_ff= d_ff\n",
    "        \n",
    "        # Linear transformation y = xW+b\n",
    "        self.fc1 = nn.Linear(self.d_model, self.d_ff, bias = True)\n",
    "        self.fc2 = nn.Linear(self.d_ff, self.d_model, bias = True)\n",
    "        \n",
    "        # for potential speed up\n",
    "        # Pre-normalize the weights (can help with training stability)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # check input and first FF layer dimension matching\n",
    "        batch_size, seq_length, d_input = input.size()\n",
    "        assert self.d_model == d_input, \"d_model must be the same dimension as the input\"\n",
    "\n",
    "        # First linear transformation followed by ReLU\n",
    "        # There's no need for explicit torch.max() as F.relu() already implements max(0,x)\n",
    "        f1 = F.relu(self.fc1(input))\n",
    "\n",
    "        # max(0, xW_1 + b_1)W_2 + b_2 \n",
    "        f2 =  self.fc2(f1)\n",
    "\n",
    "        return f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5645371",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Causal Self-Attention (no cross attention, GPT style)\n",
    "    Args:\n",
    "        d_model: total hidden dimension of the model\n",
    "        num_head: number of attention heads\n",
    "        dropout: dropout rate for attention scores\n",
    "        bias: whether to include bias in linear projections\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_head, dropout=0.1, bias=True): # infer d_k, d_v, d_q from d_model\n",
    "        super().__init__()  # Missing in the original implementation\n",
    "        assert d_model % num_head == 0, \"d_model must be divisible by num_head\"\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "        self.d_head=d_model//num_head\n",
    "        self.dropout_rate = dropout  # Store dropout rate separately\n",
    "\n",
    "        # linear transformations\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.output_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Initiialize scaler\n",
    "        self.scaler = float(1.0 / math.sqrt(self.d_head)) # Store as float in initialization\n",
    "        \n",
    "\n",
    "    def forward(self, sequence, att_mask=None):\n",
    "        \"\"\"Input shape: [batch_size, seq_len, d_model=num_head * d_head]\"\"\"\n",
    "        batch_size, seq_len, model_dim = sequence.size()\n",
    "\n",
    "        # Check only critical input dimensions\n",
    "        assert model_dim == self.d_model, f\"Input dimension {model_dim} doesn't match model dimension {self.d_model}\"\n",
    "    \n",
    "        \n",
    "        # Linear projections and reshape for multi-head\n",
    "        Q_state = self.q_proj(sequence)\n",
    "        \n",
    "        kv_seq_len = seq_len\n",
    "        K_state = self.k_proj(sequence)\n",
    "        V_state = self.v_proj(sequence)\n",
    "\n",
    "        #[batch_size, self.num_head, seq_len, self.d_head]\n",
    "        Q_state = Q_state.view(batch_size, seq_len, self.num_head, self.d_head).transpose(1,2) \n",
    "            \n",
    "        # in cross-attention, key/value sequence length might be different from query sequence length\n",
    "        K_state = K_state.view(batch_size, kv_seq_len, self.num_head, self.d_head).transpose(1,2)\n",
    "        V_state = V_state.view(batch_size, kv_seq_len, self.num_head, self.d_head).transpose(1,2)\n",
    "\n",
    "        # Scale Q by 1/sqrt(d_k)\n",
    "        Q_state = Q_state * self.scaler\n",
    "    \n",
    "    \n",
    "        # Compute attention matrix: QK^T\n",
    "        self.att_matrix = torch.matmul(Q_state, K_state.transpose(-1,-2)) \n",
    "\n",
    "    \n",
    "        # apply attention mask to attention matrix\n",
    "        if att_mask is not None and not isinstance(att_mask, torch.Tensor):\n",
    "            raise TypeError(\"att_mask must be a torch.Tensor\")\n",
    "\n",
    "        if att_mask is not None:\n",
    "            self.att_matrix = self.att_matrix + att_mask\n",
    "        \n",
    "        # apply softmax to the last dimension to get the attention score: softmax(QK^T)\n",
    "        att_score = F.softmax(self.att_matrix, dim = -1)\n",
    "    \n",
    "        # apply drop out to attention score\n",
    "        att_score = self.dropout(att_score)\n",
    "    \n",
    "        # get final output: softmax(QK^T)V\n",
    "        att_output = torch.matmul(att_score, V_state)\n",
    "    \n",
    "        # concatinate all attention heads\n",
    "        att_output = att_output.transpose(1, 2)\n",
    "        att_output = att_output.contiguous().view(batch_size, seq_len, self.num_head*self.d_head) \n",
    "    \n",
    "        # final linear transformation to the concatenated output\n",
    "        att_output = self.output_proj(att_output)\n",
    "\n",
    "        assert att_output.size() == (batch_size, seq_len, self.d_model), \\\n",
    "        f\"Final output shape {att_output.size()} incorrect\"\n",
    "\n",
    "        return att_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd20b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.att = TransformerAttention(d_model, n_head, dropout=dropout)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FFN(d_model, d_ff)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_causal_mask(seq_len):\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, embed_input, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        embed_input: Decoder input sequence [batch_size, seq_len, d_model]\n",
    "        casual_attention_mask: Causal mask for self-attention [batch_size, seq_len, seq_len]\n",
    "        padding_mask: Padding mask for cross-attention [batch_size, seq_len, encoder_seq_len]\n",
    "        Returns:\n",
    "        Tensor: Decoded output [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = embed_input.size()\n",
    "        \n",
    "        assert embed_input.size(-1) == self.d_model, f\"Input dimension {embed_input.size(-1)} doesn't match model dimension {self.d_model}\"\n",
    "\n",
    "        # Generate and expand causal mask for self-attention\n",
    "        causal_mask = self.create_causal_mask(seq_len).to(embed_input.device)  # [seq_len, seq_len]\n",
    "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(1)  # [1, 1, seq_len, seq_len]\n",
    "\n",
    "\n",
    "        # Self-attention + residual + norm\n",
    "        att_out = self.att(x, att_mask=causal_mask)\n",
    "        x = self.ln1(x + self.dropout(att_out))\n",
    "\n",
    "        # FFN + residual + norm\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.ln2(x + self.dropout(ffn_out))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b905ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layer, n_embd, n_head, d_ff, max_seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, n_embd)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GPTBlock(n_embd, n_head, d_ff, dropout) for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        batch_size, seq_len = idx.size()\n",
    "        \n",
    "        pos = torch.arange(0, seq_len, device=idx.device).unsqueeze(0)\n",
    "        x = self.token_emb(idx) + self.pos_emb(pos)\n",
    "        # Causal mask for GPT\n",
    "        mask = GPTBlock.create_causal_mask(seq_len).to(idx.device)\n",
    "        mask = mask.unsqueeze(0).unsqueeze(1)  # [1, 1, seq_len, seq_len]\n",
    "        for block in self.blocks:\n",
    "            x = block(x, att_mask=mask)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb70b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000      # depends on tokenizer\n",
    "n_layer = 4            # small for testing\n",
    "n_embd = 128           # embedding dimension\n",
    "n_head = 4             # must divide n_embd\n",
    "d_ff = 512             # feed-forward dimension\n",
    "max_seq_len = 128      # context window\n",
    "\n",
    "model = MiniGPT(vocab_size, n_layer, n_embd, n_head, d_ff, max_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f44e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_len = 10\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))  # random token ids\n",
    "\n",
    "logits = model(x)  # [batch_size, seq_len, vocab_size]\n",
    "print(logits.shape)\n",
    "# torch.Size([2, 10, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f821d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "for step in range(100):\n",
    "    x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    y = x.clone()  # next-token prediction (shifted later)\n",
    "\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step} | Loss {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906fc060",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(idx)                       # [batch, seq, vocab_size]\n",
    "        logits = logits[:, -1, :]                 # last token logits\n",
    "        probs = F.softmax(logits, dim=-1)         # convert to probs\n",
    "        next_token = torch.multinomial(probs, 1)  # sample\n",
    "        idx = torch.cat([idx, next_token], dim=1) # append\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac0590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generation\n",
    "start = torch.tensor([[1]])  # BOS token or just any token id\n",
    "out = generate(model, start, max_new_tokens=20)\n",
    "print(\"Generated sequence:\", out.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ceb3e",
   "metadata": {},
   "source": [
    "## All in One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8438c661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--k K] [--batch_size BATCH_SIZE]\n",
      "                             [--block_size BLOCK_SIZE] [--n_layer N_LAYER]\n",
      "                             [--n_head N_HEAD] [--n_embd N_EMBD]\n",
      "                             [--dropout DROPOUT] [--lr LR]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--max_steps MAX_STEPS]\n",
      "                             [--eval_interval EVAL_INTERVAL]\n",
      "                             [--eval_batches EVAL_BATCHES]\n",
      "                             [--ckpt_interval CKPT_INTERVAL]\n",
      "                             [--warmup_steps WARMUP_STEPS]\n",
      "                             [--grad_clip GRAD_CLIP] [--no_amp] [--seed SEED]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\lehel\\AppData\\Roaming\\jupyter\\runtime\\kernel-v35c793b586c4374b712068d4e50e03652ae63714a.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3675: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GPT (Transformer) training pipeline for Shakespeare with BPE tokenizer\n",
    "====================================================================\n",
    "\n",
    "Goals\n",
    "-----\n",
    "- PyTorch implementation of a small GPT (nanoGPT-style) with clean structure.\n",
    "- Re-use your existing BPE merges and token conventions (</w>, <bos>, <eos>).\n",
    "- Detailed logging, file outputs, checkpoints, CSV logs, and PNG loss plots.\n",
    "- Validation + test perplexity.\n",
    "- Sample text generation at checkpoints.\n",
    "\n",
    "Directory layout (inputs & outputs)\n",
    "----------------------------------\n",
    "Inputs (must exist):\n",
    "- Corpus/\n",
    "    Shakespeare_clean_train.txt\n",
    "    Shakespeare_clean_valid.txt\n",
    "    Shakespeare_clean_test.txt\n",
    "- Generated_tokens/\n",
    "    (one of) bpe_merges with k = {k}.txt, standard_bpe_merges_k{k}.txt, ...\n",
    "\n",
    "Outputs (this script will create):\n",
    "- runs/gpt_{timestamp}_k{k}/\n",
    "    config.json\n",
    "    tokenizer.json\n",
    "    train_encoded.pt, valid_encoded.pt, test_encoded.pt\n",
    "    logs.csv\n",
    "    loss_plot.png\n",
    "    ckpt_step{...}.pt (model + optimizer + scaler + config + samples)\n",
    "    samples/\n",
    "        step{...}_sample.txt\n",
    "\n",
    "Usage\n",
    "-----\n",
    "python gpt_shakespeare_trainer.py --k 1600 --batch_size 64 --block_size 128 --n_layer 4 --n_head 4 --n_embd 256 --max_steps 2000\n",
    "\n",
    "Notes\n",
    "-----\n",
    "- Designed for small models/datasets. Mixed precision is optional (amp).\n",
    "- If no CUDA, training runs on CPU (slower but fine for tiny configs).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Tuple, Dict, Iterable, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # headless\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ================================ Constants ================================\n",
    "CORPUS_DIR = \"Corpus\"\n",
    "GENERATED_DIR = \"Generated_tokens\"\n",
    "WORD_END = \"</w>\"\n",
    "EOS = \"<eos>\"\n",
    "BOS = \"<bos>\"\n",
    "_wsre = re.compile(r\"\\s+\")\n",
    "random.seed(42)\n",
    "\n",
    "# ============================= BPE Tokenizer ===============================\n",
    "\n",
    "def find_merges_file(k: int, verbose: bool = True) -> str:\n",
    "    candidates = [\n",
    "        os.path.join(GENERATED_DIR, f\"bpe_merges with k = {k}.txt\"),\n",
    "        os.path.join(GENERATED_DIR, f\"standard_bpe_merges_k{k}.txt\"),\n",
    "        os.path.join(GENERATED_DIR, f\"aggressive_clean_bpe_merges_k{k}.txt\"),\n",
    "        os.path.join(GENERATED_DIR, f\"bpe_merges_k{k}.txt\"),\n",
    "        os.path.join(GENERATED_DIR, f\"bpe_merges_k{k}_webtext_clean.txt\"),\n",
    "    ]\n",
    "    for path in candidates:\n",
    "        if os.path.exists(path):\n",
    "            if verbose:\n",
    "                print(f\"[Found] Using merges file: {path}\")\n",
    "            return path\n",
    "    raise FileNotFoundError(f\"No merges file found for k={k}. Tried: {candidates}\")\n",
    "\n",
    "def load_merges(merges_path: str) -> List[Tuple[str, str]]:\n",
    "    merges = []\n",
    "    with open(merges_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                merges.append((parts[0], parts[1]))\n",
    "    return merges\n",
    "\n",
    "def words_from_text(text: str, lowercase: bool = True) -> List[str]:\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    return [w for w in _wsre.split(text.strip()) if w]\n",
    "\n",
    "def apply_merges_to_word(word: str, merges: List[Tuple[str, str]]) -> List[str]:\n",
    "    symbols = tuple(list(word) + [WORD_END])\n",
    "    for a, b in merges:\n",
    "        out = []\n",
    "        i, L = 0, len(symbols)\n",
    "        while i < L:\n",
    "            if i < L-1 and symbols[i] == a and symbols[i+1] == b:\n",
    "                out.append(a + b); i += 2\n",
    "            else:\n",
    "                out.append(symbols[i]); i += 1\n",
    "        symbols = tuple(out)\n",
    "    return list(symbols)\n",
    "\n",
    "def tokenize_lines_with_merges(text: str, merges: List[Tuple[str, str]]) -> List[List[str]]:\n",
    "    token_lines: List[List[str]] = []\n",
    "    for line in text.strip().splitlines():\n",
    "        words = words_from_text(line)\n",
    "        if not words:\n",
    "            continue\n",
    "        toks: List[str] = []\n",
    "        for w in words:\n",
    "            toks.extend(apply_merges_to_word(w, merges))\n",
    "        toks.append(EOS)\n",
    "        token_lines.append(toks)\n",
    "    return token_lines\n",
    "\n",
    "# Convert tokens to ids, build vocab\n",
    "class BPETokenizer:\n",
    "    def __init__(self, merges: List[Tuple[str, str]], extra_tokens: Optional[List[str]] = None):\n",
    "        self.merges = merges\n",
    "        self.extra_tokens = extra_tokens or []\n",
    "        self.token_to_id: Dict[str, int] = {}\n",
    "        self.id_to_token: List[str] = []\n",
    "\n",
    "    def build_vocab_from_texts(self, texts: Dict[str, str]):\n",
    "        vocab = set()\n",
    "        for name, txt in texts.items():\n",
    "            for line in tokenize_lines_with_merges(txt, self.merges):\n",
    "                vocab.update(line)\n",
    "        vocab.update(self.extra_tokens)\n",
    "        # Deterministic order\n",
    "        self.id_to_token = sorted(vocab)\n",
    "        self.token_to_id = {t: i for i, t in enumerate(self.id_to_token)}\n",
    "\n",
    "    def encode_words(self, words: Iterable[str]) -> List[str]:\n",
    "        toks: List[str] = []\n",
    "        for w in (w.lower() for w in words):\n",
    "            toks.extend(apply_merges_to_word(w, self.merges))\n",
    "        return toks\n",
    "\n",
    "    def encode_lines(self, text: str) -> List[List[int]]:\n",
    "        lines_tok = tokenize_lines_with_merges(text, self.merges)\n",
    "        ids_lines: List[List[int]] = []\n",
    "        for line in lines_tok:\n",
    "            ids_lines.append([self.token_to_id[t] for t in line if t in self.token_to_id])\n",
    "        return ids_lines\n",
    "\n",
    "    def decode_tokens(self, token_stream: List[str]) -> List[str]:\n",
    "        words: List[str] = []\n",
    "        buf: List[str] = []\n",
    "        for t in token_stream:\n",
    "            if t == EOS:\n",
    "                break\n",
    "            buf.append(t)\n",
    "            if t.endswith(WORD_END):\n",
    "                chars: List[str] = []\n",
    "                for sub in buf:\n",
    "                    if sub.endswith(WORD_END):\n",
    "                        chars.extend(list(sub[:-len(WORD_END)]))\n",
    "                    else:\n",
    "                        chars.extend(list(sub))\n",
    "                words.append(\"\".join(chars))\n",
    "                buf = []\n",
    "        if buf:\n",
    "            chars = []\n",
    "            for sub in buf:\n",
    "                if sub.endswith(WORD_END):\n",
    "                    chars.extend(list(sub[:-len(WORD_END)]))\n",
    "                else:\n",
    "                    chars.extend(list(sub))\n",
    "            if chars:\n",
    "                words.append(\"\".join(chars))\n",
    "        return words\n",
    "\n",
    "    def save(self, path: str):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"id_to_token\": self.id_to_token,\n",
    "                \"token_to_id\": self.token_to_id,\n",
    "                \"extra_tokens\": self.extra_tokens,\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: str) -> \"BPETokenizer\":\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            obj = json.load(f)\n",
    "        tok = BPETokenizer(merges=[], extra_tokens=obj.get(\"extra_tokens\", []))\n",
    "        tok.id_to_token = obj[\"id_to_token\"]\n",
    "        tok.token_to_id = {k: int(v) for k, v in obj[\"token_to_id\"].items()}\n",
    "        return tok\n",
    "\n",
    "# ============================ Data preparation ==============================\n",
    "\n",
    "def read_text(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "@dataclass\n",
    "class EncodedSplits:\n",
    "    train: torch.Tensor\n",
    "    valid: torch.Tensor\n",
    "    test: torch.Tensor\n",
    "\n",
    "\n",
    "def build_or_load_encoded(run_dir: str, k: int) -> Tuple[BPETokenizer, EncodedSplits]:\n",
    "    enc_train_path = os.path.join(run_dir, \"train_encoded.pt\")\n",
    "    enc_valid_path = os.path.join(run_dir, \"valid_encoded.pt\")\n",
    "    enc_test_path  = os.path.join(run_dir, \"test_encoded.pt\")\n",
    "    tok_path       = os.path.join(run_dir, \"tokenizer.json\")\n",
    "\n",
    "    if all(os.path.exists(p) for p in [enc_train_path, enc_valid_path, enc_test_path, tok_path]):\n",
    "        print(\"[Load] Using cached encoded splits + tokenizer\")\n",
    "        tokenizer = BPETokenizer.load(tok_path)\n",
    "        train_ids = torch.load(enc_train_path)\n",
    "        valid_ids = torch.load(enc_valid_path)\n",
    "        test_ids  = torch.load(enc_test_path)\n",
    "        return tokenizer, EncodedSplits(train_ids, valid_ids, test_ids)\n",
    "\n",
    "    # Build from raw\n",
    "    merges_path = find_merges_file(k, verbose=True)\n",
    "    merges = load_merges(merges_path)\n",
    "\n",
    "    train_txt = read_text(os.path.join(CORPUS_DIR, \"Shakespeare_clean_train.txt\"))\n",
    "    valid_txt = read_text(os.path.join(CORPUS_DIR, \"Shakespeare_clean_valid.txt\"))\n",
    "    test_txt  = read_text(os.path.join(CORPUS_DIR, \"Shakespeare_clean_test.txt\"))\n",
    "\n",
    "    tokenizer = BPETokenizer(merges=merges, extra_tokens=[BOS, EOS])\n",
    "    tokenizer.build_vocab_from_texts({\"train\": train_txt, \"valid\": valid_txt, \"test\": test_txt})\n",
    "\n",
    "    def flatten(lines: List[List[int]]) -> List[int]:\n",
    "        flat = []\n",
    "        for ln in lines: flat.extend(ln)\n",
    "        return flat\n",
    "\n",
    "    train_ids = torch.tensor(flatten(tokenizer.encode_lines(train_txt)), dtype=torch.long)\n",
    "    valid_ids = torch.tensor(flatten(tokenizer.encode_lines(valid_txt)), dtype=torch.long)\n",
    "    test_ids  = torch.tensor(flatten(tokenizer.encode_lines(test_txt)),  dtype=torch.long)\n",
    "\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    tokenizer.save(tok_path)\n",
    "    torch.save(train_ids, enc_train_path)\n",
    "    torch.save(valid_ids, enc_valid_path)\n",
    "    torch.save(test_ids,  enc_test_path)\n",
    "    print(f\"[Save] Encoded splits to {run_dir}\")\n",
    "    print(f\"[Info] Vocab size = {len(tokenizer.id_to_token)}\")\n",
    "    return tokenizer, EncodedSplits(train_ids, valid_ids, test_ids)\n",
    "\n",
    "# ================================ Dataset ==================================\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, ids: torch.Tensor, block_size: int):\n",
    "        self.ids = ids\n",
    "        self.block_size = block_size\n",
    "        # we will sample random start positions in __getitem__\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # approximate number of sequences of length block_size we can draw\n",
    "        return max(1, len(self.ids) - self.block_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ignore idx and sample randomly to add stochasticity\n",
    "        i = random.randint(0, len(self.ids) - self.block_size - 1)\n",
    "        x = self.ids[i : i + self.block_size]\n",
    "        y = self.ids[i + 1 : i + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "# ============================== GPT Model ==================================\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd: int, n_head: int, block_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.key = nn.Linear(n_embd, n_embd)\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        self.value = nn.Linear(n_embd, n_embd)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        # causal mask\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(k.size(-1))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v  # (B, n_head, T, head_dim)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd: int, n_head: int, block_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = CausalSelfAttention(n_embd, n_head, block_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_layer: int, n_head: int, n_embd: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte': nn.Embedding(vocab_size, n_embd),\n",
    "            'wpe': nn.Embedding(block_size, n_embd),\n",
    "            'drop': nn.Dropout(dropout),\n",
    "            'h': nn.ModuleList([Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)]),\n",
    "            'ln_f': nn.LayerNorm(n_embd),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.block_size\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        tok_emb = self.transformer['wte'](idx)\n",
    "        pos_emb = self.transformer['wpe'](pos)[None, :, :]\n",
    "        x = self.transformer['drop'](tok_emb + pos_emb)\n",
    "        for block in self.transformer['h']:\n",
    "            x = block(x)\n",
    "        x = self.transformer['ln_f'](x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "# ============================ Training utilities ============================\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    k: int = 1600\n",
    "    batch_size: int = 64\n",
    "    block_size: int = 128\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 256\n",
    "    dropout: float = 0.1\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.1\n",
    "    max_steps: int = 2000\n",
    "    eval_interval: int = 200\n",
    "    eval_batches: int = 200\n",
    "    ckpt_interval: int = 500\n",
    "    warmup_steps: int = 100\n",
    "    grad_clip: float = 1.0\n",
    "    amp: bool = True\n",
    "    seed: int = 1337\n",
    "\n",
    "# ============================== Evaluation ==================================\n",
    "\n",
    "def evaluate(model: GPT, loader: DataLoader, device: torch.device, max_batches: int) -> float:\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            if i >= max_batches:\n",
    "                break\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            losses.append(loss.item())\n",
    "    model.train()\n",
    "    return float(sum(losses) / max(1, len(losses)))\n",
    "\n",
    "# ============================== Generation ==================================\n",
    "\n",
    "def generate(model: GPT, start_tokens: List[int], max_new_tokens: int, temperature: float = 1.0, top_k: Optional[int] = None) -> List[int]:\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    idx = torch.tensor(start_tokens, dtype=torch.long, device=device)[None, :]\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -model.block_size:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / max(1e-8, temperature)\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, next_id), dim=1)\n",
    "    return idx[0].tolist()\n",
    "\n",
    "# ============================== Plot / CSV ==================================\n",
    "\n",
    "def save_plot_and_csv(run_dir: str, history: List[Dict[str, float]]):\n",
    "    csv_path = os.path.join(run_dir, \"logs.csv\")\n",
    "    png_path = os.path.join(run_dir, \"loss_plot.png\")\n",
    "\n",
    "    # CSV\n",
    "    import csv\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"step\", \"train_loss\", \"val_loss\", \"val_ppl\"])\n",
    "        w.writeheader()\n",
    "        for r in history:\n",
    "            w.writerow(r)\n",
    "\n",
    "    # Plot\n",
    "    steps = [r[\"step\"] for r in history]\n",
    "    tloss = [r[\"train_loss\"] for r in history]\n",
    "    vloss = [r[\"val_loss\"] for r in history]\n",
    "\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(steps, tloss, label=\"train\")\n",
    "    plt.plot(steps, vloss, label=\"valid\")\n",
    "    plt.xlabel(\"step\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(png_path)\n",
    "    plt.close()\n",
    "    print(f\"[Save] plots → {png_path}, csv → {csv_path}\")\n",
    "\n",
    "# ============================== Main training ===============================\n",
    "\n",
    "def main(cfg: TrainConfig):\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(\"runs\", f\"gpt_{ts}_k{cfg.k}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(run_dir, \"samples\"), exist_ok=True)\n",
    "\n",
    "    # Save config\n",
    "    with open(os.path.join(run_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(asdict(cfg), f, indent=2)\n",
    "\n",
    "    # Encode or load\n",
    "    tokenizer, splits = build_or_load_encoded(run_dir, cfg.k)\n",
    "    vocab_size = len(tokenizer.id_to_token)\n",
    "\n",
    "    # Datasets/loaders\n",
    "    train_ds = GPTDataset(splits.train, cfg.block_size)\n",
    "    valid_ds = GPTDataset(splits.valid, cfg.block_size)\n",
    "    test_ds  = GPTDataset(splits.test,  cfg.block_size)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=cfg.batch_size, shuffle=False, drop_last=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    # Model\n",
    "    model = GPT(vocab_size=vocab_size, block_size=cfg.block_size, n_layer=cfg.n_layer, n_head=cfg.n_head, n_embd=cfg.n_embd, dropout=cfg.dropout)\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"[Info] Device: {device} | Parameters: {count_parameters(model)/1e6:.2f}M | Vocab={vocab_size} | block={cfg.block_size}\")\n",
    "\n",
    "    # Optimizer / Scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    lr_sched = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: min(1.0, step / max(1, cfg.warmup_steps))\n",
    "    )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.amp and device.type==\"cuda\")\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = []\n",
    "    running_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    step = 0\n",
    "    while step < cfg.max_steps:\n",
    "        for xb, yb in train_loader:\n",
    "            step += 1\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=scaler.is_enabled()):\n",
    "                logits = model(xb)\n",
    "                loss = loss_fn(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            if cfg.grad_clip:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            lr_sched.step()\n",
    "\n",
    "            running_loss = 0.9 * running_loss + 0.1 * loss.item() if step > 1 else loss.item()\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                print(f\"[Step {step:5d}] train_loss={loss.item():.4f} (ema {running_loss:.4f})\")\n",
    "\n",
    "            # Eval\n",
    "            if step % cfg.eval_interval == 0 or step == cfg.max_steps:\n",
    "                val_loss = evaluate(model, valid_loader, device, cfg.eval_batches)\n",
    "                val_ppl = math.exp(val_loss)\n",
    "                print(f\"[Eval  {step:5d}] val_loss={val_loss:.4f} | val_ppl={val_ppl:.2f}\")\n",
    "                history.append({\"step\": step, \"train_loss\": running_loss, \"val_loss\": val_loss, \"val_ppl\": val_ppl})\n",
    "                save_plot_and_csv(run_dir, history)\n",
    "\n",
    "            # Checkpoint + sample\n",
    "            if step % cfg.ckpt_interval == 0 or step == cfg.max_steps:\n",
    "                ckpt_path = os.path.join(run_dir, f\"ckpt_step{step}.pt\")\n",
    "                torch.save({\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"optimizer_state\": optimizer.state_dict(),\n",
    "                    \"scaler_state\": scaler.state_dict(),\n",
    "                    \"config\": asdict(cfg),\n",
    "                    \"vocab_size\": vocab_size,\n",
    "                    \"step\": step,\n",
    "                }, ckpt_path)\n",
    "                print(f\"[Save] checkpoint → {ckpt_path}\")\n",
    "\n",
    "                # sample generation from random position of valid set\n",
    "                start = random.randint(0, max(0, len(splits.valid) - cfg.block_size - 1))\n",
    "                prefix = splits.valid[start:start+min(32, cfg.block_size)].tolist()\n",
    "                gen_ids = generate(model, start_tokens=prefix, max_new_tokens=60, temperature=0.8, top_k=50)\n",
    "                sample_txt = \" \".join([tokenizer.id_to_token[i] for i in gen_ids])\n",
    "                sample_path = os.path.join(run_dir, \"samples\", f\"step{step}_sample.txt\")\n",
    "                with open(sample_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(sample_txt)\n",
    "                print(f\"[Sample] saved → {sample_path}\")\n",
    "\n",
    "            if step >= cfg.max_steps:\n",
    "                break\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_loss = evaluate(model, test_loader, device, cfg.eval_batches)\n",
    "    test_ppl = math.exp(test_loss)\n",
    "    print(f\"[Test] loss={test_loss:.4f} | ppl={test_ppl:.2f}\")\n",
    "\n",
    "    # append to history and save\n",
    "    if len(history) == 0 or history[-1][\"step\"] != step:\n",
    "        history.append({\"step\": step, \"train_loss\": running_loss, \"val_loss\": None, \"val_ppl\": None})\n",
    "    save_plot_and_csv(run_dir, history)\n",
    "\n",
    "# =============================== CLI wrapper ================================\n",
    "\n",
    "def parse_args() -> TrainConfig:\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--k\", type=int, default=1000)\n",
    "    p.add_argument(\"--batch_size\", type=int, default=64)\n",
    "    p.add_argument(\"--block_size\", type=int, default=128)\n",
    "    p.add_argument(\"--n_layer\", type=int, default=4)\n",
    "    p.add_argument(\"--n_head\", type=int, default=4)\n",
    "    p.add_argument(\"--n_embd\", type=int, default=256)\n",
    "    p.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "    p.add_argument(\"--lr\", type=float, default=3e-4)\n",
    "    p.add_argument(\"--weight_decay\", type=float, default=0.1)\n",
    "    p.add_argument(\"--max_steps\", type=int, default=2000)\n",
    "    p.add_argument(\"--eval_interval\", type=int, default=200)\n",
    "    p.add_argument(\"--eval_batches\", type=int, default=200)\n",
    "    p.add_argument(\"--ckpt_interval\", type=int, default=500)\n",
    "    p.add_argument(\"--warmup_steps\", type=int, default=100)\n",
    "    p.add_argument(\"--grad_clip\", type=float, default=1.0)\n",
    "    p.add_argument(\"--no_amp\", action=\"store_true\")\n",
    "    p.add_argument(\"--seed\", type=int, default=1337)\n",
    "    args = p.parse_args()\n",
    "    cfg = TrainConfig(\n",
    "        k=args.k,\n",
    "        batch_size=args.batch_size,\n",
    "        block_size=args.block_size,\n",
    "        n_layer=args.n_layer,\n",
    "        n_head=args.n_head,\n",
    "        n_embd=args.n_embd,\n",
    "        dropout=args.dropout,\n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay,\n",
    "        max_steps=args.max_steps,\n",
    "        eval_interval=args.eval_interval,\n",
    "        eval_batches=args.eval_batches,\n",
    "        ckpt_interval=args.ckpt_interval,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        grad_clip=args.grad_clip,\n",
    "        amp=not args.no_amp,\n",
    "        seed=args.seed,\n",
    "    )\n",
    "    return cfg\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = parse_args()\n",
    "    main(cfg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
