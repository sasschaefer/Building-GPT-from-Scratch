{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc25d52",
   "metadata": {},
   "source": [
    "## Task 4:\n",
    "Task remarks: GPT – Hand in, until 31.08.\n",
    "\n",
    "* If something is underspecified, just make decision yourself\n",
    "* Well-documented code\n",
    "* Submission format\n",
    "    * Notebook (incl. pdf) or GitHub readme (submit pdf with link to repo) as technical\n",
    "report of what we did\n",
    "        * Nice narrative and way to navigate code, not scientific paper\n",
    "        * Include plots (loss, perplexity scores, hyperparameters, etc.)\n",
    "        * Optional include pseudocode\n",
    "        * Qualitative analysis nice to have, e.g, add and evaluate generated text in\n",
    "report\n",
    "        * Can add appendix for additional plots\n",
    "* Hand in every mile stone, starting from UNIX comments\n",
    "* Removed in-between milestone of causal-self attention\n",
    "* Everything together in one file\n",
    "* Compare the models from each milestone, report perplexity for all\n",
    "    * Old-school n-gram\n",
    "    * Best neural n-gram\n",
    "    * GPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31503ede",
   "metadata": {},
   "source": [
    "**GPT itself**\n",
    "* Hyperparameter tuning: do not need all of them, choose what is most interesting and\n",
    "explain why\n",
    "    * Number of merges in BPE (not complete gridsearch, isolate top three number of\n",
    "merges in perplexity in n-gram, test those for GPT)\n",
    "    * Regularisation\n",
    "    * How small can we make neural embedding\n",
    "    * Do not change optimiser\n",
    "* General remarks\n",
    "    * Transformer blocks from scratch would be beyond 1.0, not required\n",
    "    * Implement causal self-attention yourself, do not use ready-made PyTorch version\n",
    "    * For computing perplexity: Implementing teacher forcing annealing is necessary\n",
    "for good generation performance, but we don’t have to do it for our assignment\n",
    "* Reminders\n",
    "    * Skip weight initialisation and optimiser configuration\n",
    "        * Can use standard PyTorch initialisation → just get transformer\n",
    "parameters and add them when initialising the optimiser\n",
    "    * Remember to change device selection, currently “cuda”, you might want “mps” or\n",
    "“cpu”\n",
    "    * Configs: make n_embd smaller, don’t change betas and weight decay (unless\n",
    "you want to), can change batch size, chunk size, n_head, n_layer\n",
    "    * Specify temperature and top-k parameters for generate function\n",
    "    * Activation function used in MLP: not ReLU as in slides but GELU (might not be in\n",
    "PyTorch yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438c661",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GPT (Transformer) training pipeline for Shakespeare with BPE tokenizer\n",
    "====================================================================\n",
    "\n",
    "Goals\n",
    "-----\n",
    "- PyTorch implementation of a small GPT (nanoGPT-style) with clean structure.\n",
    "- Re-use your existing BPE merges and token conventions (</w>, <bos>, <eos>).\n",
    "- Detailed logging, file outputs, checkpoints, CSV logs, and PNG loss plots.\n",
    "- Validation + test perplexity.\n",
    "- Sample text generation at checkpoints.\n",
    "\n",
    "Directory layout (inputs & outputs)\n",
    "----------------------------------\n",
    "Inputs (must exist):\n",
    "- Corpus/\n",
    "    Shakespeare_clean_train.txt\n",
    "    Shakespeare_clean_valid.txt\n",
    "    Shakespeare_clean_test.txt\n",
    "- Generated_tokens/\n",
    "    (one of) bpe_merges with k = {k}.txt, standard_bpe_merges_k{k}.txt, ...\n",
    "\n",
    "Outputs (this script will create):\n",
    "- runs/gpt_{timestamp}_k{k}/\n",
    "    config.json\n",
    "    tokenizer.json\n",
    "    train_encoded.pt, valid_encoded.pt, test_encoded.pt\n",
    "    logs.csv\n",
    "    loss_plot.png\n",
    "    ckpt_step{...}.pt (model + optimizer + scaler + config + samples)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    /\n",
    "        step{...}_sample.txt\n",
    "\n",
    "Usage\n",
    "-----\n",
    "python gpt_shakespeare_trainer.py --k 1600 --batch_size 64 --block_size 128 --n_layer 4 --n_head 4 --n_embd 256 --max_steps 2000\n",
    "\n",
    "Notes\n",
    "-----\n",
    "- Designed for small models/datasets. Mixed precision is optional (amp).\n",
    "- If no CUDA, training runs on CPU (slower but fine for tiny configs).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import sys\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Tuple, Dict, Iterable, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # headless\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"  # Matplotlib’s default bundled font\n",
    "\n",
    "# ================================ Constants ================================\n",
    "CORPUS_DIR = \"Corpus\"\n",
    "GENERATED_DIR = \"Generated_tokens\"\n",
    "WORD_END = \"</w>\"\n",
    "EOS = \"<eos>\"\n",
    "BOS = \"<bos>\"\n",
    "_wsre = re.compile(r\"\\s+\")\n",
    "#random.seed(42)\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    seed: int = 42\n",
    "    k: int = 1000\n",
    "    batch_size: int = 32\n",
    "    block_size: int = 128\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 128\n",
    "    dropout: float = 0.1\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.0\n",
    "    max_steps: int = 1000\n",
    "    eval_interval: int = 200\n",
    "    eval_batches: int = 20\n",
    "    ckpt_interval: int = 500\n",
    "    warmup_steps: int = 100\n",
    "    grad_clip: float = 1.0\n",
    "    amp: bool = True\n",
    "    no_amp: bool = False \n",
    "\n",
    "\n",
    "def parse_args() -> TrainConfig:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--k\", type=int, default=1000)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--block_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--n_layer\", type=int, default=4)\n",
    "    parser.add_argument(\"--n_head\", type=int, default=4)\n",
    "    parser.add_argument(\"--n_embd\", type=int, default=128)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--lr\", type=float, default=3e-4)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-2)\n",
    "    parser.add_argument(\"--max_steps\", type=int, default=5000)\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=100)\n",
    "    parser.add_argument(\"--grad_clip\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--eval_interval\", type=int, default=500)\n",
    "    parser.add_argument(\"--eval_batches\", type=int, default=20)\n",
    "    parser.add_argument(\"--ckpt_interval\", type=int, default=1000)\n",
    "    parser.add_argument(\"--no_amp\", action=\"store_true\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "\n",
    "    if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "        args, _ = parser.parse_known_args()\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    return TrainConfig(**vars(args))\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Core training params\n",
    "    parser.add_argument(\"--k\", type=int, default=1000)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--block_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--n_layer\", type=int, default=4)\n",
    "    parser.add_argument(\"--n_head\", type=int, default=4)\n",
    "    parser.add_argument(\"--n_embd\", type=int, default=128)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "\n",
    "    # Optimization\n",
    "    parser.add_argument(\"--lr\", type=float, default=3e-4)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-2)\n",
    "    parser.add_argument(\"--max_steps\", type=int, default=5000)\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=100)\n",
    "    parser.add_argument(\"--grad_clip\", type=float, default=1.0)\n",
    "\n",
    "    # Evaluation/checkpoints\n",
    "    parser.add_argument(\"--eval_interval\", type=int, default=500)\n",
    "    parser.add_argument(\"--eval_batches\", type=int, default=20)\n",
    "    parser.add_argument(\"--ckpt_interval\", type=int, default=1000)\n",
    "\n",
    "    # Misc\n",
    "    parser.add_argument(\"--no_amp\", action=\"store_true\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "\n",
    "    # Use parse_known_args to ignore --f=...json\n",
    "    if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "        args, _ = parser.parse_known_args()\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "# ============================= BPE Tokenizer ===============================\n",
    "\n",
    "def find_merges_file(k: int, verbose: bool = True) -> str:\n",
    "    candidates = [\n",
    "        os.path.join(GENERATED_DIR, f\"bpe_merges with k = {k}.txt\"),\n",
    "        os.path.join(GENERATED_DIR, f\"standard_bpe_merges_k{k}.txt\"),\n",
    "        os.path.join(GENERATED_DIR, f\"aggressive_clean_bpe_merges_k{k}.txt\"),\n",
    "        os.path.join(GENERATED_DIR, f\"bpe_merges_k{k}.txt\"),\n",
    "        os.path.join(GENERATED_DIR, f\"bpe_merges_k{k}_webtext_clean.txt\"),\n",
    "    ]\n",
    "    for path in candidates:\n",
    "        if os.path.exists(path):\n",
    "            if verbose:\n",
    "                print(f\"[Found] Using merges file: {path}\")\n",
    "            return path\n",
    "    raise FileNotFoundError(f\"No merges file found for k={k}. Tried: {candidates}\")\n",
    "\n",
    "def load_merges(merges_path: str) -> List[Tuple[str, str]]:\n",
    "    merges = []\n",
    "    with open(merges_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                merges.append((parts[0], parts[1]))\n",
    "    return merges\n",
    "\n",
    "def words_from_text(text: str, lowercase: bool = True) -> List[str]:\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    return [w for w in _wsre.split(text.strip()) if w]\n",
    "\n",
    "def apply_merges_to_word(word: str, merges: List[Tuple[str, str]]) -> List[str]:\n",
    "    symbols = tuple(list(word) + [WORD_END])\n",
    "    for a, b in merges:\n",
    "        out = []\n",
    "        i, L = 0, len(symbols)\n",
    "        while i < L:\n",
    "            if i < L-1 and symbols[i] == a and symbols[i+1] == b:\n",
    "                out.append(a + b); i += 2\n",
    "            else:\n",
    "                out.append(symbols[i]); i += 1\n",
    "        symbols = tuple(out)\n",
    "    return list(symbols)\n",
    "\n",
    "def tokenize_lines_with_merges(text: str, merges: List[Tuple[str, str]]) -> List[List[str]]:\n",
    "    token_lines: List[List[str]] = []\n",
    "    for line in text.strip().splitlines():\n",
    "        words = words_from_text(line)\n",
    "        if not words:\n",
    "            continue\n",
    "        toks: List[str] = []\n",
    "        for w in words:\n",
    "            toks.extend(apply_merges_to_word(w, merges))\n",
    "        toks.append(EOS)\n",
    "        token_lines.append(toks)\n",
    "    return token_lines\n",
    "\n",
    "# Convert tokens to ids, build vocab\n",
    "class BPETokenizer:\n",
    "    def __init__(self, merges: List[Tuple[str, str]], extra_tokens: Optional[List[str]] = None):\n",
    "        self.merges = merges\n",
    "        self.extra_tokens = extra_tokens or []\n",
    "        self.token_to_id: Dict[str, int] = {}\n",
    "        self.id_to_token: List[str] = []\n",
    "\n",
    "    def build_vocab_from_texts(self, texts: Dict[str, str]):\n",
    "        vocab = set()\n",
    "        for name, txt in texts.items():\n",
    "            for line in tokenize_lines_with_merges(txt, self.merges):\n",
    "                vocab.update(line)\n",
    "        vocab.update(self.extra_tokens)\n",
    "        # Deterministic order\n",
    "        self.id_to_token = sorted(vocab)\n",
    "        self.token_to_id = {t: i for i, t in enumerate(self.id_to_token)}\n",
    "\n",
    "    def encode_words(self, words: Iterable[str]) -> List[str]:\n",
    "        toks: List[str] = []\n",
    "        for w in (w.lower() for w in words):\n",
    "            toks.extend(apply_merges_to_word(w, self.merges))\n",
    "        return toks\n",
    "\n",
    "    def encode_lines(self, text: str) -> List[List[int]]:\n",
    "        lines_tok = tokenize_lines_with_merges(text, self.merges)\n",
    "        ids_lines: List[List[int]] = []\n",
    "        for line in lines_tok:\n",
    "            ids_lines.append([self.token_to_id[t] for t in line if t in self.token_to_id])\n",
    "        return ids_lines\n",
    "\n",
    "    def decode_tokens(self, token_stream: List[str]) -> List[str]:\n",
    "        words: List[str] = []\n",
    "        buf: List[str] = []\n",
    "        for t in token_stream:\n",
    "            if t == EOS:\n",
    "                break\n",
    "            buf.append(t)\n",
    "            if t.endswith(WORD_END):\n",
    "                chars: List[str] = []\n",
    "                for sub in buf:\n",
    "                    if sub.endswith(WORD_END):\n",
    "                        chars.extend(list(sub[:-len(WORD_END)]))\n",
    "                    else:\n",
    "                        chars.extend(list(sub))\n",
    "                words.append(\"\".join(chars))\n",
    "                buf = []\n",
    "        if buf:\n",
    "            chars = []\n",
    "            for sub in buf:\n",
    "                if sub.endswith(WORD_END):\n",
    "                    chars.extend(list(sub[:-len(WORD_END)]))\n",
    "                else:\n",
    "                    chars.extend(list(sub))\n",
    "            if chars:\n",
    "                words.append(\"\".join(chars))\n",
    "        return words\n",
    "\n",
    "    def save(self, path: str):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"id_to_token\": self.id_to_token,\n",
    "                \"token_to_id\": self.token_to_id,\n",
    "                \"extra_tokens\": self.extra_tokens,\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: str) -> \"BPETokenizer\":\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            obj = json.load(f)\n",
    "        tok = BPETokenizer(merges=[], extra_tokens=obj.get(\"extra_tokens\", []))\n",
    "        tok.id_to_token = obj[\"id_to_token\"]\n",
    "        tok.token_to_id = {k: int(v) for k, v in obj[\"token_to_id\"].items()}\n",
    "        return tok\n",
    "\n",
    "# ============================ Data preparation ==============================\n",
    "\n",
    "def read_text(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "@dataclass\n",
    "class EncodedSplits:\n",
    "    train: torch.Tensor\n",
    "    valid: torch.Tensor\n",
    "    test: torch.Tensor\n",
    "\n",
    "\n",
    "def build_or_load_encoded(run_dir: str, k: int) -> Tuple[BPETokenizer, EncodedSplits]:\n",
    "    enc_train_path = os.path.join(run_dir, \"train_encoded.pt\")\n",
    "    enc_valid_path = os.path.join(run_dir, \"valid_encoded.pt\")\n",
    "    enc_test_path  = os.path.join(run_dir, \"test_encoded.pt\")\n",
    "    tok_path       = os.path.join(run_dir, \"tokenizer.json\")\n",
    "\n",
    "    if all(os.path.exists(p) for p in [enc_train_path, enc_valid_path, enc_test_path, tok_path]):\n",
    "        print(\"[Load] Using cached encoded splits + tokenizer\")\n",
    "        tokenizer = BPETokenizer.load(tok_path)\n",
    "        train_ids = torch.load(enc_train_path)\n",
    "        valid_ids = torch.load(enc_valid_path)\n",
    "        test_ids  = torch.load(enc_test_path)\n",
    "        return tokenizer, EncodedSplits(train_ids, valid_ids, test_ids)\n",
    "\n",
    "    # Build from raw\n",
    "    merges_path = find_merges_file(k, verbose=True)\n",
    "    merges = load_merges(merges_path)\n",
    "\n",
    "    train_txt = read_text(os.path.join(CORPUS_DIR, \"Shakespeare_clean_train.txt\"))\n",
    "    valid_txt = read_text(os.path.join(CORPUS_DIR, \"Shakespeare_clean_valid.txt\"))\n",
    "    test_txt  = read_text(os.path.join(CORPUS_DIR, \"Shakespeare_clean_test.txt\"))\n",
    "\n",
    "    tokenizer = BPETokenizer(merges=merges, extra_tokens=[BOS, EOS])\n",
    "    tokenizer.build_vocab_from_texts({\"train\": train_txt, \"valid\": valid_txt, \"test\": test_txt})\n",
    "\n",
    "    def flatten(lines: List[List[int]]) -> List[int]:\n",
    "        flat = []\n",
    "        for ln in lines: flat.extend(ln)\n",
    "        return flat\n",
    "\n",
    "    train_ids = torch.tensor(flatten(tokenizer.encode_lines(train_txt)), dtype=torch.long)\n",
    "    valid_ids = torch.tensor(flatten(tokenizer.encode_lines(valid_txt)), dtype=torch.long)\n",
    "    test_ids  = torch.tensor(flatten(tokenizer.encode_lines(test_txt)),  dtype=torch.long)\n",
    "\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    tokenizer.save(tok_path)\n",
    "    torch.save(train_ids, enc_train_path)\n",
    "    torch.save(valid_ids, enc_valid_path)\n",
    "    torch.save(test_ids,  enc_test_path)\n",
    "    print(f\"[Save] Encoded splits to {run_dir}\")\n",
    "    print(f\"[Info] Vocab size = {len(tokenizer.id_to_token)}\")\n",
    "    return tokenizer, EncodedSplits(train_ids, valid_ids, test_ids)\n",
    "\n",
    "# ================================ Dataset ==================================\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, ids: torch.Tensor, block_size: int):\n",
    "        self.ids = ids\n",
    "        self.block_size = block_size\n",
    "        # we will sample random start positions in __getitem__\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # approximate number of sequences of length block_size we can draw\n",
    "        return max(1, len(self.ids) - self.block_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ignore idx and sample randomly to add stochasticity\n",
    "        i = random.randint(0, len(self.ids) - self.block_size - 1)\n",
    "        x = self.ids[i : i + self.block_size]\n",
    "        y = self.ids[i + 1 : i + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "# ============================== GPT Model ==================================\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd: int, n_head: int, block_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.key = nn.Linear(n_embd, n_embd)\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        self.value = nn.Linear(n_embd, n_embd)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        # causal mask\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(k.size(-1))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v  # (B, n_head, T, head_dim)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd: int, n_head: int, block_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = CausalSelfAttention(n_embd, n_head, block_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_layer: int, n_head: int, n_embd: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte': nn.Embedding(vocab_size, n_embd),\n",
    "            'wpe': nn.Embedding(block_size, n_embd),\n",
    "            'drop': nn.Dropout(dropout),\n",
    "            'h': nn.ModuleList([Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)]),\n",
    "            'ln_f': nn.LayerNorm(n_embd),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.block_size\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        tok_emb = self.transformer['wte'](idx)\n",
    "        pos_emb = self.transformer['wpe'](pos)[None, :, :]\n",
    "        x = self.transformer['drop'](tok_emb + pos_emb)\n",
    "        for block in self.transformer['h']:\n",
    "            x = block(x)\n",
    "        x = self.transformer['ln_f'](x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "# ============================ Training utilities ============================\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# ============================== Evaluation ==================================\n",
    "\n",
    "def evaluate(model: GPT, loader: DataLoader, device: torch.device, max_batches: int) -> float:\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            if i >= max_batches:\n",
    "                break\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            losses.append(loss.item())\n",
    "    model.train()\n",
    "    return float(sum(losses) / max(1, len(losses)))\n",
    "\n",
    "# ============================== Generation ==================================\n",
    "\n",
    "def generate(model: GPT, start_tokens: List[int], max_new_tokens: int, temperature: float = 1.0, top_k: Optional[int] = None) -> List[int]:\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    idx = torch.tensor(start_tokens, dtype=torch.long, device=device)[None, :]\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -model.block_size:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / max(1e-8, temperature)\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, next_id), dim=1)\n",
    "    return idx[0].tolist()\n",
    "\n",
    "# ============================== Plot / CSV ==================================\n",
    "\n",
    "def save_plot_and_csv(run_dir, history):\n",
    "    \"\"\"Save training/validation loss plot and history CSV.\"\"\"\n",
    "    # make sure run_dir and subdirs exist\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(run_dir, \"samples\"), exist_ok=True)\n",
    "\n",
    "    csv_path = os.path.join(run_dir, \"history.csv\")\n",
    "    png_path = os.path.join(run_dir, \"history.png\")\n",
    "\n",
    "    # Save CSV\n",
    "    df = pd.DataFrame(history)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    # Try plotting\n",
    "    try:\n",
    "        # Force matplotlib to use a safe font\n",
    "        matplotlib.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "\n",
    "        steps = [h[\"step\"] for h in history]\n",
    "        train_loss = [h[\"train_loss\"] for h in history]\n",
    "        val_loss = [h[\"val_loss\"] for h in history]\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(steps, train_loss, label=\"train_loss\")\n",
    "        plt.plot(steps, val_loss, label=\"val_loss\")\n",
    "        plt.xlabel(\"step\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(png_path)\n",
    "        plt.close()\n",
    "        print(f\"[Plot] Saved to {png_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Plot Warning] Could not generate plot: {e}\")\n",
    "\n",
    "\n",
    "# ============================== Main training ===============================\n",
    "\n",
    "def train_and_eval_with_logging(cfg: TrainConfig):\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(\"runs\", f\"gpt_{ts}_k{cfg.k}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(run_dir, \"samples\"), exist_ok=True)\n",
    "\n",
    "    # Save config\n",
    "    with open(os.path.join(run_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(asdict(cfg), f, indent=2)\n",
    "\n",
    "    # Encode or load\n",
    "    tokenizer, splits = build_or_load_encoded(run_dir, cfg.k)\n",
    "    vocab_size = len(tokenizer.id_to_token)\n",
    "\n",
    "    # Datasets/loaders\n",
    "    train_ds = GPTDataset(splits.train, cfg.block_size)\n",
    "    valid_ds = GPTDataset(splits.valid, cfg.block_size)\n",
    "    test_ds  = GPTDataset(splits.test,  cfg.block_size)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=cfg.batch_size, shuffle=False, drop_last=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    # Model\n",
    "    model = GPT(vocab_size=vocab_size, block_size=cfg.block_size, n_layer=cfg.n_layer, n_head=cfg.n_head, n_embd=cfg.n_embd, dropout=cfg.dropout)\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"[Info] Device: {device} | Parameters: {count_parameters(model)/1e6:.2f}M | Vocab={vocab_size} | block={cfg.block_size}\")\n",
    "\n",
    "    # Optimizer / Scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    lr_sched = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: min(1.0, step / max(1, cfg.warmup_steps))\n",
    "    )\n",
    "\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=cfg.amp and device.type == \"cuda\")\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = []\n",
    "    running_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    step = 0\n",
    "    while step < cfg.max_steps:\n",
    "        for xb, yb in train_loader:\n",
    "            step += 1\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\", enabled=scaler.is_enabled()):\n",
    "                logits = model(xb)\n",
    "                loss = loss_fn(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            if cfg.grad_clip:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            lr_sched.step()\n",
    "\n",
    "            running_loss = 0.9 * running_loss + 0.1 * loss.item() if step > 1 else loss.item()\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                print(f\"[Step {step:5d}] train_loss={loss.item():.4f} (ema {running_loss:.4f})\")\n",
    "\n",
    "\n",
    "            # Eval\n",
    "            if step % cfg.eval_interval == 0 or step == cfg.max_steps:\n",
    "                val_loss = evaluate(model, valid_loader, device, cfg.eval_batches)\n",
    "                val_ppl = math.exp(val_loss)\n",
    "                print(f\"[Eval  {step:5d}] val_loss={val_loss:.4f} | val_ppl={val_ppl:.2f}\")\n",
    "                history.append({\"step\": step, \"train_loss\": running_loss, \"val_loss\": val_loss, \"val_ppl\": val_ppl})\n",
    "                save_plot_and_csv(run_dir, history)\n",
    "\n",
    "                # save extrinsic evaluation with fixed prompts\n",
    "                prompts = [\"To be or not to be\", \"Once upon a midnight dreary\"]\n",
    "                eval_path = os.path.join(run_dir, f\"samples/step{step}_eval.txt\")\n",
    "                with open(eval_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    for prompt in prompts:\n",
    "                        start_ids = tokenizer.encode_words(prompt.split())\n",
    "                        start_ids = [tokenizer.token_to_id.get(tok, 0) for tok in start_ids]\n",
    "                        f.write(f\"\\nPrompt: {prompt}\\n\")\n",
    "\n",
    "                        greedy_ids = generate(model, start_tokens=start_ids, max_new_tokens=40, temperature=1.0, top_k=None)\n",
    "                        greedy_text = \" \".join([tokenizer.id_to_token[i] for i in greedy_ids])\n",
    "                        f.write(\"Greedy: \" + greedy_text + \"\\n\")\n",
    "\n",
    "                        topk_ids = generate(model, start_tokens=start_ids, max_new_tokens=40, temperature=0.8, top_k=50)\n",
    "                        topk_text = \" \".join([tokenizer.id_to_token[i] for i in topk_ids])\n",
    "                        f.write(\"Top-k: \" + topk_text + \"\\n\")\n",
    "                print(f\"[Sample Eval] saved → {eval_path}\")\n",
    "\n",
    "            # Checkpoint + sample\n",
    "            if step % cfg.ckpt_interval == 0 or step == cfg.max_steps:\n",
    "                ckpt_path = os.path.join(run_dir, f\"ckpt_step{step}.pt\")\n",
    "                torch.save({\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"optimizer_state\": optimizer.state_dict(),\n",
    "                    \"scaler_state\": scaler.state_dict(),\n",
    "                    \"config\": asdict(cfg),\n",
    "                    \"vocab_size\": vocab_size,\n",
    "                    \"step\": step,\n",
    "                }, ckpt_path)\n",
    "                print(f\"[Save] checkpoint → {ckpt_path}\")\n",
    "\n",
    "            if step >= cfg.max_steps:\n",
    "                break\n",
    "\n",
    "    # Final test evaluation\n",
    "    val_loss = evaluate(model, valid_loader, device, cfg.eval_batches)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    test_loss = evaluate(model, test_loader, device, cfg.eval_batches)\n",
    "    test_ppl = math.exp(test_loss)\n",
    "    \n",
    "    print(f\"[Final Val] loss={val_loss:.4f} | ppl={val_ppl:.2f}\")\n",
    "    print(f\"[Final Test] loss={test_loss:.4f} | ppl={test_ppl:.2f}\")\n",
    "\n",
    "    # Save final metrics\n",
    "    results = {\n",
    "        \"k\": cfg.k,\n",
    "        \"n_embd\": cfg.n_embd,\n",
    "        \"dropout\": cfg.dropout,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_ppl\": val_ppl,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_ppl\": test_ppl,\n",
    "    }\n",
    "    with open(os.path.join(run_dir, \"final_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = parse_args()   # returns a TrainConfig object\n",
    "    results = train_and_eval_with_logging(cfg)\n",
    "    save_plot_and_csv(os.path.join(\"runs\", f\"gpt_final_k{cfg.k}\"), [results])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e4c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"C:/Users/lehel/Desktop/UOS/Master/Sem2/Building_GPT_from_scratch/Building-GPT-from-Scratch/runs/gpt_20250831_200113_k1000/ckpt_step5000.pt\")\n",
    "tokenizer = BPETokenizer()\n",
    "prompt = \"To be\"\n",
    "# Convert words in the prompt to token IDs\n",
    "start_ids = [tokenizer.token_to_id.get(tok, 0) for tok in tokenizer.encode_words(prompt.split())]\n",
    "\n",
    "# Generate new tokens\n",
    "generated_ids = generate(model, start_tokens=start_ids, max_new_tokens=50, temperature=1.0, top_k=50)\n",
    "\n",
    "# Convert token IDs back to text\n",
    "generated_tokens = [tokenizer.id_to_token[i] for i in generated_ids]\n",
    "generated_text = \" \".join(generated_tokens)\n",
    "\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "530f680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_sweep():\n",
    "    # Sweep only dropout\n",
    "    sweep_params = {\n",
    "        \"dropout\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    }\n",
    "\n",
    "    keys, values = zip(*sweep_params.items())\n",
    "    experiments = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    # Fixed defaults for k and n_embd\n",
    "    default_k = 1000\n",
    "    default_n_embd = 128\n",
    "\n",
    "    for exp in experiments:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"Running experiment: k={default_k}, n_embd={default_n_embd}, dropout={exp['dropout']}\")\n",
    "\n",
    "        cfg = TrainConfig(\n",
    "            k=default_k,\n",
    "            n_embd=default_n_embd,\n",
    "            dropout=exp[\"dropout\"],\n",
    "            batch_size=32,\n",
    "            block_size=128,\n",
    "            n_layer=4,\n",
    "            n_head=4,\n",
    "            max_steps=1000,\n",
    "        )\n",
    "\n",
    "        res = train_and_eval_with_logging(cfg)\n",
    "        all_results.append(res)\n",
    "\n",
    "    # Save sweep results\n",
    "    sweep_csv = \"runs/hyperparam_sweep_results.csv\"\n",
    "    pd.DataFrame(all_results).to_csv(sweep_csv, index=False)\n",
    "    print(f\"\\n[Done] Sweep results saved → {sweep_csv}\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_history_from_csv(history_csv_path):\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    df = pd.read_csv(history_csv_path)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    if \"train_loss\" in df.columns:\n",
    "        plt.plot(df[\"step\"], df[\"train_loss\"], label=\"train_loss\")\n",
    "    if \"val_loss\" in df.columns:\n",
    "        plt.plot(df[\"step\"], df[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.xlabel(\"step\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    # Avoid tight_layout() if fonts are broken\n",
    "    png_path = history_csv_path.replace(\".csv\", \"_plot.png\")\n",
    "    plt.savefig(png_path)\n",
    "    plt.close()\n",
    "    print(f\"[Plot] Saved loss plot → {png_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09836d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Plot] Saved loss plot → C:/Users/lehel/Desktop/UOS/Master/Sem2/Building_GPT_from_scratch/Building-GPT-from-Scratch/runs/gpt_20250831_200113_k1000/history_plot.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = ['Arial', 'Liberation Sans', 'Tahoma']\n",
    "\n",
    "plot_history_from_csv(\"C:/Users/lehel/Desktop/UOS/Master/Sem2/Building_GPT_from_scratch/Building-GPT-from-Scratch/runs/gpt_20250831_200113_k1000/history.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab2e0c",
   "metadata": {},
   "source": [
    "Why Dropout?\n",
    "\n",
    "Dropout is a method used in neural networks to prevent overfitting. During training, it randomly switches off some neurons, so the model does not rely too much on any single part. This helps the network learn more general patterns that work well on new data. If dropout is too low, the model can memorize the training data and perform poorly on validation data. If it is too high, the model may not learn enough and perform badly on both training and validation data. Choosing the right dropout rate is important because it helps the model find a balance between learning the data and generalizing to new examples. It also interacts with other settings, like the size of the model and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57715805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running experiment: k=1000, n_embd=128, dropout=0.0\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "[Save] Encoded splits to runs\\gpt_20250831_223002_k1000\n",
      "[Info] Vocab size = 1028\n",
      "[Info] Device: cpu | Parameters: 1.07M | Vocab=1028 | block=128\n",
      "[Step    50] train_loss=6.2277 (ema 6.3745)\n",
      "[Step   100] train_loss=5.4465 (ema 5.6047)\n",
      "[Step   150] train_loss=4.8206 (ema 4.9408)\n",
      "[Step   200] train_loss=4.5385 (ema 4.6062)\n",
      "[Eval    200] val_loss=4.5475 | val_ppl=94.40\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_223002_k1000\\samples/step200_eval.txt\n",
      "[Step   250] train_loss=4.4228 (ema 4.4467)\n",
      "[Step   300] train_loss=4.2193 (ema 4.3085)\n",
      "[Step   350] train_loss=4.2382 (ema 4.2464)\n",
      "[Step   400] train_loss=4.1082 (ema 4.1492)\n",
      "[Eval    400] val_loss=4.1672 | val_ppl=64.54\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_223002_k1000\\samples/step400_eval.txt\n",
      "[Step   450] train_loss=4.0154 (ema 4.1063)\n",
      "[Step   500] train_loss=4.0019 (ema 4.0459)\n",
      "[Save] checkpoint → runs\\gpt_20250831_223002_k1000\\ckpt_step500.pt\n",
      "[Step   550] train_loss=4.0257 (ema 4.0155)\n",
      "[Step   600] train_loss=4.0395 (ema 3.9813)\n",
      "[Eval    600] val_loss=4.0103 | val_ppl=55.16\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_223002_k1000\\samples/step600_eval.txt\n",
      "[Step   650] train_loss=3.9151 (ema 3.9319)\n",
      "[Step   700] train_loss=3.9507 (ema 3.8813)\n",
      "[Step   750] train_loss=3.8603 (ema 3.8555)\n",
      "[Step   800] train_loss=3.8510 (ema 3.8285)\n",
      "[Eval    800] val_loss=3.8860 | val_ppl=48.72\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_223002_k1000\\samples/step800_eval.txt\n",
      "[Step   850] train_loss=3.8363 (ema 3.7902)\n",
      "[Step   900] train_loss=3.8593 (ema 3.7790)\n",
      "[Step   950] train_loss=3.6707 (ema 3.7207)\n",
      "[Step  1000] train_loss=3.7126 (ema 3.7065)\n",
      "[Eval   1000] val_loss=3.7683 | val_ppl=43.31\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_223002_k1000\\samples/step1000_eval.txt\n",
      "[Save] checkpoint → runs\\gpt_20250831_223002_k1000\\ckpt_step1000.pt\n",
      "[Final Val] loss=3.7763 | ppl=43.65\n",
      "[Final Test] loss=3.7919 | ppl=44.34\n",
      "\n",
      "============================================================\n",
      "Running experiment: k=1000, n_embd=128, dropout=0.1\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "[Save] Encoded splits to runs\\gpt_20250831_223734_k1000\n",
      "[Info] Vocab size = 1028\n",
      "[Info] Device: cpu | Parameters: 1.07M | Vocab=1028 | block=128\n",
      "[Step    50] train_loss=6.2341 (ema 6.4002)\n",
      "[Step   100] train_loss=5.5125 (ema 5.6415)\n",
      "[Step   150] train_loss=4.9241 (ema 4.9963)\n",
      "[Step   200] train_loss=4.6762 (ema 4.6904)\n",
      "[Eval    200] val_loss=4.6197 | val_ppl=101.46\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_223734_k1000\\samples/step200_eval.txt\n",
      "[Step   250] train_loss=4.4233 (ema 4.4997)\n",
      "[Step   300] train_loss=4.3223 (ema 4.3656)\n",
      "[Step   350] train_loss=4.1934 (ema 4.2696)\n",
      "[Step   400] train_loss=4.2810 (ema 4.2040)\n",
      "[Eval    400] val_loss=4.2022 | val_ppl=66.83\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_223734_k1000\\samples/step400_eval.txt\n",
      "[Step   450] train_loss=4.0341 (ema 4.1060)\n",
      "[Step   500] train_loss=4.0235 (ema 4.0735)\n",
      "[Save] checkpoint → runs\\gpt_20250831_223734_k1000\\ckpt_step500.pt\n",
      "[Step   550] train_loss=3.9856 (ema 4.0190)\n",
      "[Step   600] train_loss=3.9657 (ema 3.9767)\n",
      "[Eval    600] val_loss=4.0313 | val_ppl=56.33\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_223734_k1000\\samples/step600_eval.txt\n",
      "[Step   650] train_loss=3.8336 (ema 3.9431)\n",
      "[Step   700] train_loss=3.9235 (ema 3.9284)\n",
      "[Step   750] train_loss=3.8609 (ema 3.8752)\n",
      "[Step   800] train_loss=3.7617 (ema 3.8089)\n",
      "[Eval    800] val_loss=3.8935 | val_ppl=49.08\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_223734_k1000\\samples/step800_eval.txt\n",
      "[Step   850] train_loss=3.7862 (ema 3.7896)\n",
      "[Step   900] train_loss=3.7785 (ema 3.7626)\n",
      "[Step   950] train_loss=3.6915 (ema 3.7239)\n",
      "[Step  1000] train_loss=3.6171 (ema 3.6944)\n",
      "[Eval   1000] val_loss=3.7940 | val_ppl=44.43\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_223734_k1000\\samples/step1000_eval.txt\n",
      "[Save] checkpoint → runs\\gpt_20250831_223734_k1000\\ckpt_step1000.pt\n",
      "[Final Val] loss=3.7636 | ppl=43.10\n",
      "[Final Test] loss=3.7708 | ppl=43.41\n",
      "\n",
      "============================================================\n",
      "Running experiment: k=1000, n_embd=128, dropout=0.2\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "[Save] Encoded splits to runs\\gpt_20250831_224515_k1000\n",
      "[Info] Vocab size = 1028\n",
      "[Info] Device: cpu | Parameters: 1.07M | Vocab=1028 | block=128\n",
      "[Step    50] train_loss=6.2905 (ema 6.4257)\n",
      "[Step   100] train_loss=5.5853 (ema 5.6694)\n",
      "[Step   150] train_loss=5.0370 (ema 5.1060)\n",
      "[Step   200] train_loss=4.6939 (ema 4.7855)\n",
      "[Eval    200] val_loss=4.6939 | val_ppl=109.28\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_224515_k1000\\samples/step200_eval.txt\n",
      "[Step   250] train_loss=4.5604 (ema 4.5471)\n",
      "[Step   300] train_loss=4.4684 (ema 4.4099)\n",
      "[Step   350] train_loss=4.3223 (ema 4.3104)\n",
      "[Step   400] train_loss=4.2529 (ema 4.2379)\n",
      "[Eval    400] val_loss=4.2361 | val_ppl=69.14\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_224515_k1000\\samples/step400_eval.txt\n",
      "[Step   450] train_loss=4.1473 (ema 4.1469)\n",
      "[Step   500] train_loss=4.0162 (ema 4.0852)\n",
      "[Save] checkpoint → runs\\gpt_20250831_224515_k1000\\ckpt_step500.pt\n",
      "[Step   550] train_loss=4.0520 (ema 4.0431)\n",
      "[Step   600] train_loss=3.9804 (ema 4.0019)\n",
      "[Eval    600] val_loss=4.0409 | val_ppl=56.88\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_224515_k1000\\samples/step600_eval.txt\n",
      "[Step   650] train_loss=3.8623 (ema 3.9644)\n",
      "[Step   700] train_loss=3.8526 (ema 3.9128)\n",
      "[Step   750] train_loss=3.8199 (ema 3.8728)\n",
      "[Step   800] train_loss=3.8404 (ema 3.8491)\n",
      "[Eval    800] val_loss=3.9228 | val_ppl=50.54\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_224515_k1000\\samples/step800_eval.txt\n",
      "[Step   850] train_loss=3.8327 (ema 3.8115)\n",
      "[Step   900] train_loss=3.7630 (ema 3.7942)\n",
      "[Step   950] train_loss=3.7465 (ema 3.7407)\n",
      "[Step  1000] train_loss=3.6713 (ema 3.6988)\n",
      "[Eval   1000] val_loss=3.8095 | val_ppl=45.13\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_224515_k1000\\samples/step1000_eval.txt\n",
      "[Save] checkpoint → runs\\gpt_20250831_224515_k1000\\ckpt_step1000.pt\n",
      "[Final Val] loss=3.7959 | ppl=44.52\n",
      "[Final Test] loss=3.7943 | ppl=44.45\n",
      "\n",
      "============================================================\n",
      "Running experiment: k=1000, n_embd=128, dropout=0.3\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "[Save] Encoded splits to runs\\gpt_20250831_225252_k1000\n",
      "[Info] Vocab size = 1028\n",
      "[Info] Device: cpu | Parameters: 1.07M | Vocab=1028 | block=128\n",
      "[Step    50] train_loss=6.2974 (ema 6.4532)\n",
      "[Step   100] train_loss=5.6273 (ema 5.7066)\n",
      "[Step   150] train_loss=5.0880 (ema 5.2164)\n",
      "[Step   200] train_loss=4.8724 (ema 4.8791)\n",
      "[Eval    200] val_loss=4.7689 | val_ppl=117.79\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_225252_k1000\\samples/step200_eval.txt\n",
      "[Step   250] train_loss=4.5715 (ema 4.6250)\n",
      "[Step   300] train_loss=4.4618 (ema 4.4835)\n",
      "[Step   350] train_loss=4.3139 (ema 4.3511)\n",
      "[Step   400] train_loss=4.1471 (ema 4.2733)\n",
      "[Eval    400] val_loss=4.2514 | val_ppl=70.20\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_225252_k1000\\samples/step400_eval.txt\n",
      "[Step   450] train_loss=4.2739 (ema 4.1930)\n",
      "[Step   500] train_loss=4.1521 (ema 4.1277)\n",
      "[Save] checkpoint → runs\\gpt_20250831_225252_k1000\\ckpt_step500.pt\n",
      "[Step   550] train_loss=4.1429 (ema 4.0692)\n",
      "[Step   600] train_loss=4.0724 (ema 4.0225)\n",
      "[Eval    600] val_loss=4.0671 | val_ppl=58.39\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_225252_k1000\\samples/step600_eval.txt\n",
      "[Step   650] train_loss=4.0577 (ema 3.9860)\n",
      "[Step   700] train_loss=3.9603 (ema 3.9441)\n",
      "[Step   750] train_loss=3.9468 (ema 3.9037)\n",
      "[Step   800] train_loss=3.8230 (ema 3.8729)\n",
      "[Eval    800] val_loss=3.9127 | val_ppl=50.03\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_225252_k1000\\samples/step800_eval.txt\n",
      "[Step   850] train_loss=3.8312 (ema 3.8180)\n",
      "[Step   900] train_loss=3.8119 (ema 3.7918)\n",
      "[Step   950] train_loss=3.8150 (ema 3.7564)\n",
      "[Step  1000] train_loss=3.7828 (ema 3.7300)\n",
      "[Eval   1000] val_loss=3.8101 | val_ppl=45.16\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_225252_k1000\\samples/step1000_eval.txt\n",
      "[Save] checkpoint → runs\\gpt_20250831_225252_k1000\\ckpt_step1000.pt\n",
      "[Final Val] loss=3.8227 | ppl=45.73\n",
      "[Final Test] loss=3.8044 | ppl=44.90\n",
      "\n",
      "============================================================\n",
      "Running experiment: k=1000, n_embd=128, dropout=0.4\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "[Save] Encoded splits to runs\\gpt_20250831_230047_k1000\n",
      "[Info] Vocab size = 1028\n",
      "[Info] Device: cpu | Parameters: 1.07M | Vocab=1028 | block=128\n",
      "[Step    50] train_loss=6.3652 (ema 6.4809)\n",
      "[Step   100] train_loss=5.6465 (ema 5.7313)\n",
      "[Step   150] train_loss=5.3181 (ema 5.3561)\n",
      "[Step   200] train_loss=4.9658 (ema 4.9844)\n",
      "[Eval    200] val_loss=4.8301 | val_ppl=125.23\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lehel\\AppData\\Local\\Temp\\ipykernel_28348\\1522189915.py:522: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(8, 5))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sample Eval] saved → runs\\gpt_20250831_230047_k1000\\samples/step200_eval.txt\n",
      "[Step   250] train_loss=4.6301 (ema 4.6973)\n",
      "[Step   300] train_loss=4.5556 (ema 4.5288)\n",
      "[Step   350] train_loss=4.4958 (ema 4.4059)\n",
      "[Step   400] train_loss=4.2571 (ema 4.2759)\n",
      "[Eval    400] val_loss=4.3106 | val_ppl=74.49\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_230047_k1000\\samples/step400_eval.txt\n",
      "[Step   450] train_loss=4.1352 (ema 4.2186)\n",
      "[Step   500] train_loss=4.1244 (ema 4.1262)\n",
      "[Save] checkpoint → runs\\gpt_20250831_230047_k1000\\ckpt_step500.pt\n",
      "[Step   550] train_loss=4.0768 (ema 4.0768)\n",
      "[Step   600] train_loss=4.0269 (ema 4.0474)\n",
      "[Eval    600] val_loss=4.0565 | val_ppl=57.77\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_230047_k1000\\samples/step600_eval.txt\n",
      "[Step   650] train_loss=3.9748 (ema 3.9783)\n",
      "[Step   700] train_loss=3.9288 (ema 3.9462)\n",
      "[Step   750] train_loss=3.8447 (ema 3.9146)\n",
      "[Step   800] train_loss=3.9260 (ema 3.8701)\n",
      "[Eval    800] val_loss=3.9443 | val_ppl=51.64\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_230047_k1000\\samples/step800_eval.txt\n",
      "[Step   850] train_loss=3.8199 (ema 3.8402)\n",
      "[Step   900] train_loss=3.7470 (ema 3.7791)\n",
      "[Step   950] train_loss=3.7076 (ema 3.7636)\n",
      "[Step  1000] train_loss=3.6625 (ema 3.7139)\n",
      "[Eval   1000] val_loss=3.8061 | val_ppl=44.98\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_230047_k1000\\samples/step1000_eval.txt\n",
      "[Save] checkpoint → runs\\gpt_20250831_230047_k1000\\ckpt_step1000.pt\n",
      "[Final Val] loss=3.7928 | ppl=44.38\n",
      "[Final Test] loss=3.8031 | ppl=44.84\n",
      "\n",
      "============================================================\n",
      "Running experiment: k=1000, n_embd=128, dropout=0.5\n",
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "[Save] Encoded splits to runs\\gpt_20250831_230822_k1000\n",
      "[Info] Vocab size = 1028\n",
      "[Info] Device: cpu | Parameters: 1.07M | Vocab=1028 | block=128\n",
      "[Step    50] train_loss=6.3665 (ema 6.5130)\n",
      "[Step   100] train_loss=5.6217 (ema 5.7490)\n",
      "[Step   150] train_loss=5.4648 (ema 5.4609)\n",
      "[Step   200] train_loss=5.0479 (ema 5.1343)\n",
      "[Eval    200] val_loss=4.9727 | val_ppl=144.42\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_230822_k1000\\samples/step200_eval.txt\n",
      "[Step   250] train_loss=4.7266 (ema 4.7786)\n",
      "[Step   300] train_loss=4.6063 (ema 4.6089)\n",
      "[Step   350] train_loss=4.4691 (ema 4.4600)\n",
      "[Step   400] train_loss=4.2922 (ema 4.3259)\n",
      "[Eval    400] val_loss=4.3045 | val_ppl=74.03\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_230822_k1000\\samples/step400_eval.txt\n",
      "[Step   450] train_loss=4.2449 (ema 4.2468)\n",
      "[Step   500] train_loss=4.1194 (ema 4.1684)\n",
      "[Save] checkpoint → runs\\gpt_20250831_230822_k1000\\ckpt_step500.pt\n",
      "[Step   550] train_loss=4.1709 (ema 4.1217)\n",
      "[Step   600] train_loss=4.0303 (ema 4.0579)\n",
      "[Eval    600] val_loss=4.0889 | val_ppl=59.67\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_230822_k1000\\samples/step600_eval.txt\n",
      "[Step   650] train_loss=4.0069 (ema 4.0070)\n",
      "[Step   700] train_loss=3.9179 (ema 3.9500)\n",
      "[Step   750] train_loss=3.9350 (ema 3.9072)\n",
      "[Step   800] train_loss=3.9745 (ema 3.8954)\n",
      "[Eval    800] val_loss=3.9313 | val_ppl=50.97\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_230822_k1000\\samples/step800_eval.txt\n",
      "[Step   850] train_loss=3.8892 (ema 3.8523)\n",
      "[Step   900] train_loss=3.8143 (ema 3.8100)\n",
      "[Step   950] train_loss=3.7049 (ema 3.7681)\n",
      "[Step  1000] train_loss=3.7476 (ema 3.7232)\n",
      "[Eval   1000] val_loss=3.8170 | val_ppl=45.47\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_230822_k1000\\samples/step1000_eval.txt\n",
      "[Save] checkpoint → runs\\gpt_20250831_230822_k1000\\ckpt_step1000.pt\n",
      "[Final Val] loss=3.8168 | ppl=45.46\n",
      "[Final Test] loss=3.8308 | ppl=46.10\n",
      "\n",
      "[Done] Sweep results saved → runs/hyperparam_sweep_results.csv\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can not load face (invalid stream operation; error code 0x55)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m hyperparam_sweep()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m plot_history_from_csv(\u001b[33m\"\u001b[39m\u001b[33mC:/Users/lehel/Desktop/UOS/Master/Sem2/Building_GPT_from_scratch/Building-GPT-from-Scratch/runs/gpt_20250831_200113_k1000/history.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mplot_history_from_csv\u001b[39m\u001b[34m(history_csv_path)\u001b[39m\n\u001b[32m     56\u001b[39m plt.ylabel(\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m plt.legend()\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m plt.tight_layout()\n\u001b[32m     59\u001b[39m png_path = history_csv_path.replace(\u001b[33m\"\u001b[39m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_plot.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m plt.savefig(png_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\pyplot.py:2835\u001b[39m, in \u001b[36mtight_layout\u001b[39m\u001b[34m(pad, h_pad, w_pad, rect)\u001b[39m\n\u001b[32m   2827\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Figure.tight_layout)\n\u001b[32m   2828\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtight_layout\u001b[39m(\n\u001b[32m   2829\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2833\u001b[39m     rect: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2834\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2835\u001b[39m     gcf().tight_layout(pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\figure.py:3640\u001b[39m, in \u001b[36mFigure.tight_layout\u001b[39m\u001b[34m(self, pad, h_pad, w_pad, rect)\u001b[39m\n\u001b[32m   3638\u001b[39m previous_engine = \u001b[38;5;28mself\u001b[39m.get_layout_engine()\n\u001b[32m   3639\u001b[39m \u001b[38;5;28mself\u001b[39m.set_layout_engine(engine)\n\u001b[32m-> \u001b[39m\u001b[32m3640\u001b[39m engine.execute(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   3641\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m previous_engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m   3642\u001b[39m     previous_engine, (TightLayoutEngine, PlaceHolderLayoutEngine)\n\u001b[32m   3643\u001b[39m ):\n\u001b[32m   3644\u001b[39m     _api.warn_external(\u001b[33m'\u001b[39m\u001b[33mThe figure layout has changed to tight\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\layout_engine.py:183\u001b[39m, in \u001b[36mTightLayoutEngine.execute\u001b[39m\u001b[34m(self, fig)\u001b[39m\n\u001b[32m    181\u001b[39m renderer = fig._get_renderer()\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[33m\"\u001b[39m\u001b[33m_draw_disabled\u001b[39m\u001b[33m\"\u001b[39m, nullcontext)():\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     kwargs = get_tight_layout_figure(\n\u001b[32m    184\u001b[39m         fig, fig.axes, get_subplotspec_list(fig.axes), renderer,\n\u001b[32m    185\u001b[39m         pad=info[\u001b[33m'\u001b[39m\u001b[33mpad\u001b[39m\u001b[33m'\u001b[39m], h_pad=info[\u001b[33m'\u001b[39m\u001b[33mh_pad\u001b[39m\u001b[33m'\u001b[39m], w_pad=info[\u001b[33m'\u001b[39m\u001b[33mw_pad\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    186\u001b[39m         rect=info[\u001b[33m'\u001b[39m\u001b[33mrect\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[32m    188\u001b[39m     fig.subplots_adjust(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\_tight_layout.py:266\u001b[39m, in \u001b[36mget_tight_layout_figure\u001b[39m\u001b[34m(fig, axes_list, subplotspec_list, renderer, pad, h_pad, w_pad, rect)\u001b[39m\n\u001b[32m    261\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[32m    262\u001b[39m     span_pairs.append((\n\u001b[32m    263\u001b[39m         \u001b[38;5;28mslice\u001b[39m(ss.rowspan.start * div_row, ss.rowspan.stop * div_row),\n\u001b[32m    264\u001b[39m         \u001b[38;5;28mslice\u001b[39m(ss.colspan.start * div_col, ss.colspan.stop * div_col)))\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m kwargs = _auto_adjust_subplotpars(fig, renderer,\n\u001b[32m    267\u001b[39m                                   shape=(max_nrows, max_ncols),\n\u001b[32m    268\u001b[39m                                   span_pairs=span_pairs,\n\u001b[32m    269\u001b[39m                                   subplot_list=subplot_list,\n\u001b[32m    270\u001b[39m                                   ax_bbox_list=ax_bbox_list,\n\u001b[32m    271\u001b[39m                                   pad=pad, h_pad=h_pad, w_pad=w_pad)\n\u001b[32m    273\u001b[39m \u001b[38;5;66;03m# kwargs can be none if tight_layout fails...\u001b[39;00m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# if rect is given, the whole subplots area (including\u001b[39;00m\n\u001b[32m    276\u001b[39m     \u001b[38;5;66;03m# labels) will fit into the rect instead of the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    280\u001b[39m     \u001b[38;5;66;03m# auto_adjust_subplotpars twice, where the second run\u001b[39;00m\n\u001b[32m    281\u001b[39m     \u001b[38;5;66;03m# with adjusted rect parameters.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\_tight_layout.py:82\u001b[39m, in \u001b[36m_auto_adjust_subplotpars\u001b[39m\u001b[34m(fig, renderer, shape, span_pairs, subplot_list, ax_bbox_list, pad, h_pad, w_pad, rect)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m subplots:\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ax.get_visible():\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m         bb += [martist._get_tightbbox_for_layout_only(ax, renderer)]\n\u001b[32m     84\u001b[39m tight_bbox_raw = Bbox.union(bb)\n\u001b[32m     85\u001b[39m tight_bbox = fig.transFigure.inverted().transform_bbox(tight_bbox_raw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\artist.py:1402\u001b[39m, in \u001b[36m_get_tightbbox_for_layout_only\u001b[39m\u001b[34m(obj, *args, **kwargs)\u001b[39m\n\u001b[32m   1396\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1397\u001b[39m \u001b[33;03mMatplotlib's `.Axes.get_tightbbox` and `.Axis.get_tightbbox` support a\u001b[39;00m\n\u001b[32m   1398\u001b[39m \u001b[33;03m*for_layout_only* kwarg; this helper tries to use the kwarg but skips it\u001b[39;00m\n\u001b[32m   1399\u001b[39m \u001b[33;03mwhen encountering third-party subclasses that do not support it.\u001b[39;00m\n\u001b[32m   1400\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.get_tightbbox(*args, **{**kwargs, \u001b[33m\"\u001b[39m\u001b[33mfor_layout_only\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m})\n\u001b[32m   1403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1404\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.get_tightbbox(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\axes\\_base.py:4519\u001b[39m, in \u001b[36m_AxesBase.get_tightbbox\u001b[39m\u001b[34m(self, renderer, call_axes_locator, bbox_extra_artists, for_layout_only)\u001b[39m\n\u001b[32m   4517\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._axis_map.values():\n\u001b[32m   4518\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.axison \u001b[38;5;129;01mand\u001b[39;00m axis.get_visible():\n\u001b[32m-> \u001b[39m\u001b[32m4519\u001b[39m         ba = martist._get_tightbbox_for_layout_only(axis, renderer)\n\u001b[32m   4520\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m ba:\n\u001b[32m   4521\u001b[39m             bb.append(ba)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\artist.py:1402\u001b[39m, in \u001b[36m_get_tightbbox_for_layout_only\u001b[39m\u001b[34m(obj, *args, **kwargs)\u001b[39m\n\u001b[32m   1396\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1397\u001b[39m \u001b[33;03mMatplotlib's `.Axes.get_tightbbox` and `.Axis.get_tightbbox` support a\u001b[39;00m\n\u001b[32m   1398\u001b[39m \u001b[33;03m*for_layout_only* kwarg; this helper tries to use the kwarg but skips it\u001b[39;00m\n\u001b[32m   1399\u001b[39m \u001b[33;03mwhen encountering third-party subclasses that do not support it.\u001b[39;00m\n\u001b[32m   1400\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.get_tightbbox(*args, **{**kwargs, \u001b[33m\"\u001b[39m\u001b[33mfor_layout_only\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m})\n\u001b[32m   1403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1404\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.get_tightbbox(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\axis.py:1364\u001b[39m, in \u001b[36mAxis.get_tightbbox\u001b[39m\u001b[34m(self, renderer, for_layout_only)\u001b[39m\n\u001b[32m   1361\u001b[39m     renderer = \u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m)._get_renderer()\n\u001b[32m   1362\u001b[39m ticks_to_draw = \u001b[38;5;28mself\u001b[39m._update_ticks()\n\u001b[32m-> \u001b[39m\u001b[32m1364\u001b[39m \u001b[38;5;28mself\u001b[39m._update_label_position(renderer)\n\u001b[32m   1366\u001b[39m \u001b[38;5;66;03m# go back to just this axis's tick labels\u001b[39;00m\n\u001b[32m   1367\u001b[39m tlb1, tlb2 = \u001b[38;5;28mself\u001b[39m._get_ticklabel_bboxes(ticks_to_draw, renderer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\axis.py:2459\u001b[39m, in \u001b[36mXAxis._update_label_position\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m   2455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2457\u001b[39m \u001b[38;5;66;03m# get bounding boxes for this axis and any siblings\u001b[39;00m\n\u001b[32m   2458\u001b[39m \u001b[38;5;66;03m# that have been set by `fig.align_xlabels()`\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2459\u001b[39m bboxes, bboxes2 = \u001b[38;5;28mself\u001b[39m._get_tick_boxes_siblings(renderer=renderer)\n\u001b[32m   2460\u001b[39m x, y = \u001b[38;5;28mself\u001b[39m.label.get_position()\n\u001b[32m   2462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.label_position == \u001b[33m'\u001b[39m\u001b[33mbottom\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m   2463\u001b[39m     \u001b[38;5;66;03m# Union with extents of the bottom spine if present, of the axes otherwise.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\axis.py:2252\u001b[39m, in \u001b[36mAxis._get_tick_boxes_siblings\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m   2250\u001b[39m axis = ax._axis_map[name]\n\u001b[32m   2251\u001b[39m ticks_to_draw = axis._update_ticks()\n\u001b[32m-> \u001b[39m\u001b[32m2252\u001b[39m tlb, tlb2 = axis._get_ticklabel_bboxes(ticks_to_draw, renderer)\n\u001b[32m   2253\u001b[39m bboxes.extend(tlb)\n\u001b[32m   2254\u001b[39m bboxes2.extend(tlb2)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\axis.py:1343\u001b[39m, in \u001b[36mAxis._get_ticklabel_bboxes\u001b[39m\u001b[34m(self, ticks, renderer)\u001b[39m\n\u001b[32m   1341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1342\u001b[39m     renderer = \u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m)._get_renderer()\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ([tick.label1.get_window_extent(renderer)\n\u001b[32m   1344\u001b[39m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick.label1.get_visible()],\n\u001b[32m   1345\u001b[39m         [tick.label2.get_window_extent(renderer)\n\u001b[32m   1346\u001b[39m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick.label2.get_visible()])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\axis.py:1343\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1342\u001b[39m     renderer = \u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m)._get_renderer()\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ([tick.label1.get_window_extent(renderer)\n\u001b[32m   1344\u001b[39m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick.label1.get_visible()],\n\u001b[32m   1345\u001b[39m         [tick.label2.get_window_extent(renderer)\n\u001b[32m   1346\u001b[39m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick.label2.get_visible()])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\text.py:969\u001b[39m, in \u001b[36mText.get_window_extent\u001b[39m\u001b[34m(self, renderer, dpi)\u001b[39m\n\u001b[32m    964\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot get window extent of text w/o renderer. You likely \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    966\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwant to call \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfigure.draw_without_rendering()\u001b[39m\u001b[33m'\u001b[39m\u001b[33m first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m cbook._setattr_cm(fig, dpi=dpi):\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     bbox, info, descent = \u001b[38;5;28mself\u001b[39m._get_layout(\u001b[38;5;28mself\u001b[39m._renderer)\n\u001b[32m    970\u001b[39m     x, y = \u001b[38;5;28mself\u001b[39m.get_unitless_position()\n\u001b[32m    971\u001b[39m     x, y = \u001b[38;5;28mself\u001b[39m.get_transform().transform((x, y))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\text.py:373\u001b[39m, in \u001b[36mText._get_layout\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m    370\u001b[39m ys = []\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# Full vertical extent of font, including ascenders and descenders:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m _, lp_h, lp_d = _get_text_metrics_with_cache(\n\u001b[32m    374\u001b[39m     renderer, \u001b[33m\"\u001b[39m\u001b[33mlp\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._fontproperties,\n\u001b[32m    375\u001b[39m     ismath=\u001b[33m\"\u001b[39m\u001b[33mTeX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_usetex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    376\u001b[39m     dpi=\u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m).dpi)\n\u001b[32m    377\u001b[39m min_dy = (lp_h - lp_d) * \u001b[38;5;28mself\u001b[39m._linespacing\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lines):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\text.py:69\u001b[39m, in \u001b[36m_get_text_metrics_with_cache\u001b[39m\u001b[34m(renderer, text, fontprop, ismath, dpi)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Call ``renderer.get_text_width_height_descent``, caching the results.\"\"\"\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Cached based on a copy of fontprop so that later in-place mutations of\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# the passed-in argument do not mess up the cache.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _get_text_metrics_with_cache_impl(\n\u001b[32m     70\u001b[39m     weakref.ref(renderer), text, fontprop.copy(), ismath, dpi)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\text.py:77\u001b[39m, in \u001b[36m_get_text_metrics_with_cache_impl\u001b[39m\u001b[34m(renderer_ref, text, fontprop, ismath, dpi)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache(\u001b[32m4096\u001b[39m)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_text_metrics_with_cache_impl\u001b[39m(\n\u001b[32m     75\u001b[39m         renderer_ref, text, fontprop, ismath, dpi):\n\u001b[32m     76\u001b[39m     \u001b[38;5;66;03m# dpi is unused, but participates in cache invalidation (via the renderer).\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m renderer_ref().get_text_width_height_descent(text, fontprop, ismath)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:218\u001b[39m, in \u001b[36mRendererAgg.get_text_width_height_descent\u001b[39m\u001b[34m(self, s, prop, ismath)\u001b[39m\n\u001b[32m    214\u001b[39m     ox, oy, width, height, descent, font_image = \\\n\u001b[32m    215\u001b[39m         \u001b[38;5;28mself\u001b[39m.mathtext_parser.parse(s, \u001b[38;5;28mself\u001b[39m.dpi, prop)\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m width, height, descent\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m font = \u001b[38;5;28mself\u001b[39m._prepare_font(prop)\n\u001b[32m    219\u001b[39m font.set_text(s, \u001b[32m0.0\u001b[39m, flags=get_hinting_flag())\n\u001b[32m    220\u001b[39m w, h = font.get_width_height()  \u001b[38;5;66;03m# width and height of unrotated string\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:252\u001b[39m, in \u001b[36mRendererAgg._prepare_font\u001b[39m\u001b[34m(self, font_prop)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_font\u001b[39m(\u001b[38;5;28mself\u001b[39m, font_prop):\n\u001b[32m    249\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[33;03m    Get the `.FT2Font` for *font_prop*, clear its buffer, and set its size.\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     font = get_font(_fontManager._find_fonts_by_props(font_prop))\n\u001b[32m    253\u001b[39m     font.clear()\n\u001b[32m    254\u001b[39m     size = font_prop.get_size_in_points()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\font_manager.py:1615\u001b[39m, in \u001b[36mget_font\u001b[39m\u001b[34m(font_filepaths, hinting_factor)\u001b[39m\n\u001b[32m   1612\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hinting_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1613\u001b[39m     hinting_factor = mpl.rcParams[\u001b[33m'\u001b[39m\u001b[33mtext.hinting_factor\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m1615\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _get_font(\n\u001b[32m   1616\u001b[39m     \u001b[38;5;66;03m# must be a tuple to be cached\u001b[39;00m\n\u001b[32m   1617\u001b[39m     paths,\n\u001b[32m   1618\u001b[39m     hinting_factor,\n\u001b[32m   1619\u001b[39m     _kerning_factor=mpl.rcParams[\u001b[33m'\u001b[39m\u001b[33mtext.kerning_factor\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   1620\u001b[39m     \u001b[38;5;66;03m# also key on the thread ID to prevent segfaults with multi-threading\u001b[39;00m\n\u001b[32m   1621\u001b[39m     thread_id=threading.get_ident()\n\u001b[32m   1622\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\matplotlib\\font_manager.py:1557\u001b[39m, in \u001b[36m_get_font\u001b[39m\u001b[34m(font_filepaths, hinting_factor, _kerning_factor, thread_id)\u001b[39m\n\u001b[32m   1554\u001b[39m \u001b[38;5;129m@lru_cache\u001b[39m(\u001b[32m64\u001b[39m)\n\u001b[32m   1555\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_font\u001b[39m(font_filepaths, hinting_factor, *, _kerning_factor, thread_id):\n\u001b[32m   1556\u001b[39m     first_fontpath, *rest = font_filepaths\n\u001b[32m-> \u001b[39m\u001b[32m1557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ft2font.FT2Font(\n\u001b[32m   1558\u001b[39m         first_fontpath, hinting_factor,\n\u001b[32m   1559\u001b[39m         _fallback_list=[\n\u001b[32m   1560\u001b[39m             ft2font.FT2Font(\n\u001b[32m   1561\u001b[39m                 fpath, hinting_factor,\n\u001b[32m   1562\u001b[39m                 _kerning_factor=_kerning_factor\n\u001b[32m   1563\u001b[39m             )\n\u001b[32m   1564\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m fpath \u001b[38;5;129;01min\u001b[39;00m rest\n\u001b[32m   1565\u001b[39m         ],\n\u001b[32m   1566\u001b[39m         _kerning_factor=_kerning_factor\n\u001b[32m   1567\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: Can not load face (invalid stream operation; error code 0x55)"
     ]
    }
   ],
   "source": [
    "hyperparam_sweep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f8ce81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lehel\\AppData\\Local\\Temp\\ipykernel_28348\\3260712314.py:19: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"C:/Users/lehel/Desktop/UOS/Master/Sem2/Building_GPT_from_scratch/Building-GPT-from-Scratch/runs/hyperparam_sweep_results.csv\")\n",
    "\n",
    "# Plot validation loss and test loss vs dropout\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(df[\"dropout\"], df[\"val_loss\"], marker='o', label=\"Validation Loss\")\n",
    "plt.plot(df[\"dropout\"], df[\"test_loss\"], marker='o', label=\"Test Loss\")\n",
    "plt.xlabel(\"Dropout Rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Effect of Dropout on Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: plot perplexity as well\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(df[\"dropout\"], df[\"val_ppl\"], marker='o', label=\"Validation Perplexity\")\n",
    "plt.plot(df[\"dropout\"], df[\"test_ppl\"], marker='o', label=\"Test Perplexity\")\n",
    "plt.xlabel(\"Dropout Rate\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.title(\"Effect of Dropout on Perplexity\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plot.png\")\n",
    "plt.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
