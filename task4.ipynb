{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc25d52",
   "metadata": {},
   "source": [
    "## Task 4:\n",
    "Task remarks: GPT – Hand in, until 31.08.\n",
    "\n",
    "* If something is underspecified, just make decision yourself\n",
    "* Well-documented code\n",
    "* Submission format\n",
    "    * Notebook (incl. pdf) or GitHub readme (submit pdf with link to repo) as technical\n",
    "report of what we did\n",
    "        * Nice narrative and way to navigate code, not scientific paper\n",
    "        * Include plots (loss, perplexity scores, hyperparameters, etc.)\n",
    "        * Optional include pseudocode\n",
    "        * Qualitative analysis nice to have, e.g, add and evaluate generated text in\n",
    "report\n",
    "        * Can add appendix for additional plots\n",
    "* Hand in every mile stone, starting from UNIX comments\n",
    "* Removed in-between milestone of causal-self attention\n",
    "* Everything together in one file\n",
    "* Compare the models from each milestone, report perplexity for all\n",
    "    * Old-school n-gram\n",
    "    * Best neural n-gram\n",
    "    * GPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31503ede",
   "metadata": {},
   "source": [
    "**GPT itself**\n",
    "* Hyperparameter tuning: do not need all of them, choose what is most interesting and\n",
    "explain why\n",
    "    * Number of merges in BPE (not complete gridsearch, isolate top three number of\n",
    "merges in perplexity in n-gram, test those for GPT)\n",
    "    * Regularisation\n",
    "    * How small can we make neural embedding\n",
    "    * Do not change optimiser\n",
    "* General remarks\n",
    "    * Transformer blocks from scratch would be beyond 1.0, not required\n",
    "    * Implement causal self-attention yourself, do not use ready-made PyTorch version\n",
    "    * For computing perplexity: Implementing teacher forcing annealing is necessary\n",
    "for good generation performance, but we don’t have to do it for our assignment\n",
    "* Reminders\n",
    "    * Skip weight initialisation and optimiser configuration\n",
    "        * Can use standard PyTorch initialisation → just get transformer\n",
    "parameters and add them when initialising the optimiser\n",
    "    * Remember to change device selection, currently “cuda”, you might want “mps” or\n",
    "“cpu”\n",
    "    * Configs: make n_embd smaller, don’t change betas and weight decay (unless\n",
    "you want to), can change batch size, chunk size, n_head, n_layer\n",
    "    * Specify temperature and top-k parameters for generate function\n",
    "    * Activation function used in MLP: not ReLU as in slides but GELU (might not be in\n",
    "PyTorch yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b1cbad",
   "metadata": {},
   "source": [
    "## First try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b642298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Networks\n",
    "    This consists of two linear transformations with a ReLU activation in between.\n",
    "    \n",
    "    FFN(x) = max(0, xW1 + b1 )W2 + b2\n",
    "    d_model: embedding dimension (e.g., 512)\n",
    "    d_ff: feed-forward dimension (e.g., 2048)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.d_ff= d_ff\n",
    "        \n",
    "        # Linear transformation y = xW+b\n",
    "        self.fc1 = nn.Linear(self.d_model, self.d_ff, bias = True)\n",
    "        self.fc2 = nn.Linear(self.d_ff, self.d_model, bias = True)\n",
    "        \n",
    "        # for potential speed up\n",
    "        # Pre-normalize the weights (can help with training stability)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # check input and first FF layer dimension matching\n",
    "        batch_size, seq_length, d_input = input.size()\n",
    "        assert self.d_model == d_input, \"d_model must be the same dimension as the input\"\n",
    "\n",
    "        # First linear transformation followed by ReLU\n",
    "        # There's no need for explicit torch.max() as F.relu() already implements max(0,x)\n",
    "        f1 = F.relu(self.fc1(input))\n",
    "\n",
    "        # max(0, xW_1 + b_1)W_2 + b_2 \n",
    "        f2 =  self.fc2(f1)\n",
    "\n",
    "        return f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5645371",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Causal Self-Attention (no cross attention, GPT style)\n",
    "    Args:\n",
    "        d_model: total hidden dimension of the model\n",
    "        num_head: number of attention heads\n",
    "        dropout: dropout rate for attention scores\n",
    "        bias: whether to include bias in linear projections\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_head, dropout=0.1, bias=True): # infer d_k, d_v, d_q from d_model\n",
    "        super().__init__()  # Missing in the original implementation\n",
    "        assert d_model % num_head == 0, \"d_model must be divisible by num_head\"\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "        self.d_head=d_model//num_head\n",
    "        self.dropout_rate = dropout  # Store dropout rate separately\n",
    "\n",
    "        # linear transformations\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.output_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Initiialize scaler\n",
    "        self.scaler = float(1.0 / math.sqrt(self.d_head)) # Store as float in initialization\n",
    "        \n",
    "\n",
    "    def forward(self, sequence, att_mask=None):\n",
    "        \"\"\"Input shape: [batch_size, seq_len, d_model=num_head * d_head]\"\"\"\n",
    "        batch_size, seq_len, model_dim = sequence.size()\n",
    "\n",
    "        # Check only critical input dimensions\n",
    "        assert model_dim == self.d_model, f\"Input dimension {model_dim} doesn't match model dimension {self.d_model}\"\n",
    "    \n",
    "        \n",
    "        # Linear projections and reshape for multi-head\n",
    "        Q_state = self.q_proj(sequence)\n",
    "        \n",
    "        kv_seq_len = seq_len\n",
    "        K_state = self.k_proj(sequence)\n",
    "        V_state = self.v_proj(sequence)\n",
    "\n",
    "        #[batch_size, self.num_head, seq_len, self.d_head]\n",
    "        Q_state = Q_state.view(batch_size, seq_len, self.num_head, self.d_head).transpose(1,2) \n",
    "            \n",
    "        # in cross-attention, key/value sequence length might be different from query sequence length\n",
    "        K_state = K_state.view(batch_size, kv_seq_len, self.num_head, self.d_head).transpose(1,2)\n",
    "        V_state = V_state.view(batch_size, kv_seq_len, self.num_head, self.d_head).transpose(1,2)\n",
    "\n",
    "        # Scale Q by 1/sqrt(d_k)\n",
    "        Q_state = Q_state * self.scaler\n",
    "    \n",
    "    \n",
    "        # Compute attention matrix: QK^T\n",
    "        self.att_matrix = torch.matmul(Q_state, K_state.transpose(-1,-2)) \n",
    "\n",
    "    \n",
    "        # apply attention mask to attention matrix\n",
    "        if att_mask is not None and not isinstance(att_mask, torch.Tensor):\n",
    "            raise TypeError(\"att_mask must be a torch.Tensor\")\n",
    "\n",
    "        if att_mask is not None:\n",
    "            self.att_matrix = self.att_matrix + att_mask\n",
    "        \n",
    "        # apply softmax to the last dimension to get the attention score: softmax(QK^T)\n",
    "        att_score = F.softmax(self.att_matrix, dim = -1)\n",
    "    \n",
    "        # apply drop out to attention score\n",
    "        att_score = self.dropout(att_score)\n",
    "    \n",
    "        # get final output: softmax(QK^T)V\n",
    "        att_output = torch.matmul(att_score, V_state)\n",
    "    \n",
    "        # concatinate all attention heads\n",
    "        att_output = att_output.transpose(1, 2)\n",
    "        att_output = att_output.contiguous().view(batch_size, seq_len, self.num_head*self.d_head) \n",
    "    \n",
    "        # final linear transformation to the concatenated output\n",
    "        att_output = self.output_proj(att_output)\n",
    "\n",
    "        assert att_output.size() == (batch_size, seq_len, self.d_model), \\\n",
    "        f\"Final output shape {att_output.size()} incorrect\"\n",
    "\n",
    "        return att_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd20b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.att = TransformerAttention(d_model, n_head, dropout=dropout)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FFN(d_model, d_ff)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_causal_mask(seq_len):\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, embed_input, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        embed_input: Decoder input sequence [batch_size, seq_len, d_model]\n",
    "        casual_attention_mask: Causal mask for self-attention [batch_size, seq_len, seq_len]\n",
    "        padding_mask: Padding mask for cross-attention [batch_size, seq_len, encoder_seq_len]\n",
    "        Returns:\n",
    "        Tensor: Decoded output [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = embed_input.size()\n",
    "        \n",
    "        assert embed_input.size(-1) == self.d_model, f\"Input dimension {embed_input.size(-1)} doesn't match model dimension {self.d_model}\"\n",
    "\n",
    "        # Generate and expand causal mask for self-attention\n",
    "        causal_mask = self.create_causal_mask(seq_len).to(embed_input.device)  # [seq_len, seq_len]\n",
    "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(1)  # [1, 1, seq_len, seq_len]\n",
    "\n",
    "\n",
    "        # Self-attention + residual + norm\n",
    "        att_out = self.att(x, att_mask=causal_mask)\n",
    "        x = self.ln1(x + self.dropout(att_out))\n",
    "\n",
    "        # FFN + residual + norm\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.ln2(x + self.dropout(ffn_out))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b905ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layer, n_embd, n_head, d_ff, max_seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, n_embd)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GPTBlock(n_embd, n_head, d_ff, dropout) for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        batch_size, seq_len = idx.size()\n",
    "        \n",
    "        pos = torch.arange(0, seq_len, device=idx.device).unsqueeze(0)\n",
    "        x = self.token_emb(idx) + self.pos_emb(pos)\n",
    "        # Causal mask for GPT\n",
    "        mask = GPTBlock.create_causal_mask(seq_len).to(idx.device)\n",
    "        mask = mask.unsqueeze(0).unsqueeze(1)  # [1, 1, seq_len, seq_len]\n",
    "        for block in self.blocks:\n",
    "            x = block(x, att_mask=mask)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb70b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000      # depends on tokenizer\n",
    "n_layer = 4            # small for testing\n",
    "n_embd = 128           # embedding dimension\n",
    "n_head = 4             # must divide n_embd\n",
    "d_ff = 512             # feed-forward dimension\n",
    "max_seq_len = 128      # context window\n",
    "\n",
    "model = MiniGPT(vocab_size, n_layer, n_embd, n_head, d_ff, max_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f44e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_len = 10\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))  # random token ids\n",
    "\n",
    "logits = model(x)  # [batch_size, seq_len, vocab_size]\n",
    "print(logits.shape)\n",
    "# torch.Size([2, 10, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f821d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "for step in range(100):\n",
    "    x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    y = x.clone()  # next-token prediction (shifted later)\n",
    "\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step} | Loss {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906fc060",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(idx)                       # [batch, seq, vocab_size]\n",
    "        logits = logits[:, -1, :]                 # last token logits\n",
    "        probs = F.softmax(logits, dim=-1)         # convert to probs\n",
    "        next_token = torch.multinomial(probs, 1)  # sample\n",
    "        idx = torch.cat([idx, next_token], dim=1) # append\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac0590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generation\n",
    "start = torch.tensor([[1]])  # BOS token or just any token id\n",
    "out = generate(model, start, max_new_tokens=20)\n",
    "print(\"Generated sequence:\", out.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ceb3e",
   "metadata": {},
   "source": [
    "## All in One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf05d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # backend without GUI, avoids font rendering issues\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"  # Matplotlib’s default bundled font\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438c661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Found] Using merges file: Generated_tokens\\bpe_merges with k = 1000.txt\n",
      "[Save] Encoded splits to runs\\gpt_20250831_194556_k1000\n",
      "[Info] Vocab size = 1028\n",
      "[Info] Device: cpu | Parameters: 1.06M | Vocab=1028 | block=64\n",
      "[Step    50] train_loss=6.2965 (ema 6.4524)\n",
      "[Step   100] train_loss=5.6004 (ema 5.6783)\n",
      "[Step   150] train_loss=4.9912 (ema 5.1023)\n",
      "[Step   200] train_loss=4.7255 (ema 4.7917)\n",
      "[Step   250] train_loss=4.5611 (ema 4.6257)\n",
      "[Step   300] train_loss=4.4378 (ema 4.5209)\n",
      "[Step   350] train_loss=4.4139 (ema 4.4327)\n",
      "[Step   400] train_loss=4.3971 (ema 4.3671)\n",
      "[Step   450] train_loss=4.2738 (ema 4.2474)\n",
      "[Step   500] train_loss=4.1142 (ema 4.2118)\n",
      "[Eval    500] val_loss=4.2128 | val_ppl=67.55\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_194556_k1000\\samples/step500_eval.txt\n",
      "[Step   550] train_loss=4.1264 (ema 4.1518)\n",
      "[Step   600] train_loss=4.1204 (ema 4.0922)\n",
      "[Step   650] train_loss=4.0664 (ema 4.0559)\n",
      "[Step   700] train_loss=4.1015 (ema 4.0099)\n",
      "[Step   750] train_loss=3.8997 (ema 3.9549)\n",
      "[Step   800] train_loss=3.8165 (ema 3.9373)\n",
      "[Step   850] train_loss=3.8524 (ema 3.9016)\n",
      "[Step   900] train_loss=3.8912 (ema 3.8780)\n",
      "[Step   950] train_loss=3.7803 (ema 3.8186)\n",
      "[Step  1000] train_loss=3.8857 (ema 3.8333)\n",
      "[Eval   1000] val_loss=3.8468 | val_ppl=46.84\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_194556_k1000\\samples/step1000_eval.txt\n",
      "[Save] checkpoint → runs\\gpt_20250831_194556_k1000\\ckpt_step1000.pt\n",
      "[Step  1050] train_loss=3.7931 (ema 3.7603)\n",
      "[Step  1100] train_loss=3.7121 (ema 3.7485)\n",
      "[Step  1150] train_loss=3.7554 (ema 3.7254)\n",
      "[Step  1200] train_loss=3.7136 (ema 3.6658)\n",
      "[Step  1250] train_loss=3.5733 (ema 3.6612)\n",
      "[Step  1300] train_loss=3.6713 (ema 3.6573)\n",
      "[Step  1350] train_loss=3.5685 (ema 3.6091)\n",
      "[Step  1400] train_loss=3.5908 (ema 3.5725)\n",
      "[Step  1450] train_loss=3.4792 (ema 3.5790)\n",
      "[Step  1500] train_loss=3.5338 (ema 3.5203)\n",
      "[Eval   1500] val_loss=3.6651 | val_ppl=39.06\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_194556_k1000\\samples/step1500_eval.txt\n",
      "[Step  1550] train_loss=3.6047 (ema 3.5341)\n",
      "[Step  1600] train_loss=3.4627 (ema 3.5222)\n",
      "[Step  1650] train_loss=3.5493 (ema 3.5139)\n",
      "[Step  1700] train_loss=3.5101 (ema 3.4946)\n",
      "[Step  1750] train_loss=3.4785 (ema 3.4704)\n",
      "[Step  1800] train_loss=3.4415 (ema 3.4467)\n",
      "[Step  1850] train_loss=3.3926 (ema 3.4372)\n",
      "[Step  1900] train_loss=3.4117 (ema 3.4244)\n",
      "[Step  1950] train_loss=3.4446 (ema 3.4099)\n",
      "[Step  2000] train_loss=3.3646 (ema 3.3886)\n",
      "[Eval   2000] val_loss=3.5480 | val_ppl=34.75\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_194556_k1000\\samples/step2000_eval.txt\n",
      "[Save] checkpoint → runs\\gpt_20250831_194556_k1000\\ckpt_step2000.pt\n",
      "[Step  2050] train_loss=3.3707 (ema 3.3463)\n",
      "[Step  2100] train_loss=3.4147 (ema 3.3661)\n",
      "[Step  2150] train_loss=3.3684 (ema 3.3432)\n",
      "[Step  2200] train_loss=3.4249 (ema 3.3603)\n",
      "[Step  2250] train_loss=3.2608 (ema 3.3007)\n",
      "[Step  2300] train_loss=3.2399 (ema 3.3189)\n",
      "[Step  2350] train_loss=3.3044 (ema 3.3228)\n",
      "[Step  2400] train_loss=3.3216 (ema 3.2783)\n",
      "[Step  2450] train_loss=3.1295 (ema 3.2259)\n",
      "[Step  2500] train_loss=3.2668 (ema 3.2571)\n",
      "[Eval   2500] val_loss=3.4583 | val_ppl=31.76\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_194556_k1000\\samples/step2500_eval.txt\n",
      "[Step  2550] train_loss=3.2602 (ema 3.2247)\n",
      "[Step  2600] train_loss=3.3638 (ema 3.2451)\n",
      "[Step  2650] train_loss=3.3290 (ema 3.2632)\n",
      "[Step  2700] train_loss=3.1566 (ema 3.2142)\n",
      "[Step  2750] train_loss=3.1538 (ema 3.1979)\n",
      "[Step  2800] train_loss=3.0814 (ema 3.1741)\n",
      "[Step  2850] train_loss=3.1902 (ema 3.1960)\n",
      "[Step  2900] train_loss=3.1657 (ema 3.1721)\n",
      "[Step  2950] train_loss=3.0985 (ema 3.1604)\n",
      "[Step  3000] train_loss=3.1826 (ema 3.1352)\n",
      "[Eval   3000] val_loss=3.3943 | val_ppl=29.79\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_194556_k1000\\samples/step3000_eval.txt\n",
      "[Save] checkpoint → runs\\gpt_20250831_194556_k1000\\ckpt_step3000.pt\n",
      "[Step  3050] train_loss=3.0225 (ema 3.1291)\n",
      "[Step  3100] train_loss=3.1251 (ema 3.1149)\n",
      "[Step  3150] train_loss=3.1184 (ema 3.1022)\n",
      "[Step  3200] train_loss=3.0965 (ema 3.0877)\n",
      "[Step  3250] train_loss=3.1092 (ema 3.0727)\n",
      "[Step  3300] train_loss=3.0599 (ema 3.0592)\n",
      "[Step  3350] train_loss=3.1398 (ema 3.0819)\n",
      "[Step  3400] train_loss=3.0570 (ema 3.0717)\n",
      "[Step  3450] train_loss=3.0244 (ema 3.0219)\n",
      "[Step  3500] train_loss=2.9413 (ema 3.0433)\n",
      "[Eval   3500] val_loss=3.3517 | val_ppl=28.55\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_194556_k1000\\samples/step3500_eval.txt\n",
      "[Step  3550] train_loss=3.0516 (ema 3.0261)\n",
      "[Step  3600] train_loss=2.9063 (ema 3.0130)\n",
      "[Step  3650] train_loss=3.1171 (ema 2.9934)\n",
      "[Step  3700] train_loss=2.9154 (ema 2.9819)\n",
      "[Step  3750] train_loss=3.0362 (ema 2.9779)\n",
      "[Step  3800] train_loss=2.8819 (ema 2.9597)\n",
      "[Step  3850] train_loss=2.9215 (ema 2.9690)\n",
      "[Step  3900] train_loss=2.8709 (ema 2.9330)\n",
      "[Step  3950] train_loss=2.9568 (ema 2.9296)\n",
      "[Step  4000] train_loss=3.0458 (ema 2.9359)\n",
      "[Eval   4000] val_loss=3.3960 | val_ppl=29.84\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_194556_k1000\\samples/step4000_eval.txt\n",
      "[Save] checkpoint → runs\\gpt_20250831_194556_k1000\\ckpt_step4000.pt\n",
      "[Step  4050] train_loss=2.7083 (ema 2.8850)\n",
      "[Step  4100] train_loss=2.9291 (ema 2.8847)\n",
      "[Step  4150] train_loss=2.7895 (ema 2.8626)\n",
      "[Step  4200] train_loss=2.8218 (ema 2.8747)\n",
      "[Step  4250] train_loss=2.8551 (ema 2.8715)\n",
      "[Step  4300] train_loss=2.7686 (ema 2.8469)\n",
      "[Step  4350] train_loss=2.6274 (ema 2.8260)\n",
      "[Step  4400] train_loss=2.8546 (ema 2.8090)\n",
      "[Step  4450] train_loss=2.8401 (ema 2.8306)\n",
      "[Step  4500] train_loss=2.8101 (ema 2.8327)\n",
      "[Eval   4500] val_loss=3.3370 | val_ppl=28.14\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n",
      "[Sample Eval] saved → runs\\gpt_20250831_194556_k1000\\samples/step4500_eval.txt\n",
      "[Step  4550] train_loss=2.8570 (ema 2.8249)\n",
      "[Step  4600] train_loss=2.8337 (ema 2.7899)\n",
      "[Step  4650] train_loss=2.8077 (ema 2.7904)\n",
      "[Step  4700] train_loss=2.6499 (ema 2.7396)\n",
      "[Step  4750] train_loss=2.7580 (ema 2.7542)\n",
      "[Step  4800] train_loss=2.7288 (ema 2.7199)\n",
      "[Step  4850] train_loss=2.7487 (ema 2.7336)\n",
      "[Step  4900] train_loss=2.7044 (ema 2.7021)\n",
      "[Step  4950] train_loss=2.7205 (ema 2.7272)\n",
      "[Step  5000] train_loss=2.6339 (ema 2.7005)\n",
      "[Eval   5000] val_loss=3.3766 | val_ppl=29.27\n",
      "[Plot Warning] Could not generate plot: Can not load face (invalid stream operation; error code 0x55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lehel\\AppData\\Local\\Temp\\ipykernel_4080\\1083658976.py:513: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(8, 5))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sample Eval] saved → runs\\gpt_20250831_194556_k1000\\samples/step5000_eval.txt\n",
      "[Save] checkpoint → runs\\gpt_20250831_194556_k1000\\ckpt_step5000.pt\n",
      "[Final Val] loss=3.3733 | ppl=29.17\n",
      "[Final Test] loss=3.4242 | ppl=30.70\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'runs\\gpt_final_k1000'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 672\u001b[39m\n\u001b[32m    670\u001b[39m cfg = parse_args()   \u001b[38;5;66;03m# returns a TrainConfig object\u001b[39;00m\n\u001b[32m    671\u001b[39m results = train_and_eval_with_logging(cfg)\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m save_plot_and_csv(os.path.join(\u001b[33m\"\u001b[39m\u001b[33mruns\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgpt_final_k\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.k\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m), [results])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 502\u001b[39m, in \u001b[36msave_plot_and_csv\u001b[39m\u001b[34m(run_dir, history)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;66;03m# Save CSV\u001b[39;00m\n\u001b[32m    501\u001b[39m df = pd.DataFrame(history)\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m df.to_csv(csv_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# Try plotting\u001b[39;00m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    506\u001b[39m     \u001b[38;5;66;03m# Force matplotlib to use a safe font\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\pandas\\core\\generic.py:3986\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3975\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3977\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3978\u001b[39m     frame=df,\n\u001b[32m   3979\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3983\u001b[39m     decimal=decimal,\n\u001b[32m   3984\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3986\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter).to_csv(\n\u001b[32m   3987\u001b[39m     path_or_buf,\n\u001b[32m   3988\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   3989\u001b[39m     sep=sep,\n\u001b[32m   3990\u001b[39m     encoding=encoding,\n\u001b[32m   3991\u001b[39m     errors=errors,\n\u001b[32m   3992\u001b[39m     compression=compression,\n\u001b[32m   3993\u001b[39m     quoting=quoting,\n\u001b[32m   3994\u001b[39m     columns=columns,\n\u001b[32m   3995\u001b[39m     index_label=index_label,\n\u001b[32m   3996\u001b[39m     mode=mode,\n\u001b[32m   3997\u001b[39m     chunksize=chunksize,\n\u001b[32m   3998\u001b[39m     quotechar=quotechar,\n\u001b[32m   3999\u001b[39m     date_format=date_format,\n\u001b[32m   4000\u001b[39m     doublequote=doublequote,\n\u001b[32m   4001\u001b[39m     escapechar=escapechar,\n\u001b[32m   4002\u001b[39m     storage_options=storage_options,\n\u001b[32m   4003\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m csv_formatter.save()\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[32m    252\u001b[39m     \u001b[38;5;28mself\u001b[39m.filepath_or_buffer,\n\u001b[32m    253\u001b[39m     \u001b[38;5;28mself\u001b[39m.mode,\n\u001b[32m    254\u001b[39m     encoding=\u001b[38;5;28mself\u001b[39m.encoding,\n\u001b[32m    255\u001b[39m     errors=\u001b[38;5;28mself\u001b[39m.errors,\n\u001b[32m    256\u001b[39m     compression=\u001b[38;5;28mself\u001b[39m.compression,\n\u001b[32m    257\u001b[39m     storage_options=\u001b[38;5;28mself\u001b[39m.storage_options,\n\u001b[32m    258\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     check_parent_directory(\u001b[38;5;28mstr\u001b[39m(handle))\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lehel\\miniconda3\\envs\\bgpt\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: 'runs\\gpt_final_k1000'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GPT (Transformer) training pipeline for Shakespeare with BPE tokenizer\n",
    "====================================================================\n",
    "\n",
    "Goals\n",
    "-----\n",
    "- PyTorch implementation of a small GPT (nanoGPT-style) with clean structure.\n",
    "- Re-use your existing BPE merges and token conventions (</w>, <bos>, <eos>).\n",
    "- Detailed logging, file outputs, checkpoints, CSV logs, and PNG loss plots.\n",
    "- Validation + test perplexity.\n",
    "- Sample text generation at checkpoints.\n",
    "\n",
    "Directory layout (inputs & outputs)\n",
    "----------------------------------\n",
    "Inputs (must exist):\n",
    "- Corpus/\n",
    "    Shakespeare_clean_train.txt\n",
    "    Shakespeare_clean_valid.txt\n",
    "    Shakespeare_clean_test.txt\n",
    "- Generated_tokens/\n",
    "    (one of) bpe_merges with k = {k}.txt, standard_bpe_merges_k{k}.txt, ...\n",
    "\n",
    "Outputs (this script will create):\n",
    "- runs/gpt_{timestamp}_k{k}/\n",
    "    config.json\n",
    "    tokenizer.json\n",
    "    train_encoded.pt, valid_encoded.pt, test_encoded.pt\n",
    "    logs.csv\n",
    "    loss_plot.png\n",
    "    ckpt_step{...}.pt (model + optimizer + scaler + config + samples)\n",
    "    samples/\n",
    "        step{...}_sample.txt\n",
    "\n",
    "Usage\n",
    "-----\n",
    "python gpt_shakespeare_trainer.py --k 1600 --batch_size 64 --block_size 128 --n_layer 4 --n_head 4 --n_embd 256 --max_steps 2000\n",
    "\n",
    "Notes\n",
    "-----\n",
    "- Designed for small models/datasets. Mixed precision is optional (amp).\n",
    "- If no CUDA, training runs on CPU (slower but fine for tiny configs).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import sys\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Tuple, Dict, Iterable, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # headless\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"  # Matplotlib’s default bundled font\n",
    "\n",
    "# ================================ Constants ================================\n",
    "CORPUS_DIR = \"Corpus\"\n",
    "GENERATED_DIR = \"Generated_tokens\"\n",
    "WORD_END = \"</w>\"\n",
    "EOS = \"<eos>\"\n",
    "BOS = \"<bos>\"\n",
    "_wsre = re.compile(r\"\\s+\")\n",
    "#random.seed(42)\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    seed: int = 42\n",
    "    k: int = 1000\n",
    "    batch_size: int = 32\n",
    "    block_size: int = 128\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 128\n",
    "    dropout: float = 0.1\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 0.0\n",
    "    max_steps: int = 1000\n",
    "    eval_interval: int = 200\n",
    "    eval_batches: int = 20\n",
    "    ckpt_interval: int = 500\n",
    "    warmup_steps: int = 100\n",
    "    grad_clip: float = 1.0\n",
    "    amp: bool = True\n",
    "    no_amp: bool = False \n",
    "\n",
    "\n",
    "def parse_args() -> TrainConfig:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--k\", type=int, default=1000)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--block_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--n_layer\", type=int, default=4)\n",
    "    parser.add_argument(\"--n_head\", type=int, default=4)\n",
    "    parser.add_argument(\"--n_embd\", type=int, default=128)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--lr\", type=float, default=3e-4)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-2)\n",
    "    parser.add_argument(\"--max_steps\", type=int, default=5000)\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=100)\n",
    "    parser.add_argument(\"--grad_clip\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--eval_interval\", type=int, default=500)\n",
    "    parser.add_argument(\"--eval_batches\", type=int, default=20)\n",
    "    parser.add_argument(\"--ckpt_interval\", type=int, default=1000)\n",
    "    parser.add_argument(\"--no_amp\", action=\"store_true\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "\n",
    "    if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "        args, _ = parser.parse_known_args()\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    return TrainConfig(**vars(args))\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Core training params\n",
    "    parser.add_argument(\"--k\", type=int, default=1000)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--block_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--n_layer\", type=int, default=4)\n",
    "    parser.add_argument(\"--n_head\", type=int, default=4)\n",
    "    parser.add_argument(\"--n_embd\", type=int, default=128)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "\n",
    "    # Optimization\n",
    "    parser.add_argument(\"--lr\", type=float, default=3e-4)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-2)\n",
    "    parser.add_argument(\"--max_steps\", type=int, default=5000)\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=100)\n",
    "    parser.add_argument(\"--grad_clip\", type=float, default=1.0)\n",
    "\n",
    "    # Evaluation/checkpoints\n",
    "    parser.add_argument(\"--eval_interval\", type=int, default=500)\n",
    "    parser.add_argument(\"--eval_batches\", type=int, default=20)\n",
    "    parser.add_argument(\"--ckpt_interval\", type=int, default=1000)\n",
    "\n",
    "    # Misc\n",
    "    parser.add_argument(\"--no_amp\", action=\"store_true\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "\n",
    "    # Use parse_known_args to ignore --f=...json\n",
    "    if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "        args, _ = parser.parse_known_args()\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "# ============================= BPE Tokenizer ===============================\n",
    "\n",
    "def find_merges_file(k: int, verbose: bool = True) -> str:\n",
    "    candidates = [\n",
    "        os.path.join(GENERATED_DIR, f\"bpe_merges with k = {k}.txt\"),\n",
    "        os.path.join(GENERATED_DIR, f\"standard_bpe_merges_k{k}.txt\"),\n",
    "        os.path.join(GENERATED_DIR, f\"aggressive_clean_bpe_merges_k{k}.txt\"),\n",
    "        os.path.join(GENERATED_DIR, f\"bpe_merges_k{k}.txt\"),\n",
    "        os.path.join(GENERATED_DIR, f\"bpe_merges_k{k}_webtext_clean.txt\"),\n",
    "    ]\n",
    "    for path in candidates:\n",
    "        if os.path.exists(path):\n",
    "            if verbose:\n",
    "                print(f\"[Found] Using merges file: {path}\")\n",
    "            return path\n",
    "    raise FileNotFoundError(f\"No merges file found for k={k}. Tried: {candidates}\")\n",
    "\n",
    "def load_merges(merges_path: str) -> List[Tuple[str, str]]:\n",
    "    merges = []\n",
    "    with open(merges_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                merges.append((parts[0], parts[1]))\n",
    "    return merges\n",
    "\n",
    "def words_from_text(text: str, lowercase: bool = True) -> List[str]:\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    return [w for w in _wsre.split(text.strip()) if w]\n",
    "\n",
    "def apply_merges_to_word(word: str, merges: List[Tuple[str, str]]) -> List[str]:\n",
    "    symbols = tuple(list(word) + [WORD_END])\n",
    "    for a, b in merges:\n",
    "        out = []\n",
    "        i, L = 0, len(symbols)\n",
    "        while i < L:\n",
    "            if i < L-1 and symbols[i] == a and symbols[i+1] == b:\n",
    "                out.append(a + b); i += 2\n",
    "            else:\n",
    "                out.append(symbols[i]); i += 1\n",
    "        symbols = tuple(out)\n",
    "    return list(symbols)\n",
    "\n",
    "def tokenize_lines_with_merges(text: str, merges: List[Tuple[str, str]]) -> List[List[str]]:\n",
    "    token_lines: List[List[str]] = []\n",
    "    for line in text.strip().splitlines():\n",
    "        words = words_from_text(line)\n",
    "        if not words:\n",
    "            continue\n",
    "        toks: List[str] = []\n",
    "        for w in words:\n",
    "            toks.extend(apply_merges_to_word(w, merges))\n",
    "        toks.append(EOS)\n",
    "        token_lines.append(toks)\n",
    "    return token_lines\n",
    "\n",
    "# Convert tokens to ids, build vocab\n",
    "class BPETokenizer:\n",
    "    def __init__(self, merges: List[Tuple[str, str]], extra_tokens: Optional[List[str]] = None):\n",
    "        self.merges = merges\n",
    "        self.extra_tokens = extra_tokens or []\n",
    "        self.token_to_id: Dict[str, int] = {}\n",
    "        self.id_to_token: List[str] = []\n",
    "\n",
    "    def build_vocab_from_texts(self, texts: Dict[str, str]):\n",
    "        vocab = set()\n",
    "        for name, txt in texts.items():\n",
    "            for line in tokenize_lines_with_merges(txt, self.merges):\n",
    "                vocab.update(line)\n",
    "        vocab.update(self.extra_tokens)\n",
    "        # Deterministic order\n",
    "        self.id_to_token = sorted(vocab)\n",
    "        self.token_to_id = {t: i for i, t in enumerate(self.id_to_token)}\n",
    "\n",
    "    def encode_words(self, words: Iterable[str]) -> List[str]:\n",
    "        toks: List[str] = []\n",
    "        for w in (w.lower() for w in words):\n",
    "            toks.extend(apply_merges_to_word(w, self.merges))\n",
    "        return toks\n",
    "\n",
    "    def encode_lines(self, text: str) -> List[List[int]]:\n",
    "        lines_tok = tokenize_lines_with_merges(text, self.merges)\n",
    "        ids_lines: List[List[int]] = []\n",
    "        for line in lines_tok:\n",
    "            ids_lines.append([self.token_to_id[t] for t in line if t in self.token_to_id])\n",
    "        return ids_lines\n",
    "\n",
    "    def decode_tokens(self, token_stream: List[str]) -> List[str]:\n",
    "        words: List[str] = []\n",
    "        buf: List[str] = []\n",
    "        for t in token_stream:\n",
    "            if t == EOS:\n",
    "                break\n",
    "            buf.append(t)\n",
    "            if t.endswith(WORD_END):\n",
    "                chars: List[str] = []\n",
    "                for sub in buf:\n",
    "                    if sub.endswith(WORD_END):\n",
    "                        chars.extend(list(sub[:-len(WORD_END)]))\n",
    "                    else:\n",
    "                        chars.extend(list(sub))\n",
    "                words.append(\"\".join(chars))\n",
    "                buf = []\n",
    "        if buf:\n",
    "            chars = []\n",
    "            for sub in buf:\n",
    "                if sub.endswith(WORD_END):\n",
    "                    chars.extend(list(sub[:-len(WORD_END)]))\n",
    "                else:\n",
    "                    chars.extend(list(sub))\n",
    "            if chars:\n",
    "                words.append(\"\".join(chars))\n",
    "        return words\n",
    "\n",
    "    def save(self, path: str):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"id_to_token\": self.id_to_token,\n",
    "                \"token_to_id\": self.token_to_id,\n",
    "                \"extra_tokens\": self.extra_tokens,\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: str) -> \"BPETokenizer\":\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            obj = json.load(f)\n",
    "        tok = BPETokenizer(merges=[], extra_tokens=obj.get(\"extra_tokens\", []))\n",
    "        tok.id_to_token = obj[\"id_to_token\"]\n",
    "        tok.token_to_id = {k: int(v) for k, v in obj[\"token_to_id\"].items()}\n",
    "        return tok\n",
    "\n",
    "# ============================ Data preparation ==============================\n",
    "\n",
    "def read_text(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "@dataclass\n",
    "class EncodedSplits:\n",
    "    train: torch.Tensor\n",
    "    valid: torch.Tensor\n",
    "    test: torch.Tensor\n",
    "\n",
    "\n",
    "def build_or_load_encoded(run_dir: str, k: int) -> Tuple[BPETokenizer, EncodedSplits]:\n",
    "    enc_train_path = os.path.join(run_dir, \"train_encoded.pt\")\n",
    "    enc_valid_path = os.path.join(run_dir, \"valid_encoded.pt\")\n",
    "    enc_test_path  = os.path.join(run_dir, \"test_encoded.pt\")\n",
    "    tok_path       = os.path.join(run_dir, \"tokenizer.json\")\n",
    "\n",
    "    if all(os.path.exists(p) for p in [enc_train_path, enc_valid_path, enc_test_path, tok_path]):\n",
    "        print(\"[Load] Using cached encoded splits + tokenizer\")\n",
    "        tokenizer = BPETokenizer.load(tok_path)\n",
    "        train_ids = torch.load(enc_train_path)\n",
    "        valid_ids = torch.load(enc_valid_path)\n",
    "        test_ids  = torch.load(enc_test_path)\n",
    "        return tokenizer, EncodedSplits(train_ids, valid_ids, test_ids)\n",
    "\n",
    "    # Build from raw\n",
    "    merges_path = find_merges_file(k, verbose=True)\n",
    "    merges = load_merges(merges_path)\n",
    "\n",
    "    train_txt = read_text(os.path.join(CORPUS_DIR, \"Shakespeare_clean_train.txt\"))\n",
    "    valid_txt = read_text(os.path.join(CORPUS_DIR, \"Shakespeare_clean_valid.txt\"))\n",
    "    test_txt  = read_text(os.path.join(CORPUS_DIR, \"Shakespeare_clean_test.txt\"))\n",
    "\n",
    "    tokenizer = BPETokenizer(merges=merges, extra_tokens=[BOS, EOS])\n",
    "    tokenizer.build_vocab_from_texts({\"train\": train_txt, \"valid\": valid_txt, \"test\": test_txt})\n",
    "\n",
    "    def flatten(lines: List[List[int]]) -> List[int]:\n",
    "        flat = []\n",
    "        for ln in lines: flat.extend(ln)\n",
    "        return flat\n",
    "\n",
    "    train_ids = torch.tensor(flatten(tokenizer.encode_lines(train_txt)), dtype=torch.long)\n",
    "    valid_ids = torch.tensor(flatten(tokenizer.encode_lines(valid_txt)), dtype=torch.long)\n",
    "    test_ids  = torch.tensor(flatten(tokenizer.encode_lines(test_txt)),  dtype=torch.long)\n",
    "\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    tokenizer.save(tok_path)\n",
    "    torch.save(train_ids, enc_train_path)\n",
    "    torch.save(valid_ids, enc_valid_path)\n",
    "    torch.save(test_ids,  enc_test_path)\n",
    "    print(f\"[Save] Encoded splits to {run_dir}\")\n",
    "    print(f\"[Info] Vocab size = {len(tokenizer.id_to_token)}\")\n",
    "    return tokenizer, EncodedSplits(train_ids, valid_ids, test_ids)\n",
    "\n",
    "# ================================ Dataset ==================================\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, ids: torch.Tensor, block_size: int):\n",
    "        self.ids = ids\n",
    "        self.block_size = block_size\n",
    "        # we will sample random start positions in __getitem__\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # approximate number of sequences of length block_size we can draw\n",
    "        return max(1, len(self.ids) - self.block_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ignore idx and sample randomly to add stochasticity\n",
    "        i = random.randint(0, len(self.ids) - self.block_size - 1)\n",
    "        x = self.ids[i : i + self.block_size]\n",
    "        y = self.ids[i + 1 : i + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "# ============================== GPT Model ==================================\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd: int, n_head: int, block_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.key = nn.Linear(n_embd, n_embd)\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        self.value = nn.Linear(n_embd, n_embd)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        # causal mask\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(k.size(-1))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v  # (B, n_head, T, head_dim)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd: int, n_head: int, block_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = CausalSelfAttention(n_embd, n_head, block_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_layer: int, n_head: int, n_embd: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte': nn.Embedding(vocab_size, n_embd),\n",
    "            'wpe': nn.Embedding(block_size, n_embd),\n",
    "            'drop': nn.Dropout(dropout),\n",
    "            'h': nn.ModuleList([Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)]),\n",
    "            'ln_f': nn.LayerNorm(n_embd),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.block_size\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        tok_emb = self.transformer['wte'](idx)\n",
    "        pos_emb = self.transformer['wpe'](pos)[None, :, :]\n",
    "        x = self.transformer['drop'](tok_emb + pos_emb)\n",
    "        for block in self.transformer['h']:\n",
    "            x = block(x)\n",
    "        x = self.transformer['ln_f'](x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "# ============================ Training utilities ============================\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# ============================== Evaluation ==================================\n",
    "\n",
    "def evaluate(model: GPT, loader: DataLoader, device: torch.device, max_batches: int) -> float:\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            if i >= max_batches:\n",
    "                break\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            losses.append(loss.item())\n",
    "    model.train()\n",
    "    return float(sum(losses) / max(1, len(losses)))\n",
    "\n",
    "# ============================== Generation ==================================\n",
    "\n",
    "def generate(model: GPT, start_tokens: List[int], max_new_tokens: int, temperature: float = 1.0, top_k: Optional[int] = None) -> List[int]:\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    idx = torch.tensor(start_tokens, dtype=torch.long, device=device)[None, :]\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -model.block_size:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / max(1e-8, temperature)\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, next_id), dim=1)\n",
    "    return idx[0].tolist()\n",
    "\n",
    "# ============================== Plot / CSV ==================================\n",
    "\n",
    "def save_plot_and_csv(run_dir, history):\n",
    "    \"\"\"Save training/validation loss plot and history CSV.\"\"\"\n",
    "    # make sure run_dir and subdirs exist\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(run_dir, \"samples\"), exist_ok=True)\n",
    "\n",
    "    csv_path = os.path.join(run_dir, \"history.csv\")\n",
    "    png_path = os.path.join(run_dir, \"history.png\")\n",
    "\n",
    "    # Save CSV\n",
    "    df = pd.DataFrame(history)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    # Try plotting\n",
    "    try:\n",
    "        # Force matplotlib to use a safe font\n",
    "        matplotlib.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "\n",
    "        steps = [h[\"step\"] for h in history]\n",
    "        train_loss = [h[\"train_loss\"] for h in history]\n",
    "        val_loss = [h[\"val_loss\"] for h in history]\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(steps, train_loss, label=\"train_loss\")\n",
    "        plt.plot(steps, val_loss, label=\"val_loss\")\n",
    "        plt.xlabel(\"step\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(png_path)\n",
    "        plt.close()\n",
    "        print(f\"[Plot] Saved to {png_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Plot Warning] Could not generate plot: {e}\")\n",
    "\n",
    "\n",
    "# ============================== Main training ===============================\n",
    "\n",
    "def train_and_eval_with_logging(cfg: TrainConfig):\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(\"runs\", f\"gpt_{ts}_k{cfg.k}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(run_dir, \"samples\"), exist_ok=True)\n",
    "\n",
    "    # Save config\n",
    "    with open(os.path.join(run_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(asdict(cfg), f, indent=2)\n",
    "\n",
    "    # Encode or load\n",
    "    tokenizer, splits = build_or_load_encoded(run_dir, cfg.k)\n",
    "    vocab_size = len(tokenizer.id_to_token)\n",
    "\n",
    "    # Datasets/loaders\n",
    "    train_ds = GPTDataset(splits.train, cfg.block_size)\n",
    "    valid_ds = GPTDataset(splits.valid, cfg.block_size)\n",
    "    test_ds  = GPTDataset(splits.test,  cfg.block_size)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=cfg.batch_size, shuffle=False, drop_last=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    # Model\n",
    "    model = GPT(vocab_size=vocab_size, block_size=cfg.block_size, n_layer=cfg.n_layer, n_head=cfg.n_head, n_embd=cfg.n_embd, dropout=cfg.dropout)\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"[Info] Device: {device} | Parameters: {count_parameters(model)/1e6:.2f}M | Vocab={vocab_size} | block={cfg.block_size}\")\n",
    "\n",
    "    # Optimizer / Scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    lr_sched = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: min(1.0, step / max(1, cfg.warmup_steps))\n",
    "    )\n",
    "\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=cfg.amp and device.type == \"cuda\")\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = []\n",
    "    running_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    step = 0\n",
    "    while step < cfg.max_steps:\n",
    "        for xb, yb in train_loader:\n",
    "            step += 1\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\", enabled=scaler.is_enabled()):\n",
    "                logits = model(xb)\n",
    "                loss = loss_fn(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            if cfg.grad_clip:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            lr_sched.step()\n",
    "\n",
    "            running_loss = 0.9 * running_loss + 0.1 * loss.item() if step > 1 else loss.item()\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                print(f\"[Step {step:5d}] train_loss={loss.item():.4f} (ema {running_loss:.4f})\")\n",
    "\n",
    "\n",
    "            # Eval\n",
    "            if step % cfg.eval_interval == 0 or step == cfg.max_steps:\n",
    "                val_loss = evaluate(model, valid_loader, device, cfg.eval_batches)\n",
    "                val_ppl = math.exp(val_loss)\n",
    "                print(f\"[Eval  {step:5d}] val_loss={val_loss:.4f} | val_ppl={val_ppl:.2f}\")\n",
    "                history.append({\"step\": step, \"train_loss\": running_loss, \"val_loss\": val_loss, \"val_ppl\": val_ppl})\n",
    "                save_plot_and_csv(run_dir, history)\n",
    "\n",
    "                # save extrinsic evaluation with fixed prompts\n",
    "                prompts = [\"To be or not to be\", \"Once upon a midnight dreary\"]\n",
    "                eval_path = os.path.join(run_dir, f\"samples/step{step}_eval.txt\")\n",
    "                with open(eval_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    for prompt in prompts:\n",
    "                        start_ids = tokenizer.encode_words(prompt.split())\n",
    "                        start_ids = [tokenizer.token_to_id.get(tok, 0) for tok in start_ids]\n",
    "                        f.write(f\"\\nPrompt: {prompt}\\n\")\n",
    "\n",
    "                        greedy_ids = generate(model, start_tokens=start_ids, max_new_tokens=40, temperature=1.0, top_k=None)\n",
    "                        greedy_text = \" \".join([tokenizer.id_to_token[i] for i in greedy_ids])\n",
    "                        f.write(\"Greedy: \" + greedy_text + \"\\n\")\n",
    "\n",
    "                        topk_ids = generate(model, start_tokens=start_ids, max_new_tokens=40, temperature=0.8, top_k=50)\n",
    "                        topk_text = \" \".join([tokenizer.id_to_token[i] for i in topk_ids])\n",
    "                        f.write(\"Top-k: \" + topk_text + \"\\n\")\n",
    "                print(f\"[Sample Eval] saved → {eval_path}\")\n",
    "\n",
    "            # Checkpoint + sample\n",
    "            if step % cfg.ckpt_interval == 0 or step == cfg.max_steps:\n",
    "                ckpt_path = os.path.join(run_dir, f\"ckpt_step{step}.pt\")\n",
    "                torch.save({\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"optimizer_state\": optimizer.state_dict(),\n",
    "                    \"scaler_state\": scaler.state_dict(),\n",
    "                    \"config\": asdict(cfg),\n",
    "                    \"vocab_size\": vocab_size,\n",
    "                    \"step\": step,\n",
    "                }, ckpt_path)\n",
    "                print(f\"[Save] checkpoint → {ckpt_path}\")\n",
    "\n",
    "            if step >= cfg.max_steps:\n",
    "                break\n",
    "\n",
    "    # Final test evaluation\n",
    "    val_loss = evaluate(model, valid_loader, device, cfg.eval_batches)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    test_loss = evaluate(model, test_loader, device, cfg.eval_batches)\n",
    "    test_ppl = math.exp(test_loss)\n",
    "    \n",
    "    print(f\"[Final Val] loss={val_loss:.4f} | ppl={val_ppl:.2f}\")\n",
    "    print(f\"[Final Test] loss={test_loss:.4f} | ppl={test_ppl:.2f}\")\n",
    "\n",
    "    # Save final metrics\n",
    "    results = {\n",
    "        \"k\": cfg.k,\n",
    "        \"n_embd\": cfg.n_embd,\n",
    "        \"dropout\": cfg.dropout,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_ppl\": val_ppl,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_ppl\": test_ppl,\n",
    "    }\n",
    "    with open(os.path.join(run_dir, \"final_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = parse_args()   # returns a TrainConfig object\n",
    "    results = train_and_eval_with_logging(cfg)\n",
    "    save_plot_and_csv(os.path.join(\"runs\", f\"gpt_final_k{cfg.k}\"), [results])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc73fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Training & Evaluation ----------------\n",
    "def train_and_eval_for_gpt(k, n_embd, dropout, run_dir, num_steps=2000, batch_size=32, lr=3e-4, eval_batches=50):\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# --- Load data & tokenizer ---\n",
    "tokenizer, splits = build_or_load_encoded(run_dir, k)\n",
    "vocab_size = len(tokenizer.token_to_id)\n",
    "\n",
    "\n",
    "train_ds = GPTDataset(splits.train, block_size=64)\n",
    "val_ds = GPTDataset(splits.valid, block_size=64)\n",
    "test_ds = GPTDataset(splits.test, block_size=64)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "\n",
    "# --- Build model ---\n",
    "model = GPT(\n",
    "vocab_size=vocab_size,\n",
    "block_size=64,\n",
    "n_layer=4,\n",
    "n_head=4,\n",
    "n_embd=n_embd,\n",
    "dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# --- Training loop ---\n",
    "print(f\"Training GPT (k={k}, n_embd={n_embd}, dropout={dropout})...\")\n",
    "for step in range(1, num_steps+1):\n",
    "loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "\n",
    "if step % 200 == 0:\n",
    "val_loss = evaluate(model, val_loader, device, max_batches=eval_batches)\n",
    "val_ppl = math.exp(val_loss)\n",
    "print(f\"Step {step}: train_loss={loss:.4f}, val_loss={val_loss:.4f}, val_ppl={val_ppl:.2f}\")\n",
    "\n",
    "\n",
    "# --- Final evaluation ---\n",
    "val_loss = evaluate(model, val_loader, device, max_batches=eval_batches)\n",
    "test_loss = evaluate(model, test_loader, device, max_batches=eval_batches)\n",
    "val_ppl = math.exp(val_loss)\n",
    "test_ppl = math.exp(test_loss)\n",
    "\n",
    "\n",
    "print(f\"Final [Val] loss={val_loss:.4f}, ppl={val_ppl:.2f}\")\n",
    "print(f\"Final [Test] loss={test_loss:.4f}, ppl={test_ppl:.2f}\")\n",
    "\n",
    "\n",
    "# --- Extrinsic: Generation ---\n",
    "prompts = [\"To be or not to be\", \"Once upon a midnight dreary\"]\n",
    "for prompt in prompts:\n",
    "start_ids = tokenizer.encode_words(prompt.split())\n",
    "start_ids = [tokenizer.token_to_id.get(tok, 0) for tok in start_ids]\n",
    "\n",
    "\n",
    "print(\"\\nPrompt:\", prompt)\n",
    "print(\"Greedy:\")\n",
    "greedy_ids = generate(model, start_tokens=start_ids, max_new_tokens=40, temperature=1.0, top_k=None)\n",
    "print(\" \".join(tokenizer.decode_tokens([tokenizer.id_to_token[i] for i in greedy_ids])))\n",
    "\n",
    "\n",
    "print(\"Top-k:\")\n",
    "topk_ids = generate(model, start_tokens=start_ids, max_new_tokens=40, temperature=0.8, top_k=50)\n",
    "print(\" \".join(tokenizer.decode_tokens([tokenizer.id_to_token[i] for i in topk_ids])))\n",
    "\n",
    "\n",
    "return {\n",
    "\"k\": k,\n",
    "\"n_embd\": n_embd,\n",
    "\"dropout\": dropout,\n",
    "\"val_loss\": val_loss,\n",
    "\"val_ppl\": val_ppl,\n",
    "\"test_loss\": test_loss,\n",
    "\"test_ppl\": test_ppl,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d674a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for k in [200, 500, 1000]:   # your top 3 from n-grams\n",
    "    for n_embd in [64, 128]:\n",
    "        res = train_and_eval_for_gpt(k, n_embd, dropout=0.1, run_dir=\"runs/gpt_exp\")\n",
    "        results.append(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83749dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## probably not used\n",
    "\n",
    "\"\"\"# sample generation from random position of valid set\n",
    "start = random.randint(0, max(0, len(splits.valid) - cfg.block_size - 1))\n",
    "prefix = splits.valid[start:start+min(32, cfg.block_size)].tolist()\n",
    "gen_ids = generate(model, start_tokens=prefix, max_new_tokens=60, temperature=0.8, top_k=50)\n",
    "sample_txt = \" \".join([tokenizer.id_to_token[i] for i in gen_ids])\n",
    "sample_path = os.path.join(run_dir, \"samples\", f\"step{step}_sample.txt\")\n",
    "with open(sample_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sample_txt)\n",
    "print(f\"[Sample] saved → {sample_path}\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
